{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb7b2e2-c2fb-40c8-b703-435d08210a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TRAIN: 261 anemic / 2214 total\n",
      "VAL: 42 anemic / 369 total\n",
      "TEST: 47 anemic / 369 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 17.4395\n",
      "Epoch 40/120 Loss: 3.9500\n",
      "Epoch 60/120 Loss: 2.6751\n",
      "Epoch 80/120 Loss: 2.7094\n",
      "Epoch 100/120 Loss: 1.6504\n",
      "Epoch 120/120 Loss: 0.6012\n",
      "Fold 1 â†’ P=0.273, R=0.113\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 13.5626\n",
      "Epoch 40/120 Loss: 2.6841\n",
      "Epoch 60/120 Loss: 2.7825\n",
      "Epoch 80/120 Loss: 1.0533\n",
      "Epoch 100/120 Loss: 1.5211\n",
      "Epoch 120/120 Loss: 1.4311\n",
      "Fold 2 â†’ P=0.286, R=0.154\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 17.4963\n",
      "Epoch 40/120 Loss: 2.7392\n",
      "Epoch 60/120 Loss: 2.2847\n",
      "Epoch 80/120 Loss: 1.2580\n",
      "Epoch 100/120 Loss: 0.9016\n",
      "Epoch 120/120 Loss: 1.2250\n",
      "Fold 3 â†’ P=0.235, R=0.154\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 13.0562\n",
      "Epoch 40/120 Loss: 3.5227\n",
      "Epoch 60/120 Loss: 1.4606\n",
      "Epoch 80/120 Loss: 1.8726\n",
      "Epoch 100/120 Loss: 0.5412\n",
      "Epoch 120/120 Loss: 0.4747\n",
      "Fold 4 â†’ P=0.500, R=0.115\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 13.0257\n",
      "Epoch 40/120 Loss: 2.4589\n",
      "Epoch 60/120 Loss: 1.4561\n",
      "Epoch 80/120 Loss: 2.1991\n",
      "Epoch 100/120 Loss: 2.5627\n",
      "Epoch 120/120 Loss: 0.4280\n",
      "Fold 5 â†’ P=0.125, R=0.019\n",
      "âœ… Best fold = 2 | P=0.286, R=0.154\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.3103\n",
      "Recall:    0.1915\n",
      "F1 score:  0.2368\n",
      "Accuracy:  0.8428\n",
      "AUC:       0.6814\n",
      "TP, TN, FP, FN: 9, 302, 20, 38\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 21:17:22.436731: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-13 21:17:22.438028: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-13 21:17:22.464379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-13 21:17:22.990843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 21:17:24.692521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-10-13 21:17:24.694151: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_20_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 21:17:29.642029: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-10-13 21:17:29.642061: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-10-13 21:17:29.644448: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model\n",
      "2025-10-13 21:17:29.675829: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-10-13 21:17:29.675852: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model\n",
      "2025-10-13 21:17:29.707957: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-10-13 21:17:29.708693: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-10-13 21:17:29.783248: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/tf_model\n",
      "2025-10-13 21:17:29.806489: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 162042 microseconds.\n",
      "2025-10-13 21:17:29.920728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient (no duplication / multi-view)\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 22\n",
    "LR_CV = 0.00022\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"tongue_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING\n",
    "# =========================\n",
    "def load_images(folder, label):\n",
    "    imgs, lbls = [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "    return imgs, lbls\n",
    "\n",
    "train_imgs, train_lbls = [], []\n",
    "val_imgs, val_lbls = [], []\n",
    "test_imgs, test_lbls = [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l\n",
    "\n",
    "print(f\"TRAIN: {sum(train_lbls)} anemic / {len(train_lbls)} total\")\n",
    "print(f\"VAL: {sum(val_lbls)} anemic / {len(val_lbls)} total\")\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        preds.extend((p>0.5).astype(int))\n",
    "        probs.extend(p)\n",
    "        labels_all.extend(labels.cpu().numpy().flatten())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP (FIXED: CV uses ONLY train data)\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    # ðŸ”¥ CRITICAL FIX: Use ONLY original training data for CV\n",
    "    X = train_imgs  # â† Only training images\n",
    "    y = train_lbls  # â† Only training labels\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        # Create train/val splits from ORIGINAL TRAINING DATA ONLY\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Print loss every 20 epochs\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping check (every epoch for safety)\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _ = evaluate(model, vl_loader)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        # Final evaluation on fold's validation set\n",
    "        val_metrics = evaluate(model, vl_loader)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    # Save CV results\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Load best model (fix PyTorch 2.6 loading)\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False  # <--- add this if using PyTorch >=2.6\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    # Evaluate on ORIGINAL TEST SET\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    return model\n",
    "'''\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "'''\n",
    "''' single right eye 3 alone model results'''\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    \"\"\"\n",
    "    Converts a trained PyTorch model to TFLite format via ONNX and TensorFlow.\n",
    "    \"\"\"\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress ONNX-TF/TF warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # --- 1. Preparation and Tracing ---\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # --- 2. PyTorch to ONNX ---\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        # Export the model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. ONNX to TensorFlow SavedModel ---\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        # Load the ONNX model\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        \n",
    "        # Prepare the TensorFlow backend\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        \n",
    "        # Export the model to TensorFlow SavedModel format\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow SavedModel failed. Make sure onnx-tf and tensorflow are installed. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. TensorFlow SavedModel to TFLite ---\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        # Initialize the TFLite converter\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        \n",
    "        # Convert the model\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# RUN (Update)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # The trained model 'zz' is returned by train_and_eval_single()\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # â­ ADDED STEP: Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6791acd7-b6ae-4c19-83fb-d48a8cf25e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TRAIN: 259 anemic / 2204 total\n",
      "VAL: 43 anemic / 368 total\n",
      "TEST: 47 anemic / 366 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 13.9981\n",
      "Epoch 40/120 Loss: 3.2932\n",
      "Epoch 60/120 Loss: 1.1374\n",
      "Epoch 80/120 Loss: 3.9157\n",
      "Epoch 100/120 Loss: 0.6025\n",
      "Epoch 120/120 Loss: 2.0979\n",
      "Fold 1 â†’ P=0.077, R=0.019\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 17.9273\n",
      "Epoch 40/120 Loss: 5.2430\n",
      "Epoch 60/120 Loss: 2.3776\n",
      "Epoch 80/120 Loss: 2.9448\n",
      "Epoch 100/120 Loss: 1.2343\n",
      "Epoch 120/120 Loss: 1.6316\n",
      "Fold 2 â†’ P=0.286, R=0.077\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 14.7595\n",
      "Epoch 40/120 Loss: 2.4022\n",
      "Epoch 60/120 Loss: 1.3159\n",
      "Epoch 80/120 Loss: 3.1686\n",
      "Epoch 100/120 Loss: 1.3249\n",
      "Epoch 120/120 Loss: 1.5639\n",
      "Fold 3 â†’ P=0.071, R=0.038\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 15.4388\n",
      "Epoch 40/120 Loss: 2.4653\n",
      "Epoch 60/120 Loss: 2.2577\n",
      "Epoch 80/120 Loss: 1.4330\n",
      "Epoch 100/120 Loss: 1.8825\n",
      "Epoch 120/120 Loss: 1.4303\n",
      "Fold 4 â†’ P=0.267, R=0.077\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 16.2593\n",
      "Epoch 40/120 Loss: 4.5535\n",
      "Epoch 60/120 Loss: 3.3602\n",
      "Epoch 80/120 Loss: 1.1426\n",
      "Epoch 100/120 Loss: 0.2468\n",
      "Epoch 120/120 Loss: 1.6757\n",
      "Fold 5 â†’ P=0.500, R=0.039\n",
      "âœ… Best fold = 2 | P=0.286, R=0.077\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.5000\n",
      "Recall:    0.0213\n",
      "F1 score:  0.0408\n",
      "Accuracy:  0.8716\n",
      "AUC:       0.6568\n",
      "TP, TN, FP, FN: 1, 318, 1, 46\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_20_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 21:34:08.129686: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-10-13 21:34:08.129722: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-10-13 21:34:08.131926: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model\n",
      "2025-10-13 21:34:08.163427: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-10-13 21:34:08.163445: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model\n",
      "2025-10-13 21:34:08.194695: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-10-13 21:34:08.262147: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/tf_model\n",
      "2025-10-13 21:34:08.288615: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 156694 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tongue_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient (no duplication / multi-view)\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 22\n",
    "LR_CV = 0.00022\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"tongue_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING\n",
    "# =========================\n",
    "def load_images(folder, label):\n",
    "    imgs, lbls = [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "    return imgs, lbls\n",
    "\n",
    "train_imgs, train_lbls = [], []\n",
    "val_imgs, val_lbls = [], []\n",
    "test_imgs, test_lbls = [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l\n",
    "\n",
    "print(f\"TRAIN: {sum(train_lbls)} anemic / {len(train_lbls)} total\")\n",
    "print(f\"VAL: {sum(val_lbls)} anemic / {len(val_lbls)} total\")\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        preds.extend((p>0.5).astype(int))\n",
    "        probs.extend(p)\n",
    "        labels_all.extend(labels.cpu().numpy().flatten())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP (FIXED: CV uses ONLY train data)\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    # ðŸ”¥ CRITICAL FIX: Use ONLY original training data for CV\n",
    "    X = train_imgs  # â† Only training images\n",
    "    y = train_lbls  # â† Only training labels\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        # Create train/val splits from ORIGINAL TRAINING DATA ONLY\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Print loss every 20 epochs\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping check (every epoch for safety)\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _ = evaluate(model, vl_loader)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        # Final evaluation on fold's validation set\n",
    "        val_metrics = evaluate(model, vl_loader)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    # Save CV results\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Load best model (fix PyTorch 2.6 loading)\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False  # <--- add this if using PyTorch >=2.6\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    # Evaluate on ORIGINAL TEST SET\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    return model\n",
    "'''\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "'''\n",
    "''' single right eye 3 alone model results'''\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    \"\"\"\n",
    "    Converts a trained PyTorch model to TFLite format via ONNX and TensorFlow.\n",
    "    \"\"\"\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress ONNX-TF/TF warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # --- 1. Preparation and Tracing ---\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # --- 2. PyTorch to ONNX ---\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        # Export the model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. ONNX to TensorFlow SavedModel ---\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        # Load the ONNX model\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        \n",
    "        # Prepare the TensorFlow backend\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        \n",
    "        # Export the model to TensorFlow SavedModel format\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow SavedModel failed. Make sure onnx-tf and tensorflow are installed. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. TensorFlow SavedModel to TFLite ---\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        # Initialize the TFLite converter\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        \n",
    "        # Convert the model\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# RUN (Update)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # The trained model 'zz' is returned by train_and_eval_single()\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # â­ ADDED STEP: Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620df04e-b0fb-4678-8276-79e2f6ea52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient (no duplication / multi-view)\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 500\n",
    "BATCH_CV = 8\n",
    "LR_CV = 0.0003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"tongue_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING\n",
    "# =========================\n",
    "def load_images(folder, label):\n",
    "    imgs, lbls = [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "    return imgs, lbls\n",
    "\n",
    "train_imgs, train_lbls = [], []\n",
    "val_imgs, val_lbls = [], []\n",
    "test_imgs, test_lbls = [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l = load_images(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l\n",
    "\n",
    "print(f\"TRAIN: {sum(train_lbls)} anemic / {len(train_lbls)} total\")\n",
    "print(f\"VAL: {sum(val_lbls)} anemic / {len(val_lbls)} total\")\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        preds.extend((p>0.5).astype(int))\n",
    "        probs.extend(p)\n",
    "        labels_all.extend(labels.cpu().numpy().flatten())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP (FIXED: CV uses ONLY train data)\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    # ðŸ”¥ CRITICAL FIX: Use ONLY original training data for CV\n",
    "    X = train_imgs  # â† Only training images\n",
    "    y = train_lbls  # â† Only training labels\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        # Create train/val splits from ORIGINAL TRAINING DATA ONLY\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Print loss every 20 epochs\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping check (every epoch for safety)\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _ = evaluate(model, vl_loader)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        # Final evaluation on fold's validation set\n",
    "        val_metrics = evaluate(model, vl_loader)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    # Save CV results\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # FINAL EVALUATION ON ORIGINAL TEST SET\n",
    "    # =========================\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Load best model (fix PyTorch 2.6 loading)\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False  # <--- add this if using PyTorch >=2.6\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    # Evaluate on ORIGINAL TEST SET\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    return model\n",
    "'''\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "'''\n",
    "''' single right eye 3 alone model results'''\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    \"\"\"\n",
    "    Converts a trained PyTorch model to TFLite format via ONNX and TensorFlow.\n",
    "    \"\"\"\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress ONNX-TF/TF warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # --- 1. Preparation and Tracing ---\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # --- 2. PyTorch to ONNX ---\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        # Export the model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. ONNX to TensorFlow SavedModel ---\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        # Load the ONNX model\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        \n",
    "        # Prepare the TensorFlow backend\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        \n",
    "        # Export the model to TensorFlow SavedModel format\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow SavedModel failed. Make sure onnx-tf and tensorflow are installed. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. TensorFlow SavedModel to TFLite ---\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        # Initialize the TFLite converter\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        \n",
    "        # Convert the model\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# RUN (Update)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # The trained model 'zz' is returned by train_and_eval_single()\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # â­ ADDED STEP: Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49754da-b9d0-424d-9611-7b220e08dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TRAIN anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_train_roi/ | files=261\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_train_roi/ | files=259\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_train_roi/ | files=253\n",
      "\n",
      "ðŸ“‚ TRAIN non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/ | files=1953\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/ | files=1945\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/ | files=1940\n",
      "\n",
      "ðŸ“‚ VAL   anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_val_roi/ | files=42\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_val_roi/ | files=43\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_val_roi/ | files=43\n",
      "\n",
      "ðŸ“‚ VAL   non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/ | files=327\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/ | files=325\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/ | files=323\n",
      "\n",
      "ðŸ“‚ TEST  anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_test_roi/ | files=47\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_test_roi/ | files=47\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_test_roi/ | files=47\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/ | files=322\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/ | files=319\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/ | files=318\n",
      "\n",
      "ðŸ”Ž TRAIN (right) - discovered bases: anemic=245, non-anemic=1882\n",
      "âœ… TRAIN: tri-matched right samples -> anemic=245, non-anemic=1882, total=2127\n",
      "\n",
      "ðŸ”Ž VAL (right) - discovered bases: anemic=40, non-anemic=316\n",
      "âœ… VAL: tri-matched right samples -> anemic=40, non-anemic=316, total=356\n",
      "\n",
      "ðŸ”Ž TEST (right) - discovered bases: anemic=46, non-anemic=301\n",
      "âœ… TEST: tri-matched right samples -> anemic=46, non-anemic=301, total=347\n",
      "\n",
      "===== Processing right resolution: 224 =====\n",
      "\n",
      "--- right Fold 1 ---\n",
      "Epoch [1/120] ðŸ” Loss: 28.074776\n",
      "Epoch [2/120] ðŸ” Loss: 27.719461\n",
      "Epoch [3/120] ðŸ” Loss: 26.283124\n",
      "Epoch [4/120] ðŸ” Loss: 26.465237\n",
      "Epoch [5/120] ðŸ” Loss: 26.392708\n",
      "Epoch [6/120] ðŸ” Loss: 26.507805\n",
      "Epoch [7/120] ðŸ” Loss: 25.903379\n",
      "Epoch [8/120] ðŸ” Loss: 24.626428\n",
      "Epoch [9/120] ðŸ” Loss: 25.004090\n",
      "Epoch [10/120] ðŸ” Loss: 25.685645\n",
      "Epoch [11/120] ðŸ” Loss: 25.023626\n",
      "Epoch [12/120] ðŸ” Loss: 23.852262\n",
      "Epoch [13/120] ðŸ” Loss: 24.427963\n",
      "Epoch [14/120] ðŸ” Loss: 24.533027\n",
      "Epoch [15/120] ðŸ” Loss: 22.946629\n",
      "Epoch [16/120] ðŸ” Loss: 23.032064\n",
      "Epoch [17/120] ðŸ” Loss: 22.863245\n",
      "Epoch [18/120] ðŸ” Loss: 22.949665\n",
      "Epoch [19/120] ðŸ” Loss: 21.530383\n",
      "Epoch [20/120] ðŸ” Loss: 20.540583\n",
      "Epoch [21/120] ðŸ” Loss: 21.265456\n",
      "Epoch [22/120] ðŸ” Loss: 19.747070\n",
      "Epoch [23/120] ðŸ” Loss: 18.462094\n",
      "Epoch [24/120] ðŸ” Loss: 18.868986\n",
      "Epoch [25/120] ðŸ” Loss: 18.922697\n",
      "Epoch [26/120] ðŸ” Loss: 15.404036\n",
      "Epoch [27/120] ðŸ” Loss: 16.467829\n",
      "Epoch [28/120] ðŸ” Loss: 16.493749\n",
      "Epoch [29/120] ðŸ” Loss: 15.337020\n",
      "Epoch [30/120] ðŸ” Loss: 14.305771\n",
      "Epoch [31/120] ðŸ” Loss: 14.657773\n",
      "Epoch [32/120] ðŸ” Loss: 11.863819\n",
      "Epoch [33/120] ðŸ” Loss: 11.938460\n",
      "Epoch [34/120] ðŸ” Loss: 12.350228\n",
      "Epoch [35/120] ðŸ” Loss: 10.308050\n",
      "Epoch [36/120] ðŸ” Loss: 10.694184\n",
      "Epoch [37/120] ðŸ” Loss: 9.853725\n",
      "Epoch [38/120] ðŸ” Loss: 9.323649\n",
      "Epoch [39/120] ðŸ” Loss: 7.566641\n",
      "Epoch [40/120] ðŸ” Loss: 9.091691\n",
      "Epoch [41/120] ðŸ” Loss: 8.753752\n",
      "Epoch [42/120] ðŸ” Loss: 7.957381\n",
      "Epoch [43/120] ðŸ” Loss: 8.055474\n",
      "Epoch [44/120] ðŸ” Loss: 6.030655\n",
      "Epoch [45/120] ðŸ” Loss: 5.972829\n",
      "Epoch [46/120] ðŸ” Loss: 5.416667\n",
      "Epoch [47/120] ðŸ” Loss: 6.419161\n",
      "Epoch [48/120] ðŸ” Loss: 6.169904\n",
      "Epoch [49/120] ðŸ” Loss: 7.389209\n",
      "Epoch [50/120] ðŸ” Loss: 5.543451\n",
      "Epoch [51/120] ðŸ” Loss: 5.056443\n",
      "Epoch [52/120] ðŸ” Loss: 5.410489\n",
      "Epoch [53/120] ðŸ” Loss: 4.525448\n",
      "Epoch [54/120] ðŸ” Loss: 5.775325\n",
      "Epoch [55/120] ðŸ” Loss: 3.952402\n",
      "Epoch [56/120] ðŸ” Loss: 3.702291\n",
      "Epoch [57/120] ðŸ” Loss: 4.098017\n",
      "Epoch [58/120] ðŸ” Loss: 3.947670\n",
      "Epoch [59/120] ðŸ” Loss: 3.619502\n",
      "Epoch [60/120] ðŸ” Loss: 5.134459\n",
      "Epoch [61/120] ðŸ” Loss: 3.739272\n",
      "Epoch [62/120] ðŸ” Loss: 4.635129\n",
      "Epoch [63/120] ðŸ” Loss: 2.868894\n",
      "Epoch [64/120] ðŸ” Loss: 2.932509\n",
      "Epoch [65/120] ðŸ” Loss: 4.784086\n",
      "Epoch [66/120] ðŸ” Loss: 2.658131\n",
      "Epoch [67/120] ðŸ” Loss: 3.033385\n",
      "Epoch [68/120] ðŸ” Loss: 3.129033\n",
      "Epoch [69/120] ðŸ” Loss: 2.127071\n",
      "Epoch [70/120] ðŸ” Loss: 2.597629\n",
      "Epoch [71/120] ðŸ” Loss: 2.948419\n",
      "Epoch [72/120] ðŸ” Loss: 1.881839\n",
      "Epoch [73/120] ðŸ” Loss: 4.415653\n",
      "Epoch [74/120] ðŸ” Loss: 2.214662\n",
      "Epoch [75/120] ðŸ” Loss: 1.959636\n",
      "Epoch [76/120] ðŸ” Loss: 3.421026\n",
      "Epoch [77/120] ðŸ” Loss: 2.319586\n",
      "Epoch [78/120] ðŸ” Loss: 2.572199\n",
      "Epoch [79/120] ðŸ” Loss: 3.097060\n",
      "Epoch [80/120] ðŸ” Loss: 1.839249\n",
      "Epoch [81/120] ðŸ” Loss: 2.717220\n",
      "Epoch [82/120] ðŸ” Loss: 2.537606\n",
      "Epoch [83/120] ðŸ” Loss: 1.465285\n",
      "Epoch [84/120] ðŸ” Loss: 2.599172\n",
      "Epoch [85/120] ðŸ” Loss: 3.864447\n",
      "Epoch [86/120] ðŸ” Loss: 2.697223\n",
      "Epoch [87/120] ðŸ” Loss: 1.361379\n",
      "Epoch [88/120] ðŸ” Loss: 2.818421\n",
      "Epoch [89/120] ðŸ” Loss: 1.053801\n",
      "Epoch [90/120] ðŸ” Loss: 2.853371\n",
      "Epoch [91/120] ðŸ” Loss: 2.760205\n",
      "Epoch [92/120] ðŸ” Loss: 1.394519\n",
      "Epoch [93/120] ðŸ” Loss: 1.389832\n",
      "Epoch [94/120] ðŸ” Loss: 1.817887\n",
      "Epoch [95/120] ðŸ” Loss: 1.564256\n",
      "Epoch [96/120] ðŸ” Loss: 1.323535\n",
      "Epoch [97/120] ðŸ” Loss: 3.085930\n",
      "Epoch [98/120] ðŸ” Loss: 1.609916\n",
      "Epoch [99/120] ðŸ” Loss: 2.083360\n",
      "Epoch [100/120] ðŸ” Loss: 2.047141\n",
      "Epoch [101/120] ðŸ” Loss: 2.705274\n",
      "Epoch [102/120] ðŸ” Loss: 1.523098\n",
      "Epoch [103/120] ðŸ” Loss: 1.201950\n",
      "Epoch [104/120] ðŸ” Loss: 1.156404\n",
      "Epoch [105/120] ðŸ” Loss: 1.205025\n",
      "Epoch [106/120] ðŸ” Loss: 4.512845\n",
      "Epoch [107/120] ðŸ” Loss: 2.727098\n",
      "Epoch [108/120] ðŸ” Loss: 2.739597\n",
      "Epoch [109/120] ðŸ” Loss: 0.958199\n",
      "Epoch [110/120] ðŸ” Loss: 1.379812\n",
      "Epoch [111/120] ðŸ” Loss: 2.035673\n",
      "Epoch [112/120] ðŸ” Loss: 0.996117\n",
      "Epoch [113/120] ðŸ” Loss: 1.196347\n",
      "Epoch [114/120] ðŸ” Loss: 1.575368\n",
      "Epoch [115/120] ðŸ” Loss: 1.473259\n",
      "Epoch [116/120] ðŸ” Loss: 0.307042\n",
      "Epoch [117/120] ðŸ” Loss: 0.281173\n",
      "Epoch [118/120] ðŸ” Loss: 1.199013\n",
      "Epoch [119/120] ðŸ” Loss: 1.232391\n",
      "Epoch [120/120] ðŸ” Loss: 1.967576\n",
      "{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Epochs': 120, 'BatchSize': 22, 'LearningRate': 0.00022, 'Val_Precision': 0.375, 'Val_Recall': 0.12244897959183673, 'Val_F1': 0.1846153846153846, 'Val_Accuracy': 0.8755868544600939, 'Val_AUC': 0.5680723217669029, 'Val_TP': 6, 'Val_TN': 367, 'Val_FP': 10, 'Val_FN': 43}\n",
      "\n",
      "--- right Fold 2 ---\n",
      "Epoch [1/120] ðŸ” Loss: 28.006446\n",
      "Epoch [2/120] ðŸ” Loss: 27.502667\n",
      "Epoch [3/120] ðŸ” Loss: 26.931247\n",
      "Epoch [4/120] ðŸ” Loss: 26.782259\n",
      "Epoch [5/120] ðŸ” Loss: 26.478830\n",
      "Epoch [6/120] ðŸ” Loss: 26.170891\n",
      "Epoch [7/120] ðŸ” Loss: 25.645778\n",
      "Epoch [8/120] ðŸ” Loss: 25.748638\n",
      "Epoch [9/120] ðŸ” Loss: 25.658930\n",
      "Epoch [10/120] ðŸ” Loss: 25.464359\n",
      "Epoch [11/120] ðŸ” Loss: 24.926146\n",
      "Epoch [12/120] ðŸ” Loss: 24.471523\n",
      "Epoch [13/120] ðŸ” Loss: 24.350335\n",
      "Epoch [14/120] ðŸ” Loss: 24.264855\n",
      "Epoch [15/120] ðŸ” Loss: 23.838833\n",
      "Epoch [16/120] ðŸ” Loss: 24.603677\n",
      "Epoch [17/120] ðŸ” Loss: 22.836977\n",
      "Epoch [18/120] ðŸ” Loss: 22.806061\n",
      "Epoch [19/120] ðŸ” Loss: 22.205822\n",
      "Epoch [20/120] ðŸ” Loss: 22.015388\n",
      "Epoch [21/120] ðŸ” Loss: 21.431656\n",
      "Epoch [22/120] ðŸ” Loss: 20.435280\n",
      "Epoch [23/120] ðŸ” Loss: 21.218490\n",
      "Epoch [24/120] ðŸ” Loss: 19.619100\n",
      "Epoch [25/120] ðŸ” Loss: 17.978759\n",
      "Epoch [26/120] ðŸ” Loss: 17.324053\n",
      "Epoch [27/120] ðŸ” Loss: 16.653887\n",
      "Epoch [28/120] ðŸ” Loss: 14.782962\n",
      "Epoch [29/120] ðŸ” Loss: 16.668453\n",
      "Epoch [30/120] ðŸ” Loss: 14.610283\n",
      "Epoch [31/120] ðŸ” Loss: 14.718590\n",
      "Epoch [32/120] ðŸ” Loss: 12.440325\n",
      "Epoch [33/120] ðŸ” Loss: 11.079325\n",
      "Epoch [34/120] ðŸ” Loss: 12.023619\n",
      "Epoch [35/120] ðŸ” Loss: 11.052737\n",
      "Epoch [36/120] ðŸ” Loss: 10.321180\n",
      "Epoch [37/120] ðŸ” Loss: 8.373273\n",
      "Epoch [38/120] ðŸ” Loss: 9.670459\n",
      "Epoch [39/120] ðŸ” Loss: 10.061403\n",
      "Epoch [40/120] ðŸ” Loss: 6.945305\n",
      "Epoch [41/120] ðŸ” Loss: 7.773670\n",
      "Epoch [42/120] ðŸ” Loss: 7.888092\n",
      "Epoch [43/120] ðŸ” Loss: 6.956232\n",
      "Epoch [44/120] ðŸ” Loss: 6.318697\n",
      "Epoch [45/120] ðŸ” Loss: 6.316435\n",
      "Epoch [46/120] ðŸ” Loss: 6.831252\n",
      "Epoch [47/120] ðŸ” Loss: 5.829228\n",
      "Epoch [48/120] ðŸ” Loss: 6.733970\n",
      "Epoch [49/120] ðŸ” Loss: 5.796024\n",
      "Epoch [50/120] ðŸ” Loss: 3.728556\n",
      "Epoch [51/120] ðŸ” Loss: 5.257252\n",
      "Epoch [52/120] ðŸ” Loss: 5.051941\n",
      "Epoch [53/120] ðŸ” Loss: 4.054934\n",
      "Epoch [54/120] ðŸ” Loss: 4.920961\n",
      "Epoch [55/120] ðŸ” Loss: 3.409743\n",
      "Epoch [56/120] ðŸ” Loss: 3.893892\n",
      "Epoch [57/120] ðŸ” Loss: 3.737747\n",
      "Epoch [58/120] ðŸ” Loss: 3.510432\n",
      "Epoch [59/120] ðŸ” Loss: 3.506648\n",
      "Epoch [60/120] ðŸ” Loss: 3.791595\n",
      "Epoch [61/120] ðŸ” Loss: 3.294806\n",
      "Epoch [62/120] ðŸ” Loss: 4.447552\n",
      "Epoch [63/120] ðŸ” Loss: 2.865556\n",
      "Epoch [64/120] ðŸ” Loss: 2.159984\n",
      "Epoch [65/120] ðŸ” Loss: 7.090212\n",
      "Epoch [66/120] ðŸ” Loss: 3.433232\n",
      "Epoch [67/120] ðŸ” Loss: 3.344251\n",
      "Epoch [68/120] ðŸ” Loss: 2.768785\n",
      "Epoch [69/120] ðŸ” Loss: 3.804534\n",
      "Epoch [70/120] ðŸ” Loss: 3.328799\n",
      "Epoch [71/120] ðŸ” Loss: 3.247869\n",
      "Epoch [72/120] ðŸ” Loss: 2.416243\n",
      "Epoch [73/120] ðŸ” Loss: 2.957211\n",
      "Epoch [74/120] ðŸ” Loss: 1.739397\n",
      "Epoch [75/120] ðŸ” Loss: 2.622050\n",
      "Epoch [76/120] ðŸ” Loss: 2.866485\n",
      "Epoch [77/120] ðŸ” Loss: 1.601080\n",
      "Epoch [78/120] ðŸ” Loss: 1.800132\n",
      "Epoch [79/120] ðŸ” Loss: 3.119393\n",
      "Epoch [80/120] ðŸ” Loss: 2.248297\n",
      "Epoch [81/120] ðŸ” Loss: 1.660153\n",
      "Epoch [82/120] ðŸ” Loss: 2.114791\n",
      "Epoch [83/120] ðŸ” Loss: 2.031828\n",
      "Epoch [84/120] ðŸ” Loss: 3.040675\n",
      "Epoch [85/120] ðŸ” Loss: 2.469359\n",
      "Epoch [86/120] ðŸ” Loss: 1.558740\n",
      "Epoch [87/120] ðŸ” Loss: 1.068521\n",
      "Epoch [88/120] ðŸ” Loss: 2.919680\n",
      "Epoch [89/120] ðŸ” Loss: 2.312675\n",
      "Epoch [90/120] ðŸ” Loss: 2.737135\n",
      "Epoch [91/120] ðŸ” Loss: 3.148455\n",
      "Epoch [92/120] ðŸ” Loss: 2.116758\n",
      "Epoch [93/120] ðŸ” Loss: 1.467552\n",
      "Epoch [94/120] ðŸ” Loss: 2.633072\n",
      "Epoch [95/120] ðŸ” Loss: 2.061575\n",
      "Epoch [96/120] ðŸ” Loss: 0.893105\n",
      "Epoch [97/120] ðŸ” Loss: 1.985662\n",
      "Epoch [98/120] ðŸ” Loss: 3.612032\n",
      "Epoch [99/120] ðŸ” Loss: 1.278013\n",
      "Epoch [100/120] ðŸ” Loss: 1.810535\n",
      "Epoch [101/120] ðŸ” Loss: 1.235995\n",
      "Epoch [102/120] ðŸ” Loss: 1.495460\n",
      "Epoch [103/120] ðŸ” Loss: 2.478118\n",
      "Epoch [104/120] ðŸ” Loss: 2.256760\n",
      "Epoch [105/120] ðŸ” Loss: 1.302649\n",
      "Epoch [106/120] ðŸ” Loss: 1.567096\n",
      "Epoch [107/120] ðŸ” Loss: 1.975984\n",
      "Epoch [108/120] ðŸ” Loss: 1.091902\n",
      "Epoch [109/120] ðŸ” Loss: 1.099399\n",
      "Epoch [110/120] ðŸ” Loss: 1.215318\n",
      "Epoch [111/120] ðŸ” Loss: 3.074648\n",
      "Epoch [112/120] ðŸ” Loss: 1.812883\n",
      "Epoch [113/120] ðŸ” Loss: 0.858222\n",
      "Epoch [114/120] ðŸ” Loss: 0.618239\n",
      "Epoch [115/120] ðŸ” Loss: 1.332723\n",
      "Epoch [116/120] ðŸ” Loss: 1.706599\n",
      "Epoch [117/120] ðŸ” Loss: 1.042580\n",
      "Epoch [118/120] ðŸ” Loss: 0.885951\n",
      "Epoch [119/120] ðŸ” Loss: 1.233600\n",
      "Epoch [120/120] ðŸ” Loss: 1.720202\n",
      "{'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Epochs': 120, 'BatchSize': 22, 'LearningRate': 0.00022, 'Val_Precision': 0.25, 'Val_Recall': 0.12244897959183673, 'Val_F1': 0.1643835616438356, 'Val_Accuracy': 0.8568075117370892, 'Val_AUC': 0.5458777675526444, 'Val_TP': 6, 'Val_TN': 359, 'Val_FP': 18, 'Val_FN': 43}\n",
      "\n",
      "--- right Fold 3 ---\n",
      "Epoch [1/120] ðŸ” Loss: 28.616555\n",
      "Epoch [2/120] ðŸ” Loss: 27.502279\n",
      "Epoch [3/120] ðŸ” Loss: 26.699191\n",
      "Epoch [4/120] ðŸ” Loss: 26.902853\n",
      "Epoch [5/120] ðŸ” Loss: 26.878850\n",
      "Epoch [6/120] ðŸ” Loss: 26.358829\n",
      "Epoch [7/120] ðŸ” Loss: 26.266060\n",
      "Epoch [8/120] ðŸ” Loss: 26.152199\n",
      "Epoch [9/120] ðŸ” Loss: 25.578390\n",
      "Epoch [10/120] ðŸ” Loss: 24.773794\n",
      "Epoch [11/120] ðŸ” Loss: 24.626379\n",
      "Epoch [12/120] ðŸ” Loss: 24.738469\n",
      "Epoch [13/120] ðŸ” Loss: 23.934100\n",
      "Epoch [14/120] ðŸ” Loss: 23.988143\n",
      "Epoch [15/120] ðŸ” Loss: 23.839053\n",
      "Epoch [16/120] ðŸ” Loss: 23.420787\n",
      "Epoch [17/120] ðŸ” Loss: 22.380764\n",
      "Epoch [18/120] ðŸ” Loss: 22.306990\n",
      "Epoch [19/120] ðŸ” Loss: 21.941220\n",
      "Epoch [20/120] ðŸ” Loss: 21.709045\n",
      "Epoch [21/120] ðŸ” Loss: 21.200723\n",
      "Epoch [22/120] ðŸ” Loss: 20.572060\n",
      "Epoch [23/120] ðŸ” Loss: 18.709915\n",
      "Epoch [24/120] ðŸ” Loss: 19.985975\n",
      "Epoch [25/120] ðŸ” Loss: 17.732991\n",
      "Epoch [26/120] ðŸ” Loss: 17.431102\n",
      "Epoch [27/120] ðŸ” Loss: 16.358374\n",
      "Epoch [28/120] ðŸ” Loss: 16.091841\n",
      "Epoch [29/120] ðŸ” Loss: 16.601174\n",
      "Epoch [30/120] ðŸ” Loss: 15.409703\n",
      "Epoch [31/120] ðŸ” Loss: 12.898819\n",
      "Epoch [32/120] ðŸ” Loss: 12.879936\n",
      "Epoch [33/120] ðŸ” Loss: 13.034297\n",
      "Epoch [34/120] ðŸ” Loss: 14.183447\n",
      "Epoch [35/120] ðŸ” Loss: 10.826318\n",
      "Epoch [36/120] ðŸ” Loss: 9.823767\n",
      "Epoch [37/120] ðŸ” Loss: 9.045397\n",
      "Epoch [38/120] ðŸ” Loss: 8.371749\n",
      "Epoch [39/120] ðŸ” Loss: 9.350070\n",
      "Epoch [40/120] ðŸ” Loss: 10.468661\n",
      "Epoch [41/120] ðŸ” Loss: 6.261673\n",
      "Epoch [42/120] ðŸ” Loss: 7.167482\n",
      "Epoch [43/120] ðŸ” Loss: 7.287506\n",
      "Epoch [44/120] ðŸ” Loss: 5.211632\n",
      "Epoch [45/120] ðŸ” Loss: 4.659118\n",
      "Epoch [46/120] ðŸ” Loss: 5.691418\n",
      "Epoch [47/120] ðŸ” Loss: 4.914256\n",
      "Epoch [48/120] ðŸ” Loss: 4.374497\n",
      "Epoch [49/120] ðŸ” Loss: 3.406498\n",
      "Epoch [50/120] ðŸ” Loss: 4.487251\n",
      "Epoch [51/120] ðŸ” Loss: 3.345622\n",
      "Epoch [52/120] ðŸ” Loss: 8.551890\n",
      "Epoch [53/120] ðŸ” Loss: 3.746036\n",
      "Epoch [54/120] ðŸ” Loss: 5.153126\n",
      "Epoch [55/120] ðŸ” Loss: 3.043275\n",
      "Epoch [56/120] ðŸ” Loss: 3.357637\n",
      "Epoch [57/120] ðŸ” Loss: 4.751933\n",
      "Epoch [58/120] ðŸ” Loss: 3.875816\n",
      "Epoch [59/120] ðŸ” Loss: 3.203630\n",
      "Epoch [60/120] ðŸ” Loss: 2.917481\n",
      "Epoch [61/120] ðŸ” Loss: 3.110549\n",
      "Epoch [62/120] ðŸ” Loss: 3.000117\n",
      "Epoch [63/120] ðŸ” Loss: 3.214724\n",
      "Epoch [64/120] ðŸ” Loss: 2.295205\n",
      "Epoch [65/120] ðŸ” Loss: 3.038136\n",
      "Epoch [66/120] ðŸ” Loss: 2.449740\n",
      "Epoch [67/120] ðŸ” Loss: 2.975289\n",
      "Epoch [68/120] ðŸ” Loss: 3.704048\n",
      "Epoch [69/120] ðŸ” Loss: 1.728007\n",
      "Epoch [70/120] ðŸ” Loss: 1.577805\n",
      "Epoch [71/120] ðŸ” Loss: 3.024493\n",
      "Epoch [72/120] ðŸ” Loss: 2.364756\n",
      "Epoch [73/120] ðŸ” Loss: 2.210107\n",
      "Epoch [74/120] ðŸ” Loss: 3.118347\n",
      "Epoch [75/120] ðŸ” Loss: 1.995984\n",
      "Epoch [76/120] ðŸ” Loss: 3.509454\n",
      "Epoch [77/120] ðŸ” Loss: 2.895738\n",
      "Epoch [78/120] ðŸ” Loss: 2.489224\n",
      "Epoch [79/120] ðŸ” Loss: 1.842963\n",
      "Epoch [80/120] ðŸ” Loss: 2.201859\n",
      "Epoch [81/120] ðŸ” Loss: 0.757187\n",
      "Epoch [82/120] ðŸ” Loss: 2.372163\n",
      "Epoch [83/120] ðŸ” Loss: 1.024300\n",
      "Epoch [84/120] ðŸ” Loss: 2.916563\n",
      "Epoch [85/120] ðŸ” Loss: 1.666504\n",
      "Epoch [86/120] ðŸ” Loss: 3.497305\n",
      "Epoch [87/120] ðŸ” Loss: 1.721883\n",
      "Epoch [88/120] ðŸ” Loss: 2.198433\n",
      "Epoch [89/120] ðŸ” Loss: 2.979532\n",
      "Epoch [90/120] ðŸ” Loss: 1.321784\n",
      "Epoch [91/120] ðŸ” Loss: 1.693903\n",
      "Epoch [92/120] ðŸ” Loss: 1.864762\n",
      "Epoch [93/120] ðŸ” Loss: 0.730363\n",
      "Epoch [94/120] ðŸ” Loss: 1.153265\n",
      "Epoch [95/120] ðŸ” Loss: 2.732936\n",
      "Epoch [96/120] ðŸ” Loss: 1.778439\n",
      "Epoch [97/120] ðŸ” Loss: 3.076460\n",
      "Epoch [98/120] ðŸ” Loss: 1.322586\n",
      "Epoch [99/120] ðŸ” Loss: 0.544045\n",
      "Epoch [100/120] ðŸ” Loss: 3.197284\n",
      "Epoch [101/120] ðŸ” Loss: 1.374646\n",
      "Epoch [102/120] ðŸ” Loss: 1.532979\n",
      "Epoch [103/120] ðŸ” Loss: 0.935206\n",
      "Epoch [104/120] ðŸ” Loss: 2.662198\n",
      "Epoch [105/120] ðŸ” Loss: 1.384327\n",
      "Epoch [106/120] ðŸ” Loss: 1.717958\n",
      "Epoch [107/120] ðŸ” Loss: 1.118634\n",
      "Epoch [108/120] ðŸ” Loss: 0.967024\n",
      "Epoch [109/120] ðŸ” Loss: 0.450419\n",
      "Epoch [110/120] ðŸ” Loss: 2.765395\n",
      "Epoch [111/120] ðŸ” Loss: 1.621510\n",
      "Epoch [112/120] ðŸ” Loss: 2.303377\n",
      "Epoch [113/120] ðŸ” Loss: 0.613178\n",
      "Epoch [114/120] ðŸ” Loss: 1.505595\n",
      "Epoch [115/120] ðŸ” Loss: 2.616794\n",
      "Epoch [116/120] ðŸ” Loss: 1.687362\n",
      "Epoch [117/120] ðŸ” Loss: 1.446398\n",
      "Epoch [118/120] ðŸ” Loss: 2.336341\n",
      "Epoch [119/120] ðŸ” Loss: 1.092496\n",
      "Epoch [120/120] ðŸ” Loss: 0.533017\n",
      "{'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Epochs': 120, 'BatchSize': 22, 'LearningRate': 0.00022, 'Val_Precision': 0.22448979591836735, 'Val_Recall': 0.22448979591836735, 'Val_F1': 0.22448979591836735, 'Val_Accuracy': 0.8211764705882353, 'Val_AUC': 0.6190838037342596, 'Val_TP': 11, 'Val_TN': 338, 'Val_FP': 38, 'Val_FN': 38}\n",
      "\n",
      "--- right Fold 4 ---\n",
      "Epoch [1/120] ðŸ” Loss: 28.844610\n",
      "Epoch [2/120] ðŸ” Loss: 27.744263\n",
      "Epoch [3/120] ðŸ” Loss: 27.164456\n",
      "Epoch [4/120] ðŸ” Loss: 27.297529\n",
      "Epoch [5/120] ðŸ” Loss: 26.882039\n",
      "Epoch [6/120] ðŸ” Loss: 27.180116\n",
      "Epoch [7/120] ðŸ” Loss: 26.635898\n",
      "Epoch [8/120] ðŸ” Loss: 26.972477\n",
      "Epoch [9/120] ðŸ” Loss: 26.357120\n",
      "Epoch [10/120] ðŸ” Loss: 25.562530\n",
      "Epoch [11/120] ðŸ” Loss: 26.478197\n",
      "Epoch [12/120] ðŸ” Loss: 25.360495\n",
      "Epoch [13/120] ðŸ” Loss: 25.571859\n",
      "Epoch [14/120] ðŸ” Loss: 24.498048\n",
      "Epoch [15/120] ðŸ” Loss: 24.944313\n",
      "Epoch [16/120] ðŸ” Loss: 23.798421\n",
      "Epoch [17/120] ðŸ” Loss: 24.738982\n",
      "Epoch [18/120] ðŸ” Loss: 24.127094\n",
      "Epoch [19/120] ðŸ” Loss: 23.164725\n",
      "Epoch [20/120] ðŸ” Loss: 23.400215\n",
      "Epoch [21/120] ðŸ” Loss: 21.505547\n",
      "Epoch [22/120] ðŸ” Loss: 21.542122\n",
      "Epoch [23/120] ðŸ” Loss: 21.296324\n",
      "Epoch [24/120] ðŸ” Loss: 20.672411\n",
      "Epoch [25/120] ðŸ” Loss: 19.269789\n",
      "Epoch [26/120] ðŸ” Loss: 19.762875\n",
      "Epoch [27/120] ðŸ” Loss: 18.971072\n",
      "Epoch [28/120] ðŸ” Loss: 17.198403\n",
      "Epoch [29/120] ðŸ” Loss: 17.306281\n",
      "Epoch [30/120] ðŸ” Loss: 16.701404\n",
      "Epoch [31/120] ðŸ” Loss: 14.383628\n",
      "Epoch [32/120] ðŸ” Loss: 14.557021\n",
      "Epoch [33/120] ðŸ” Loss: 12.414654\n",
      "Epoch [34/120] ðŸ” Loss: 12.774570\n",
      "Epoch [35/120] ðŸ” Loss: 12.092781\n",
      "Epoch [36/120] ðŸ” Loss: 12.240349\n",
      "Epoch [37/120] ðŸ” Loss: 11.415818\n",
      "Epoch [38/120] ðŸ” Loss: 10.821136\n",
      "Epoch [39/120] ðŸ” Loss: 7.732012\n",
      "Epoch [40/120] ðŸ” Loss: 9.019016\n",
      "Epoch [41/120] ðŸ” Loss: 7.718498\n",
      "Epoch [42/120] ðŸ” Loss: 7.454631\n",
      "Epoch [43/120] ðŸ” Loss: 8.397795\n",
      "Epoch [44/120] ðŸ” Loss: 6.296318\n",
      "Epoch [45/120] ðŸ” Loss: 7.799702\n",
      "Epoch [46/120] ðŸ” Loss: 5.676052\n",
      "Epoch [47/120] ðŸ” Loss: 6.101072\n",
      "Epoch [48/120] ðŸ” Loss: 5.708448\n",
      "Epoch [49/120] ðŸ” Loss: 6.465551\n",
      "Epoch [50/120] ðŸ” Loss: 5.333578\n",
      "Epoch [51/120] ðŸ” Loss: 5.810275\n",
      "Epoch [52/120] ðŸ” Loss: 5.080071\n",
      "Epoch [53/120] ðŸ” Loss: 3.974139\n",
      "Epoch [54/120] ðŸ” Loss: 5.031784\n",
      "Epoch [55/120] ðŸ” Loss: 3.685532\n",
      "Epoch [56/120] ðŸ” Loss: 3.460735\n",
      "Epoch [57/120] ðŸ” Loss: 4.842938\n",
      "Epoch [58/120] ðŸ” Loss: 5.192502\n",
      "Epoch [59/120] ðŸ” Loss: 3.798827\n",
      "Epoch [60/120] ðŸ” Loss: 2.599697\n",
      "Epoch [61/120] ðŸ” Loss: 3.620757\n",
      "Epoch [62/120] ðŸ” Loss: 3.011585\n",
      "Epoch [63/120] ðŸ” Loss: 2.727975\n",
      "Epoch [64/120] ðŸ” Loss: 4.587310\n",
      "Epoch [65/120] ðŸ” Loss: 2.384300\n",
      "Epoch [66/120] ðŸ” Loss: 4.698406\n",
      "Epoch [67/120] ðŸ” Loss: 2.517896\n",
      "Epoch [68/120] ðŸ” Loss: 4.296872\n",
      "Epoch [69/120] ðŸ” Loss: 2.383348\n",
      "Epoch [70/120] ðŸ” Loss: 2.327387\n",
      "Epoch [71/120] ðŸ” Loss: 3.900234\n",
      "Epoch [72/120] ðŸ” Loss: 3.923519\n",
      "Epoch [73/120] ðŸ” Loss: 2.625963\n",
      "Epoch [74/120] ðŸ” Loss: 3.518098\n",
      "Epoch [75/120] ðŸ” Loss: 1.940283\n",
      "Epoch [76/120] ðŸ” Loss: 1.344691\n",
      "Epoch [77/120] ðŸ” Loss: 2.099348\n",
      "Epoch [78/120] ðŸ” Loss: 1.855507\n",
      "Epoch [79/120] ðŸ” Loss: 1.725969\n",
      "Epoch [80/120] ðŸ” Loss: 3.004718\n",
      "Epoch [81/120] ðŸ” Loss: 1.733521\n",
      "Epoch [82/120] ðŸ” Loss: 1.906768\n",
      "Epoch [83/120] ðŸ” Loss: 1.570303\n",
      "Epoch [84/120] ðŸ” Loss: 1.517208\n",
      "Epoch [85/120] ðŸ” Loss: 2.265928\n",
      "Epoch [86/120] ðŸ” Loss: 1.959989\n",
      "Epoch [87/120] ðŸ” Loss: 1.126513\n",
      "Epoch [88/120] ðŸ” Loss: 1.309270\n",
      "Epoch [89/120] ðŸ” Loss: 1.807012\n",
      "Epoch [90/120] ðŸ” Loss: 2.253512\n",
      "Epoch [91/120] ðŸ” Loss: 1.686322\n",
      "Epoch [92/120] ðŸ” Loss: 1.732530\n",
      "Epoch [93/120] ðŸ” Loss: 1.675742\n",
      "Epoch [94/120] ðŸ” Loss: 1.746832\n",
      "Epoch [95/120] ðŸ” Loss: 1.143731\n",
      "Epoch [96/120] ðŸ” Loss: 1.931330\n",
      "Epoch [97/120] ðŸ” Loss: 3.398926\n",
      "Epoch [98/120] ðŸ” Loss: 2.338790\n",
      "Epoch [99/120] ðŸ” Loss: 1.275424\n",
      "Epoch [100/120] ðŸ” Loss: 2.741272\n",
      "Epoch [101/120] ðŸ” Loss: 1.725940\n",
      "Epoch [102/120] ðŸ” Loss: 0.842618\n",
      "Epoch [103/120] ðŸ” Loss: 2.154245\n",
      "Epoch [104/120] ðŸ” Loss: 1.091241\n",
      "Epoch [105/120] ðŸ” Loss: 1.521538\n",
      "Epoch [106/120] ðŸ” Loss: 3.255609\n",
      "Epoch [107/120] ðŸ” Loss: 1.166661\n",
      "Epoch [108/120] ðŸ” Loss: 0.980739\n",
      "Epoch [109/120] ðŸ” Loss: 1.562657\n",
      "Epoch [110/120] ðŸ” Loss: 1.948285\n",
      "Epoch [111/120] ðŸ” Loss: 2.013327\n",
      "Epoch [112/120] ðŸ” Loss: 1.975281\n",
      "Epoch [113/120] ðŸ” Loss: 1.203609\n",
      "Epoch [114/120] ðŸ” Loss: 1.191896\n",
      "Epoch [115/120] ðŸ” Loss: 2.164013\n",
      "Epoch [116/120] ðŸ” Loss: 1.035487\n",
      "Epoch [117/120] ðŸ” Loss: 1.218605\n",
      "Epoch [118/120] ðŸ” Loss: 1.354641\n",
      "Epoch [119/120] ðŸ” Loss: 1.114969\n",
      "Epoch [120/120] ðŸ” Loss: 1.727214\n",
      "{'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Epochs': 120, 'BatchSize': 22, 'LearningRate': 0.00022, 'Val_Precision': 0.1724137931034483, 'Val_Recall': 0.10204081632653061, 'Val_F1': 0.12820512820512822, 'Val_Accuracy': 0.84, 'Val_AUC': 0.6537668258792879, 'Val_TP': 5, 'Val_TN': 352, 'Val_FP': 24, 'Val_FN': 44}\n",
      "\n",
      "--- right Fold 5 ---\n",
      "Epoch [1/120] ðŸ” Loss: 28.632850\n",
      "Epoch [2/120] ðŸ” Loss: 28.181135\n",
      "Epoch [3/120] ðŸ” Loss: 27.632017\n",
      "Epoch [4/120] ðŸ” Loss: 27.696839\n",
      "Epoch [5/120] ðŸ” Loss: 26.910924\n",
      "Epoch [6/120] ðŸ” Loss: 26.674092\n",
      "Epoch [7/120] ðŸ” Loss: 27.140006\n",
      "Epoch [8/120] ðŸ” Loss: 27.060383\n",
      "Epoch [9/120] ðŸ” Loss: 26.365610\n",
      "Epoch [10/120] ðŸ” Loss: 26.701342\n",
      "Epoch [11/120] ðŸ” Loss: 25.824447\n",
      "Epoch [12/120] ðŸ” Loss: 25.467452\n",
      "Epoch [13/120] ðŸ” Loss: 25.343770\n",
      "Epoch [14/120] ðŸ” Loss: 25.452147\n",
      "Epoch [15/120] ðŸ” Loss: 25.260860\n",
      "Epoch [16/120] ðŸ” Loss: 24.615505\n",
      "Epoch [17/120] ðŸ” Loss: 25.315377\n",
      "Epoch [18/120] ðŸ” Loss: 23.773101\n",
      "Epoch [19/120] ðŸ” Loss: 25.167752\n",
      "Epoch [20/120] ðŸ” Loss: 24.480636\n",
      "Epoch [21/120] ðŸ” Loss: 24.184679\n",
      "Epoch [22/120] ðŸ” Loss: 23.207260\n",
      "Epoch [23/120] ðŸ” Loss: 23.657356\n",
      "Epoch [24/120] ðŸ” Loss: 22.414471\n",
      "Epoch [25/120] ðŸ” Loss: 22.655724\n",
      "Epoch [26/120] ðŸ” Loss: 20.769446\n",
      "Epoch [27/120] ðŸ” Loss: 20.184850\n",
      "Epoch [28/120] ðŸ” Loss: 19.627984\n",
      "Epoch [29/120] ðŸ” Loss: 19.102251\n",
      "Epoch [30/120] ðŸ” Loss: 19.177281\n",
      "Epoch [31/120] ðŸ” Loss: 18.058338\n",
      "Epoch [32/120] ðŸ” Loss: 17.152874\n",
      "Epoch [33/120] ðŸ” Loss: 16.022280\n",
      "Epoch [34/120] ðŸ” Loss: 14.935650\n",
      "Epoch [35/120] ðŸ” Loss: 15.465293\n",
      "Epoch [36/120] ðŸ” Loss: 15.024073\n",
      "Epoch [37/120] ðŸ” Loss: 12.272359\n",
      "Epoch [38/120] ðŸ” Loss: 12.891340\n",
      "Epoch [39/120] ðŸ” Loss: 11.135284\n",
      "Epoch [40/120] ðŸ” Loss: 9.938602\n",
      "Epoch [41/120] ðŸ” Loss: 8.961470\n",
      "Epoch [42/120] ðŸ” Loss: 8.426318\n",
      "Epoch [43/120] ðŸ” Loss: 9.431770\n",
      "Epoch [44/120] ðŸ” Loss: 6.972627\n",
      "Epoch [45/120] ðŸ” Loss: 8.813637\n",
      "Epoch [46/120] ðŸ” Loss: 7.462540\n",
      "Epoch [47/120] ðŸ” Loss: 7.483388\n",
      "Epoch [48/120] ðŸ” Loss: 6.422353\n",
      "Epoch [49/120] ðŸ” Loss: 6.841923\n",
      "Epoch [50/120] ðŸ” Loss: 8.208539\n",
      "Epoch [51/120] ðŸ” Loss: 5.515037\n",
      "Epoch [52/120] ðŸ” Loss: 5.797216\n",
      "Epoch [53/120] ðŸ” Loss: 4.437734\n",
      "Epoch [54/120] ðŸ” Loss: 4.751684\n",
      "Epoch [55/120] ðŸ” Loss: 3.919669\n",
      "Epoch [56/120] ðŸ” Loss: 4.774456\n",
      "Epoch [57/120] ðŸ” Loss: 4.250998\n",
      "Epoch [58/120] ðŸ” Loss: 3.361564\n",
      "Epoch [59/120] ðŸ” Loss: 4.159959\n",
      "Epoch [60/120] ðŸ” Loss: 4.795220\n",
      "Epoch [61/120] ðŸ” Loss: 3.843642\n",
      "Epoch [62/120] ðŸ” Loss: 4.208064\n",
      "Epoch [63/120] ðŸ” Loss: 3.479301\n",
      "Epoch [64/120] ðŸ” Loss: 3.721025\n",
      "Epoch [65/120] ðŸ” Loss: 4.293780\n",
      "Epoch [66/120] ðŸ” Loss: 3.345432\n",
      "Epoch [67/120] ðŸ” Loss: 4.522402\n",
      "Epoch [68/120] ðŸ” Loss: 3.376610\n",
      "Epoch [69/120] ðŸ” Loss: 2.832077\n",
      "Epoch [70/120] ðŸ” Loss: 3.567511\n",
      "Epoch [71/120] ðŸ” Loss: 2.590742\n",
      "Epoch [72/120] ðŸ” Loss: 2.278654\n",
      "Epoch [73/120] ðŸ” Loss: 3.878010\n",
      "Epoch [74/120] ðŸ” Loss: 3.620013\n",
      "Epoch [75/120] ðŸ” Loss: 2.810563\n",
      "Epoch [76/120] ðŸ” Loss: 2.551487\n",
      "Epoch [77/120] ðŸ” Loss: 2.543794\n",
      "Epoch [78/120] ðŸ” Loss: 2.265147\n",
      "Epoch [79/120] ðŸ” Loss: 2.714119\n",
      "Epoch [80/120] ðŸ” Loss: 2.618081\n",
      "Epoch [81/120] ðŸ” Loss: 2.443248\n",
      "Epoch [82/120] ðŸ” Loss: 1.989671\n",
      "Epoch [83/120] ðŸ” Loss: 2.567592\n",
      "Epoch [84/120] ðŸ” Loss: 3.575573\n",
      "Epoch [85/120] ðŸ” Loss: 1.390558\n",
      "Epoch [86/120] ðŸ” Loss: 2.847010\n",
      "Epoch [87/120] ðŸ” Loss: 2.827873\n",
      "Epoch [88/120] ðŸ” Loss: 1.772842\n",
      "Epoch [89/120] ðŸ” Loss: 2.366407\n",
      "Epoch [90/120] ðŸ” Loss: 2.713560\n",
      "Epoch [91/120] ðŸ” Loss: 2.930468\n",
      "Epoch [92/120] ðŸ” Loss: 1.873424\n",
      "Epoch [93/120] ðŸ” Loss: 2.140289\n",
      "Epoch [94/120] ðŸ” Loss: 1.170489\n",
      "Epoch [95/120] ðŸ” Loss: 1.896376\n",
      "Epoch [96/120] ðŸ” Loss: 1.599927\n",
      "Epoch [97/120] ðŸ” Loss: 1.631247\n",
      "Epoch [98/120] ðŸ” Loss: 2.946073\n",
      "Epoch [99/120] ðŸ” Loss: 1.716730\n",
      "Epoch [100/120] ðŸ” Loss: 1.850937\n",
      "Epoch [101/120] ðŸ” Loss: 1.411709\n",
      "Epoch [102/120] ðŸ” Loss: 2.587392\n",
      "Epoch [103/120] ðŸ” Loss: 1.780157\n",
      "Epoch [104/120] ðŸ” Loss: 2.708126\n",
      "Epoch [105/120] ðŸ” Loss: 1.832894\n",
      "Epoch [106/120] ðŸ” Loss: 1.565431\n",
      "Epoch [107/120] ðŸ” Loss: 2.366228\n",
      "Epoch [108/120] ðŸ” Loss: 1.550769\n",
      "Epoch [109/120] ðŸ” Loss: 0.979679\n",
      "Epoch [110/120] ðŸ” Loss: 1.624376\n",
      "Epoch [111/120] ðŸ” Loss: 1.167747\n",
      "Epoch [112/120] ðŸ” Loss: 2.110958\n",
      "Epoch [113/120] ðŸ” Loss: 2.004769\n",
      "Epoch [114/120] ðŸ” Loss: 1.063682\n",
      "Epoch [115/120] ðŸ” Loss: 2.187340\n",
      "Epoch [116/120] ðŸ” Loss: 2.339762\n",
      "Epoch [117/120] ðŸ” Loss: 1.757336\n",
      "Epoch [118/120] ðŸ” Loss: 3.527230\n",
      "Epoch [119/120] ðŸ” Loss: 2.249337\n",
      "Epoch [120/120] ðŸ” Loss: 1.494683\n",
      "{'EyeSet': 'right', 'Resolution': 224, 'Fold': 5, 'Epochs': 120, 'BatchSize': 22, 'LearningRate': 0.00022, 'Val_Precision': 0.375, 'Val_Recall': 0.12244897959183673, 'Val_F1': 0.1846153846153846, 'Val_Accuracy': 0.8752941176470588, 'Val_AUC': 0.6051346070343031, 'Val_TP': 6, 'Val_TN': 366, 'Val_FP': 10, 'Val_FN': 43}\n",
      "\n",
      "ðŸ”Ž Selecting best fold by validation metrics (require Val P & R â‰¥ 0.90 if available)...\n",
      "âœ… Best fold = 3 | Val_P=0.224, Val_R=0.224, Val_F1=0.224, Val_AUC=0.619\n",
      "\n",
      "ðŸ“Š VALIDATION (best-fold model) Results\n",
      "Precision: 0.3000, Recall: 0.2250, F1: 0.2571\n",
      "Accuracy:  0.8539, AUC: 0.5962\n",
      "\n",
      "ðŸ“Š TEST (best-fold model) Results\n",
      "Precision: 0.3333, Recall: 0.1739, F1: 0.2286\n",
      "Accuracy:  0.8444, AUC: 0.6432\n",
      "\n",
      "âœ… tongue: Done. All discovery, 5-fold CV, model selection, and TEST evaluation are reproducible.\n",
      "\n",
      "ðŸš€ Starting final TFLite export for Tri tongue Eye model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 07:25:30.332678: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-14 07:25:30.334254: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-14 07:25:30.368451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-14 07:25:30.949267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 07:25:34.407536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-10-14 07:25:34.409164: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_187_x, onnx_tf_prefix__fc_fc_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n",
      "   âŒ TensorFlow â†’ TFLite failed: <unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.2/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\n",
      "<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n",
      "<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.2/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
      "<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.6/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\n",
      "<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n",
      "<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.6/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
      "<unknown>:0: error: failed while converting: 'main': \n",
      "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
      "TF Select ops: Erf\n",
      "Details:\n",
      "\ttf.Erf(tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<?x512xf32>) -> (tensor<?x512xf32>) : {device = \"\"}\n",
      "\n",
      "\n",
      "âœ… Tri tongue Eye TFLite conversion pipeline completed successfully.\n",
      "\n",
      "âœ… right: Done. All discovery, 5-fold CV, model selection, and TEST evaluation are reproducible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 07:25:47.686311: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-10-14 07:25:47.686358: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-10-14 07:25:47.688468: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model\n",
      "2025-10-14 07:25:47.800526: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-10-14 07:25:47.800567: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model\n",
      "2025-10-14 07:25:47.914854: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-10-14 07:25:47.917009: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-10-14 07:25:48.110099: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_tongue_hb_90_repro_bestfold_only/tri_tongue_tf_model\n",
      "2025-10-14 07:25:48.176035: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 487574 microseconds.\n",
      "2025-10-14 07:25:48.446191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.2/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"Erf:\", \"onnx_tf_prefix_/fc/fc.6/Erf@__inference___call___1631\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_1899\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\n",
      "error: failed while converting: 'main': \n",
      "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
      "TF Select ops: Erf\n",
      "Details:\n",
      "\ttf.Erf(tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<?x512xf32>) -> (tensor<?x512xf32>) : {device = \"\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-right-Eye ResNet18 (5-fold CV -> pick best fold -> TEST)\n",
    "------------------------------------------------------------------------\n",
    "Determinism safeguards:\n",
    "- Fixed global seed (random, numpy, torch, CUDA)\n",
    "- cuDNN deterministic + disable benchmarking\n",
    "- torch.use_deterministic_algorithms(True)\n",
    "- Optional: export PYTHONHASHSEED=42 before launching Python\n",
    "- OpenCV single-threaded I/O\n",
    "- Sorted directory listings and tri-eye base-ID intersections\n",
    "- StratifiedKFold with fixed random_state\n",
    "- DataLoader generator seeded; NUM_WORKERS = 0 (strict determinism)\n",
    "- Deterministic transforms (no randomness beyond our fixed Python RNG)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# GLOBAL CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0          # keep 0 for strict reproducibility\n",
    "PIN_MEMORY = False       # can be True on CUDA; doesn't affect determinism\n",
    "USE_AMP = True           # AMP is deterministic under fixed seeds + deterministic algos\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTIONS = [224]\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 22\n",
    "LR_CV = 0.00022\n",
    "\n",
    "'''\n",
    "resolutions = [224]\n",
    "EPOCHS_CV = 120\n",
    "EPOCHS_FINAL = EPOCHS_CV\n",
    "BATCH_CV = 22\n",
    "LR_CV = 0.00022\n",
    "'''\n",
    "\n",
    "# =========================\n",
    "# DEVICE + DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # ban nondeterministic kernels; warn_only True to avoid hard-crash if a path isn't supported\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        # keep matmul & TF32 paths consistent\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)  # single-threaded OpenCV\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS (edit base_path if needed)\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "output_dir = os.path.join(base_path, \"tri_tongue_hb_90_repro_bestfold_only\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'right1': 'tri_tongue/tongue_1_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_tongue/tongue_2_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_tongue/tongue_3_hb_less_than_9_0/tongue_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "# =========================\n",
    "# UTIL (tri-right intersection, loading, logging)\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    if not fname.endswith(suffix):\n",
    "        return None\n",
    "    return fname[: -len(suffix)]\n",
    "\n",
    "def common_bases_right(dirs_map):\n",
    "    \"\"\"\n",
    "    Intersection of base_ids for right1-right2-right3 (sorted).\n",
    "    \"\"\"\n",
    "    suffixes = {\n",
    "        'right1': '_tongue_1.png',\n",
    "        'right2': '_tongue_2.png',\n",
    "        'right3': '_tongue_3.png',\n",
    "    }\n",
    "    bases_sets = []\n",
    "    for k in ['right1', 'right2', 'right3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"âš ï¸ Folder missing: {folder}\")\n",
    "            return []\n",
    "        names = sorted([f for f in os.listdir(folder) if f.endswith(suffixes[k])])\n",
    "        bases = set()\n",
    "        for f in names:\n",
    "            b = base_from(f, suffixes[k])\n",
    "            if b:\n",
    "                bases.add(b)\n",
    "        bases_sets.append(bases)\n",
    "    common = set.intersection(*bases_sets) if bases_sets else set()\n",
    "    return sorted(list(common))\n",
    "\n",
    "def load_tri_images_by_bases(dirs_map, bases):\n",
    "    \"\"\"\n",
    "    Load the three right-eye images per base (RGB).\n",
    "    \"\"\"\n",
    "    out = {'r1': [], 'r2': [], 'r3': []}\n",
    "    key_map = {\n",
    "        'r1': ('right1', '_tongue_1.png'),\n",
    "        'r2': ('right2', '_tongue_2.png'),\n",
    "        'r3': ('right3', '_tongue_3.png'),\n",
    "    }\n",
    "    ok = 0\n",
    "    for b in bases:\n",
    "        imgs = {}\n",
    "        failed = False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if failed:\n",
    "            continue\n",
    "        for k in out.keys():\n",
    "            out[k].append(imgs[k])\n",
    "        ok += 1\n",
    "    return out, ok\n",
    "\n",
    "def prepare_dataset_right(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    \"\"\"\n",
    "    Enforce strict tri-right-eye match per class by intersecting base ids across the three right-eye folders.\n",
    "    \"\"\"\n",
    "    bases_anemic = common_bases_right(anemic_dirs)\n",
    "    bases_non = common_bases_right(non_dirs)\n",
    "\n",
    "    print(f\"\\nðŸ”Ž {split_name} (right) - discovered bases: anemic={len(bases_anemic)}, non-anemic={len(bases_non)}\")\n",
    "\n",
    "    imgs_a, ca = load_tri_images_by_bases(anemic_dirs, bases_anemic)\n",
    "    imgs_n, cn = load_tri_images_by_bases(non_dirs,  bases_non)\n",
    "\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "\n",
    "    print(f\"âœ… {split_name}: tri-matched right samples -> anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "\n",
    "    # Save used base IDs (audit reproducibility)\n",
    "    used_bases_df = pd.DataFrame({\n",
    "        'class': (['anemic']*len(bases_anemic)) + (['non_anemic']*len(bases_non)),\n",
    "        'base_id': bases_anemic + bases_non\n",
    "    })\n",
    "    used_bases_df.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_right.csv\"), index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in ['right1','right2','right3']:\n",
    "        p = dirs_map[k]\n",
    "        c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# BEFORE MODEL: print raw file counts per dir\n",
    "# =========================\n",
    "print_dir_stats(\"TRAIN anemic (right)\", train_dirs_anemic)\n",
    "print_dir_stats(\"TRAIN non-anemic (right)\", train_dirs_non)\n",
    "print_dir_stats(\"VAL   anemic (right)\", val_dirs_anemic)\n",
    "print_dir_stats(\"VAL   non-anemic (right)\", val_dirs_non)\n",
    "print_dir_stats(\"TEST  anemic (right)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (right)\", test_dirs_non)\n",
    "\n",
    "# =========================\n",
    "# BUILD STRICT TRI-right DATASETS (REPRODUCIBLE)\n",
    "# =========================\n",
    "train_data = prepare_dataset_right(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_right(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_right(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-right-eye TRAIN samples found. Check paths/filenames.\")\n",
    "if len(val_data['label']) == 0:\n",
    "    print(\"âš ï¸ No tri-right-eye VAL samples found. We'll still run TEST eval for best fold if available.\")\n",
    "if len(test_data['label']) == 0:\n",
    "    print(\"âš ï¸ No tri-right-eye TEST samples found. Final test evaluation will be skipped.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET / DATALOADER\n",
    "# =========================\n",
    "class TrirightDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL (3x ResNet18 + MLP head)\n",
    "# =========================\n",
    "class TriResNetright(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def res():\n",
    "            m = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "            m.fc = nn.Identity()\n",
    "            return m\n",
    "        self.models = nn.ModuleList([res() for _ in range(3)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3*512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, *x):\n",
    "        feats = [model(xi) for model, xi in zip(self.models, x)]\n",
    "        x = torch.cat(feats, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(*imgs)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        p = r = f1 = auc = float('nan')\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        tn = fp = fn = tp = 0\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return p, r, f1, acc, auc, tp, tn, fp, fn\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-tongue-Eye ResNet18 (5-fold CV -> pick best fold -> TEST -> TFLite)\n",
    "---------------------------------------------------------------------\n",
    "Fully deterministic + converts best fold model to ONNX â†’ TensorFlow â†’ TFLite.\n",
    "\"\"\"\n",
    "\n",
    "# (All your imports, global config, dataset code, and training sections remain unchanged)\n",
    "\n",
    "# =========================\n",
    "# âœ… TFLITE CONVERSION FUNCTION (fixed)\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int):\n",
    "    \"\"\"\n",
    "    Converts the tri-tongue-eye PyTorch model to TFLite via ONNX and TensorFlow.\n",
    "    Input: three (B,3,H,W) tensors â†’ Output: (B,1)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    onnx_path = os.path.join(output_dir, \"tri_tongue_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_tongue_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"tri_tongue_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1ï¸âƒ£ PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    best_model.eval().to(device)\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution, device=device) for _ in range(3))\n",
    "\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input1': {0: 'batch'},\n",
    "                          'input2': {0: 'batch'},\n",
    "                          'input3': {0: 'batch'},\n",
    "                          'output': {0: 'batch'}}\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch â†’ ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 2ï¸âƒ£ ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX â†’ TensorFlow conversion failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3ï¸âƒ£ TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        # Optional: quantization for smaller model size\n",
    "        # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   ðŸ“¦ Size: {os.path.getsize(tflite_path)/(1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow â†’ TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TRAIN / VALIDATE via 5-fold CV (on TRAIN only)\n",
    "# =========================\n",
    "results = []\n",
    "\n",
    "for resolution in RESOLUTIONS:\n",
    "    print(f\"\\n===== Processing right resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    if len(train_data['label']) < N_SPLITS:\n",
    "        raise RuntimeError(f\"Too few right train samples for {N_SPLITS}-fold CV.\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    fold = 1\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    cv_index_records = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- right Fold {fold} ---\")\n",
    "\n",
    "        cv_index_records.append({\n",
    "            \"fold\": fold,\n",
    "            \"train_indices\": train_idx.tolist(),\n",
    "            \"val_indices\": val_idx.tolist()\n",
    "        })\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrirightDataset(train_subset, train_transform), batch_size=BATCH_CV, shuffle=True)\n",
    "        val_loader   = make_loader(TrirightDataset(val_subset,   test_transform),  batch_size=BATCH_CV, shuffle=False)\n",
    "\n",
    "        model = TriResNetright().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                imgs = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(*imgs)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] ðŸ” Loss: {total_loss:.6f}\")\n",
    "\n",
    "        # fold VAL metrics\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        result_row = {\n",
    "            'EyeSet': 'right',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Epochs': EPOCHS_CV,\n",
    "            'BatchSize': BATCH_CV,\n",
    "            'LearningRate': LR_CV,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(result_row)\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"right_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'seed': SEED,\n",
    "                        'resolution': resolution}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # save CV tables/indices\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(os.path.join(output_dir, f\"{resolution}_right_val_cross_validation_results.csv\"), index=False)\n",
    "\n",
    "    pd.DataFrame(cv_index_records).to_json(os.path.join(output_dir, f\"{resolution}_right_cv_indices.json\"),\n",
    "                                           orient='records', indent=2)\n",
    "\n",
    "    # =========================\n",
    "    # SELECT BEST FOLD & EVALUATE ON TEST (no retraining)\n",
    "    # =========================\n",
    "    print(\"\\nðŸ”Ž Selecting best fold by validation metrics (require Val P & R â‰¥ 0.90 if available)...\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['min_PR'] = results_df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "\n",
    "    # candidates meeting target\n",
    "    candidates = results_df[(results_df['Val_Precision'] >= 0.90) &\n",
    "                            (results_df['Val_Recall']    >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        # strongest among those meeting the threshold\n",
    "        best = candidates.sort_values(['Val_F1','Val_AUC','min_PR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        # fallback: best compromise (highest min(PR), then F1, then AUC)\n",
    "        best = results_df.sort_values(['min_PR','Val_F1','Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | \"\n",
    "          f\"Val_P={best['Val_Precision']:.3f}, Val_R={best['Val_Recall']:.3f}, \"\n",
    "          f\"Val_F1={best['Val_F1']:.3f}, Val_AUC={best['Val_AUC']:.3f}\")\n",
    "\n",
    "    # persist selection\n",
    "    with open(os.path.join(output_dir, f\"{resolution}_right_best_fold.txt\"), \"w\") as f:\n",
    "        f.write(str(best_fold))\n",
    "\n",
    "    # load its checkpoint\n",
    "    ckpt_path = os.path.join(output_dir, f\"right_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model = TriResNetright().to(device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # build loaders for VAL (optional audit) and TEST with identical batch size\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Optional: evaluate chosen model on the *held-out* configured VAL split too (for auditing)\n",
    "    if len(val_data['label']) > 0:\n",
    "        chosen_val_loader = make_loader(TrirightDataset(val_data, test_transform),\n",
    "                                        batch_size=BATCH_CV, shuffle=False)\n",
    "        chosen_val_metrics = evaluate(best_model, chosen_val_loader)\n",
    "        print(\"\\nðŸ“Š VALIDATION (best-fold model) Results\")\n",
    "        print(f\"Precision: {chosen_val_metrics[0]:.4f}, Recall: {chosen_val_metrics[1]:.4f}, F1: {chosen_val_metrics[2]:.4f}\")\n",
    "        print(f\"Accuracy:  {chosen_val_metrics[3]:.4f}, AUC: {chosen_val_metrics[4]:.4f}\")\n",
    "        pd.DataFrame([{\n",
    "            'ChosenFold': best_fold,\n",
    "            'Val_Precision': chosen_val_metrics[0], 'Val_Recall': chosen_val_metrics[1], 'Val_F1': chosen_val_metrics[2],\n",
    "            'Val_Accuracy': chosen_val_metrics[3], 'Val_AUC': chosen_val_metrics[4], 'Val_TP': chosen_val_metrics[5],\n",
    "            'Val_TN': chosen_val_metrics[6], 'Val_FP': chosen_val_metrics[7], 'Val_FN': chosen_val_metrics[8]\n",
    "        }]).to_csv(os.path.join(output_dir, f\"{resolution}_right_bestfold_val_results.csv\"), index=False)\n",
    "\n",
    "    # TEST evaluation with the best-fold model\n",
    "    if len(test_data['label']) > 0:\n",
    "        test_loader = make_loader(TrirightDataset(test_data, test_transform),\n",
    "                                  batch_size=BATCH_CV, shuffle=False)\n",
    "        test_metrics = evaluate(best_model, test_loader)\n",
    "        print(\"\\nðŸ“Š TEST (best-fold model) Results\")\n",
    "        print(f\"Precision: {test_metrics[0]:.4f}, Recall: {test_metrics[1]:.4f}, F1: {test_metrics[2]:.4f}\")\n",
    "        print(f\"Accuracy:  {test_metrics[3]:.4f}, AUC: {test_metrics[4]:.4f}\")\n",
    "\n",
    "        pd.DataFrame([{\n",
    "            'ChosenFold': best_fold,\n",
    "            'Test_Precision': test_metrics[0], 'Test_Recall': test_metrics[1], 'Test_F1': test_metrics[2],\n",
    "            'Test_Accuracy': test_metrics[3], 'Test_AUC': test_metrics[4], 'Test_TP': test_metrics[5],\n",
    "            'Test_TN': test_metrics[6], 'Test_FP': test_metrics[7], 'Test_FN': test_metrics[8]\n",
    "        }]).to_csv(os.path.join(output_dir, f\"{resolution}_right_bestfold_test_results.csv\"), index=False)\n",
    "    print(\"\\nâœ… tongue: Done. All discovery, 5-fold CV, model selection, and TEST evaluation are reproducible.\")\n",
    "    print(\"\\nðŸš€ Starting final TFLite export for Tri tongue Eye model...\")\n",
    "    try:\n",
    "        convert_to_tflite(best_model, output_dir, resolution)\n",
    "        print(\"âœ… Tri tongue Eye TFLite conversion pipeline completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion pipeline failed: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nâœ… right: Done. All discovery, 5-fold CV, model selection, and TEST evaluation are reproducible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd538dd-4fe5-4364-a1c9-b14160e0cb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897386f-98fb-4e28-a6ad-bf97a43abf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dfb6b-2738-4db9-91e9-469ee4e1c278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2e721-17f9-4f48-a43e-86343602117f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed2d79-6a3a-447c-b0b2-ca63aef0da65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f726824-a3f6-4e61-9d8b-23706b118300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03279c7-0587-498c-8a87-bb2b4704ed4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8acae-dd46-4267-87ee-2221ebca7fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd620ad-664f-4451-a11d-8c7a879d9e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048dfbf-edc8-47af-8517-b595cbed8fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd75e3d-7baf-494d-8498-2b4e79f5b453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386f613-b525-4106-bf74-269f0879c3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec59256-46d2-40c1-8966-deb1d390551d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
