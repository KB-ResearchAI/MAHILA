{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b859cf28-732a-4c37-b877-c17fc3dfbe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/lib/python3/dist-packages (from seaborn) (1.3.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce7a2038-f481-4431-833c-df4e023ea7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for leakage by FULL FILENAME...\n",
      "\n",
      "âœ… No leakage â€” all filenames are unique to each split.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "\n",
    "ROOT = os.path.join(BASE_PATH, DATA_DIR)\n",
    "\n",
    "FOLDERS = {\n",
    "    \"train\": [\"anemic_train_roi\", \"anemic_not_train_roi\"],\n",
    "    \"val\":   [\"anemic_val_roi\", \"anemic_not_val_roi\"],\n",
    "    \"test\":  [\"anemic_test_roi\", \"anemic_not_test_roi\"]\n",
    "}\n",
    "\n",
    "# filename â†’ list of (split, folder_path)\n",
    "file_locations = defaultdict(list)\n",
    "\n",
    "# Scan folders\n",
    "for split, subfolders in FOLDERS.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(ROOT, sub)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                file_locations[f].append((split, os.path.join(sub, f)))\n",
    "\n",
    "# Detect leakage if SAME FILENAME is present in >1 split\n",
    "leaks_exist = False\n",
    "print(\"\\nChecking for leakage by FULL FILENAME...\\n\")\n",
    "\n",
    "for filename, locations in file_locations.items():\n",
    "    splits = {loc[0] for loc in locations}\n",
    "    if len(splits) > 1:\n",
    "        leaks_exist = True\n",
    "        print(f\"âŒ Leakage detected: {filename}\")\n",
    "        for split, path in locations:\n",
    "            print(f\"   â†’ {split}: {path}\")\n",
    "        print()\n",
    "\n",
    "if not leaks_exist:\n",
    "    print(\"âœ… No leakage â€” all filenames are unique to each split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45241e3e-7b5f-481e-b31e-1bbc8f6e6cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 44 anemic / 358 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "âœ… Early stop at epoch 19: P=0.980, R=0.941\n",
      "Fold 1 â†’ P=0.980, R=0.941\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/200 Loss: 5.0647\n",
      "âœ… Early stop at epoch 23: P=0.907, R=0.961\n",
      "Fold 2 â†’ P=0.907, R=0.961\n",
      "\n",
      "--- Fold 3 ---\n",
      "âœ… Early stop at epoch 16: P=1.000, R=0.961\n",
      "Fold 3 â†’ P=1.000, R=0.961\n",
      "\n",
      "--- Fold 4 ---\n",
      "âœ… Early stop at epoch 15: P=0.978, R=0.900\n",
      "Fold 4 â†’ P=0.978, R=0.900\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/200 Loss: 8.0869\n",
      "Epoch 40/200 Loss: 0.7303\n",
      "Epoch 60/200 Loss: 0.2089\n",
      "âœ… Early stop at epoch 71: P=0.938, R=0.900\n",
      "Fold 5 â†’ P=0.938, R=0.900\n",
      "âœ… Best fold = 3 | P=1.000, R=0.961\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9773\n",
      "F1 score:  0.9885\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 43, 314, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 20:54:57.788911: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 20:54:57.790196: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-16 20:54:57.816754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 20:54:58.335766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 20:55:00.014415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-16 20:55:00.016141: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 20:55:04.743024: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-16 20:55:04.743053: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-16 20:55:04.745198: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-16 20:55:04.775211: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-16 20:55:04.775230: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-16 20:55:04.806717: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-11-16 20:55:04.807384: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-16 20:55:04.869682: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-16 20:55:04.890885: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 145692 microseconds.\n",
      "2025-11-16 20:55:04.979643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-16 20:55:05.251141: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9773\n",
      "F1 score:  0.9885\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 43, 314, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.9773, AUC: 1.0000\n",
      "TFLite  â†’ P: 1.0000, R: 0.9773, AUC: 1.0000\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_1_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 200\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00012\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28b3638-4ec5-49ac-9f91-e7160a4c0a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 40 anemic / 369 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "âœ… Early stop at epoch 14: P=0.922, R=0.904\n",
      "Fold 1 â†’ P=0.922, R=0.904\n",
      "\n",
      "--- Fold 2 ---\n",
      "âœ… Early stop at epoch 16: P=0.906, R=0.923\n",
      "Fold 2 â†’ P=0.906, R=0.923\n",
      "\n",
      "--- Fold 3 ---\n",
      "âœ… Early stop at epoch 15: P=0.939, R=0.902\n",
      "Fold 3 â†’ P=0.939, R=0.902\n",
      "\n",
      "--- Fold 4 ---\n",
      "âœ… Early stop at epoch 18: P=0.961, R=0.961\n",
      "Fold 4 â†’ P=0.961, R=0.961\n",
      "\n",
      "--- Fold 5 ---\n",
      "âœ… Early stop at epoch 14: P=0.980, R=0.961\n",
      "Fold 5 â†’ P=0.980, R=0.961\n",
      "âœ… Best fold = 5 | P=0.980, R=0.961\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9750\n",
      "F1 score:  0.9873\n",
      "Accuracy:  0.9973\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 39, 329, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 22:07:58.125813: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 22:07:58.127257: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-16 22:07:58.156945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 22:07:58.690282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 22:08:00.486126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-16 22:08:00.487969: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 22:08:05.711881: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-16 22:08:05.711921: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-16 22:08:05.714150: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:08:05.757335: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-16 22:08:05.757360: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:08:05.792591: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-11-16 22:08:05.793436: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-16 22:08:05.861199: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:08:05.882263: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 168123 microseconds.\n",
      "2025-11-16 22:08:05.979945: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-16 22:08:06.248955: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9750\n",
      "F1 score:  0.9873\n",
      "Accuracy:  0.9973\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 39, 329, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.9750, AUC: 1.0000\n",
      "TFLite  â†’ P: 1.0000, R: 0.9750, AUC: 1.0000\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_2_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"RIGHT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11605ca5-acf4-4c01-827c-23870d93b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 42 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "âœ… Early stop at epoch 9: P=0.939, R=0.902\n",
      "Fold 1 â†’ P=0.939, R=0.902\n",
      "\n",
      "--- Fold 2 ---\n",
      "âœ… Early stop at epoch 7: P=0.962, R=0.980\n",
      "Fold 2 â†’ P=0.962, R=0.980\n",
      "\n",
      "--- Fold 3 ---\n",
      "âœ… Early stop at epoch 3: P=0.926, R=0.980\n",
      "Fold 3 â†’ P=0.926, R=0.980\n",
      "\n",
      "--- Fold 4 ---\n",
      "âœ… Early stop at epoch 4: P=0.960, R=0.941\n",
      "Fold 4 â†’ P=0.960, R=0.941\n",
      "\n",
      "--- Fold 5 ---\n",
      "âœ… Early stop at epoch 7: P=0.980, R=0.980\n",
      "Fold 5 â†’ P=0.980, R=0.980\n",
      "âœ… Best fold = 5 | P=0.980, R=0.980\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9762\n",
      "F1 score:  0.9880\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 41, 319, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 22:25:10.571445: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-16 22:25:10.571478: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-16 22:25:10.573253: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:25:10.605275: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-16 22:25:10.605295: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:25:10.629608: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-16 22:25:10.691337: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-16 22:25:10.711632: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 138382 microseconds.\n",
      "2025-11-16 22:25:11.029795: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9762\n",
      "F1 score:  0.9880\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 41, 319, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.9762, AUC: 1.0000\n",
      "TFLite  â†’ P: 1.0000, R: 0.9762, AUC: 1.0000\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_2_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 25\n",
    "BATCH_CV = 8\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"LEFT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20d34e9-f2a8-4161-bdae-51f120ff852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 40 anemic / 368 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 17.4501\n",
      "Epoch 40/120 Loss: 7.7666\n",
      "âœ… Early stop at epoch 59: P=0.958, R=0.902\n",
      "Fold 1 â†’ P=0.958, R=0.902\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 17.7057\n",
      "Epoch 40/120 Loss: 12.1958\n",
      "âœ… Early stop at epoch 53: P=0.902, R=0.902\n",
      "Fold 2 â†’ P=0.902, R=0.902\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 17.7456\n",
      "Epoch 40/120 Loss: 13.1826\n",
      "Epoch 60/120 Loss: 7.5461\n",
      "Epoch 80/120 Loss: 3.9320\n",
      "Epoch 100/120 Loss: 2.0900\n",
      "âœ… Early stop at epoch 113: P=0.922, R=0.922\n",
      "Fold 3 â†’ P=0.922, R=0.922\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 17.4451\n",
      "Epoch 40/120 Loss: 12.7434\n",
      "Epoch 60/120 Loss: 7.0670\n",
      "Epoch 80/120 Loss: 3.5584\n",
      "Epoch 100/120 Loss: 1.8323\n",
      "Epoch 120/120 Loss: 0.9650\n",
      "Fold 4 â†’ P=1.000, R=0.804\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 17.3121\n",
      "Epoch 40/120 Loss: 12.2740\n",
      "Epoch 60/120 Loss: 6.6113\n",
      "Epoch 80/120 Loss: 3.2226\n",
      "Epoch 100/120 Loss: 1.7944\n",
      "âœ… Early stop at epoch 113: P=0.958, R=0.902\n",
      "Fold 5 â†’ P=0.958, R=0.902\n",
      "âœ… Best fold = 5 | P=0.958, R=0.902\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.8750\n",
      "F1 score:  0.9333\n",
      "Accuracy:  0.9864\n",
      "AUC:       0.9987\n",
      "TP, TN, FP, FN: 35, 328, 0, 5\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:46:06.240110: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-17 01:46:06.240144: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-17 01:46:06.242346: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:46:06.273307: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-17 01:46:06.273331: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:46:06.303686: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-17 01:46:06.366454: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:46:06.387801: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 145460 microseconds.\n",
      "2025-11-17 01:46:06.744113: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.8750\n",
      "F1 score:  0.9333\n",
      "Accuracy:  0.9864\n",
      "AUC:       0.9987\n",
      "TP, TN, FP, FN: 35, 328, 0, 5\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.8750, AUC: 0.9987\n",
      "TFLite  â†’ P: 1.0000, R: 0.8750, AUC: 0.9987\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_3_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 32\n",
    "LR_CV = 0.000003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"RIGHT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77655a89-0d21-449a-8d7d-8d6af0325a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 43 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "âœ… Early stop at epoch 3: P=0.942, R=0.980\n",
      "Fold 1 â†’ P=0.942, R=0.980\n",
      "\n",
      "--- Fold 2 ---\n",
      "âœ… Early stop at epoch 2: P=1.000, R=0.980\n",
      "Fold 2 â†’ P=1.000, R=0.980\n",
      "\n",
      "--- Fold 3 ---\n",
      "âœ… Early stop at epoch 3: P=0.962, R=1.000\n",
      "Fold 3 â†’ P=0.962, R=1.000\n",
      "\n",
      "--- Fold 4 ---\n",
      "âœ… Early stop at epoch 4: P=1.000, R=0.960\n",
      "Fold 4 â†’ P=1.000, R=0.960\n",
      "\n",
      "--- Fold 5 ---\n",
      "âœ… Early stop at epoch 2: P=0.980, R=1.000\n",
      "Fold 5 â†’ P=0.980, R=1.000\n",
      "âœ… Best fold = 5 | P=0.980, R=1.000\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9767\n",
      "F1 score:  0.9882\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 42, 318, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:55:54.387703: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-17 01:55:54.387739: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-17 01:55:54.389450: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:55:54.423972: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-17 01:55:54.424005: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:55:54.435965: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-17 01:55:54.461155: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-17 01:55:54.484318: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 94872 microseconds.\n",
      "2025-11-17 01:55:54.796728: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.9767\n",
      "F1 score:  0.9882\n",
      "Accuracy:  0.9972\n",
      "AUC:       1.0000\n",
      "TP, TN, FP, FN: 42, 318, 0, 1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.9767, AUC: 1.0000\n",
      "TFLite  â†’ P: 1.0000, R: 0.9767, AUC: 1.0000\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/LEFT_EYE_3_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 20\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"LEFT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6214a4-36c2-4f9b-a900-c7725d12c929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da9d35f-3d07-4b7c-9481-a8eb72023742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 43 anemic / 365 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/220 Loss: 11.2012\n",
      "Epoch 40/220 Loss: 1.9405\n",
      "Epoch 60/220 Loss: 1.0357\n",
      "Epoch 80/220 Loss: 1.3969\n",
      "Epoch 100/220 Loss: 0.6448\n",
      "Epoch 120/220 Loss: 1.7892\n",
      "Epoch 140/220 Loss: 0.0969\n",
      "Epoch 160/220 Loss: 0.2910\n",
      "Epoch 180/220 Loss: 0.1500\n",
      "Epoch 200/220 Loss: 0.4321\n",
      "Epoch 220/220 Loss: 0.0362\n",
      "Fold 1 â†’ P=0.605, R=0.510\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/220 Loss: 10.9389\n",
      "Epoch 40/220 Loss: 2.5949\n",
      "Epoch 60/220 Loss: 1.4460\n",
      "Epoch 80/220 Loss: 0.8656\n",
      "Epoch 100/220 Loss: 1.3135\n",
      "Epoch 120/220 Loss: 0.9017\n",
      "Epoch 140/220 Loss: 2.0926\n",
      "Epoch 160/220 Loss: 0.0310\n",
      "Epoch 180/220 Loss: 0.6605\n",
      "Epoch 200/220 Loss: 0.0778\n",
      "Epoch 220/220 Loss: 0.1675\n",
      "Fold 2 â†’ P=0.585, R=0.471\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/220 Loss: 11.9621\n",
      "Epoch 40/220 Loss: 1.8696\n",
      "Epoch 60/220 Loss: 1.8537\n",
      "Epoch 80/220 Loss: 0.6768\n",
      "Epoch 100/220 Loss: 0.7576\n",
      "Epoch 120/220 Loss: 1.0454\n",
      "Epoch 140/220 Loss: 0.4913\n",
      "Epoch 160/220 Loss: 0.0689\n",
      "Epoch 180/220 Loss: 0.2809\n",
      "Epoch 200/220 Loss: 0.0730\n",
      "Epoch 220/220 Loss: 0.4560\n",
      "Fold 3 â†’ P=0.735, R=0.490\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/220 Loss: 12.5280\n",
      "Epoch 40/220 Loss: 2.3681\n",
      "Epoch 60/220 Loss: 2.3368\n",
      "Epoch 80/220 Loss: 1.0641\n",
      "Epoch 100/220 Loss: 1.4527\n",
      "Epoch 120/220 Loss: 0.3460\n",
      "Epoch 140/220 Loss: 0.9700\n",
      "Epoch 160/220 Loss: 0.6270\n",
      "Epoch 180/220 Loss: 0.2230\n",
      "Epoch 200/220 Loss: 1.5122\n",
      "Epoch 220/220 Loss: 0.0457\n",
      "Fold 4 â†’ P=0.583, R=0.549\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/220 Loss: 10.6545\n",
      "Epoch 40/220 Loss: 1.9783\n",
      "Epoch 60/220 Loss: 1.5864\n",
      "Epoch 80/220 Loss: 1.7800\n",
      "Epoch 100/220 Loss: 1.1856\n",
      "Epoch 120/220 Loss: 0.1485\n",
      "Epoch 140/220 Loss: 0.1469\n",
      "Epoch 160/220 Loss: 0.1437\n",
      "Epoch 180/220 Loss: 0.1021\n",
      "Epoch 200/220 Loss: 0.3210\n",
      "Epoch 220/220 Loss: 0.0321\n",
      "Fold 5 â†’ P=0.534, R=0.620\n",
      "âœ… Best fold = 4 | P=0.583, R=0.549\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.6500\n",
      "Recall:    0.3023\n",
      "F1 score:  0.4127\n",
      "Accuracy:  0.8986\n",
      "AUC:       0.8706\n",
      "TP, TN, FP, FN: 13, 315, 7, 30\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 00:11:52.786519: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 00:11:52.787907: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-03 00:11:52.815424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-03 00:11:53.477639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 00:11:55.751295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-03 00:11:55.752917: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 00:12:00.948947: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-03 00:12:00.948985: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-03 00:12:00.952196: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model\n",
      "2025-12-03 00:12:00.989071: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-03 00:12:00.989101: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model\n",
      "2025-12-03 00:12:01.023099: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-12-03 00:12:01.024059: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-03 00:12:01.090297: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/tf_model\n",
      "2025-12-03 00:12:01.110735: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 158544 microseconds.\n",
      "2025-12-03 00:12:01.201989: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-03 00:12:01.466878: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 75.905 G  ops, equivalently 37.952 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [   1    3 1024 1024]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.6500\n",
      "Recall:    0.3023\n",
      "F1 score:  0.4127\n",
      "Accuracy:  0.8986\n",
      "AUC:       0.8706\n",
      "TP, TN, FP, FN: 13, 315, 7, 30\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.6500, R: 0.3023, AUC: 0.8706\n",
      "TFLite  â†’ P: 0.6500, R: 0.3023, AUC: 0.8706\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro_1024\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 1024\n",
    "EPOCHS_CV = 220\n",
    "BATCH_CV = 14\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"RIGHT_EYE_1_eye_original_repro_1024\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572877e4-2d2b-400e-9581-212435ae5545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 43 anemic / 365 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/300 Loss: 7.3127\n",
      "Epoch 40/300 Loss: 2.7326\n",
      "Epoch 60/300 Loss: 1.6939\n",
      "Epoch 80/300 Loss: 1.4495\n",
      "Epoch 100/300 Loss: 0.5191\n",
      "Epoch 120/300 Loss: 0.1237\n",
      "Epoch 140/300 Loss: 0.2180\n",
      "Epoch 160/300 Loss: 0.2899\n",
      "Epoch 180/300 Loss: 0.1262\n",
      "Epoch 200/300 Loss: 0.0526\n",
      "Epoch 220/300 Loss: 1.1068\n",
      "Epoch 240/300 Loss: 0.7019\n",
      "Epoch 260/300 Loss: 0.0751\n",
      "Epoch 280/300 Loss: 0.9214\n",
      "Epoch 300/300 Loss: 0.0118\n",
      "Fold 1 â†’ P=0.732, R=0.588\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/300 Loss: 6.8190\n",
      "Epoch 40/300 Loss: 3.4321\n",
      "Epoch 60/300 Loss: 2.6696\n",
      "Epoch 80/300 Loss: 0.5608\n",
      "Epoch 100/300 Loss: 0.6304\n",
      "Epoch 120/300 Loss: 0.8943\n",
      "Epoch 140/300 Loss: 0.1339\n",
      "Epoch 160/300 Loss: 0.1590\n",
      "Epoch 180/300 Loss: 0.4625\n",
      "Epoch 200/300 Loss: 2.7557\n",
      "Epoch 220/300 Loss: 1.1742\n",
      "Epoch 240/300 Loss: 4.7815\n",
      "Epoch 260/300 Loss: 0.9910\n",
      "Epoch 280/300 Loss: 0.0109\n",
      "Epoch 300/300 Loss: 3.1628\n",
      "Fold 2 â†’ P=0.222, R=0.980\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/300 Loss: 8.2357\n",
      "Epoch 40/300 Loss: 1.8658\n",
      "Epoch 60/300 Loss: 2.6423\n",
      "Epoch 80/300 Loss: 0.9790\n",
      "Epoch 100/300 Loss: 0.6570\n",
      "Epoch 120/300 Loss: 0.5699\n",
      "Epoch 140/300 Loss: 0.3448\n",
      "Epoch 160/300 Loss: 0.0480\n",
      "Epoch 180/300 Loss: 0.0983\n",
      "Epoch 200/300 Loss: 0.9809\n",
      "Epoch 220/300 Loss: 0.4598\n",
      "Epoch 240/300 Loss: 0.0548\n",
      "Epoch 260/300 Loss: 1.7600\n",
      "Epoch 280/300 Loss: 0.0651\n",
      "Epoch 300/300 Loss: 0.0325\n",
      "Fold 3 â†’ P=0.674, R=0.569\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/300 Loss: 6.7547\n",
      "Epoch 40/300 Loss: 1.9057\n",
      "Epoch 60/300 Loss: 0.5349\n",
      "Epoch 80/300 Loss: 1.2182\n",
      "Epoch 100/300 Loss: 2.3978\n",
      "Epoch 120/300 Loss: 3.7040\n",
      "Epoch 140/300 Loss: 0.3256\n",
      "Epoch 160/300 Loss: 0.7845\n",
      "Epoch 180/300 Loss: 0.1187\n",
      "Epoch 200/300 Loss: 0.7571\n",
      "Epoch 220/300 Loss: 0.1224\n",
      "Epoch 240/300 Loss: 0.1920\n",
      "Epoch 260/300 Loss: 3.9883\n",
      "Epoch 280/300 Loss: 0.7381\n",
      "Epoch 300/300 Loss: 0.6568\n",
      "Fold 4 â†’ P=0.467, R=0.275\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/300 Loss: 8.1067\n",
      "Epoch 40/300 Loss: 3.3654\n",
      "Epoch 60/300 Loss: 0.7225\n",
      "Epoch 80/300 Loss: 0.6878\n",
      "Epoch 100/300 Loss: 0.6118\n",
      "Epoch 120/300 Loss: 0.4575\n",
      "Epoch 140/300 Loss: 0.3867\n",
      "Epoch 160/300 Loss: 0.2459\n",
      "Epoch 180/300 Loss: 0.5283\n",
      "Epoch 200/300 Loss: 1.1238\n",
      "Epoch 220/300 Loss: 0.3388\n",
      "Epoch 240/300 Loss: 1.0666\n",
      "Epoch 260/300 Loss: 0.0786\n",
      "Epoch 280/300 Loss: 0.1960\n",
      "Epoch 300/300 Loss: 0.0167\n",
      "Fold 5 â†’ P=0.565, R=0.260\n",
      "âœ… Best fold = 1 | P=0.732, R=0.588\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.4762\n",
      "Recall:    0.2326\n",
      "F1 score:  0.3125\n",
      "Accuracy:  0.8795\n",
      "AUC:       0.8337\n",
      "TP, TN, FP, FN: 10, 311, 11, 33\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:25:27.319722: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 16:25:27.321017: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-01 16:25:27.347819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-01 16:25:27.865682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:25:29.655368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-01 16:25:29.657007: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:25:34.059185: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-01 16:25:34.059215: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-01 16:25:34.061487: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-01 16:25:34.094169: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-01 16:25:34.094188: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-01 16:25:34.126070: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-12-01 16:25:34.126753: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-01 16:25:34.189921: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-01 16:25:34.211172: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 149690 microseconds.\n",
      "2025-12-01 16:25:34.301395: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-01 16:25:34.561853: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.4762\n",
      "Recall:    0.2326\n",
      "F1 score:  0.3125\n",
      "Accuracy:  0.8795\n",
      "AUC:       0.8337\n",
      "TP, TN, FP, FN: 10, 311, 11, 33\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.4762, R: 0.2326, AUC: 0.8337\n",
      "TFLite  â†’ P: 0.4762, R: 0.2326, AUC: 0.8337\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/RIGHT_EYE_1_eye_original_repro\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 300\n",
    "BATCH_CV = 14\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"RIGHT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "    \n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "        \n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "            \n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "        \n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "    \n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "  \n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "    \n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "    \n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    \n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce: \n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs, \n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels, \n",
    "                              tflite_preds, \n",
    "                              \"Confusion Matrix - TFLite Model\", \n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics, \n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "            \n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "            \n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c948859-d407-4b1d-9f12-391a7d2d992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=43\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=40\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=40\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=322\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=329\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=328\n",
      "âœ… TRAIN: anemic=248, non-anemic=1907, total=2155\n",
      "âœ… VAL: anemic=37, non-anemic=298, total=335\n",
      "âœ… TEST: anemic=34, non-anemic=284, total=318\n",
      "\n",
      "===== Processing right resolution: 224 =====\n",
      "\n",
      "--- right Fold 1 ---\n",
      "Epoch [10/150] Loss: 17.424706\n",
      "Epoch [20/150] Loss: 10.040600\n",
      "Epoch [30/150] Loss: 3.885601\n",
      "Epoch [40/150] Loss: 1.528084\n",
      "Epoch [50/150] Loss: 1.154758\n",
      "Epoch [60/150] Loss: 0.440781\n",
      "Epoch [70/150] Loss: 2.140917\n",
      "Epoch [80/150] Loss: 0.584430\n",
      "Epoch [90/150] Loss: 0.841203\n",
      "Epoch [100/150] Loss: 1.087194\n",
      "Epoch [110/150] Loss: 0.847431\n",
      "Epoch [120/150] Loss: 0.034776\n",
      "Epoch [130/150] Loss: 0.427053\n",
      "Epoch [140/150] Loss: 0.089803\n",
      "Epoch [150/150] Loss: 0.347930\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.998740157480315, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}]\n",
      "\n",
      "--- right Fold 2 ---\n",
      "Epoch [10/150] Loss: 17.023469\n",
      "Epoch [20/150] Loss: 6.389506\n",
      "Epoch [30/150] Loss: 2.820154\n",
      "Epoch [40/150] Loss: 1.445986\n",
      "Epoch [50/150] Loss: 0.320072\n",
      "Epoch [60/150] Loss: 0.110062\n",
      "Epoch [70/150] Loss: 0.774634\n",
      "Epoch [80/150] Loss: 0.603533\n",
      "Epoch [90/150] Loss: 0.451223\n",
      "Epoch [100/150] Loss: 0.752936\n",
      "Epoch [110/150] Loss: 0.650835\n",
      "Epoch [120/150] Loss: 0.023752\n",
      "Epoch [130/150] Loss: 0.279976\n",
      "Epoch [140/150] Loss: 0.822565\n",
      "Epoch [150/150] Loss: 0.389302\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.998740157480315, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.92, 'Val_F1': 0.9484536082474226, 'Val_Accuracy': 0.988399071925754, 'Val_AUC': 0.998740157480315, 'Val_TP': 46, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 4}]\n",
      "\n",
      "--- right Fold 3 ---\n",
      "Epoch [10/150] Loss: 17.821281\n",
      "Epoch [20/150] Loss: 10.363209\n",
      "Epoch [30/150] Loss: 4.952012\n",
      "Epoch [40/150] Loss: 1.345266\n",
      "Epoch [50/150] Loss: 1.205067\n",
      "Epoch [60/150] Loss: 2.193197\n",
      "Epoch [70/150] Loss: 0.254089\n",
      "Epoch [80/150] Loss: 0.287064\n",
      "Epoch [90/150] Loss: 0.267886\n",
      "Epoch [100/150] Loss: 1.295949\n",
      "Epoch [110/150] Loss: 0.226063\n",
      "Epoch [120/150] Loss: 0.218129\n",
      "Epoch [130/150] Loss: 0.169963\n",
      "Epoch [140/150] Loss: 0.051798\n",
      "Epoch [150/150] Loss: 0.122159\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.998740157480315, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.92, 'Val_F1': 0.9484536082474226, 'Val_Accuracy': 0.988399071925754, 'Val_AUC': 0.998740157480315, 'Val_TP': 46, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 4}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.9943307086614173, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}]\n",
      "\n",
      "--- right Fold 4 ---\n",
      "Epoch [10/150] Loss: 17.557275\n",
      "Epoch [20/150] Loss: 7.281772\n",
      "Epoch [30/150] Loss: 1.685069\n",
      "Epoch [40/150] Loss: 0.922104\n",
      "Epoch [50/150] Loss: 1.298526\n",
      "Epoch [60/150] Loss: 0.948884\n",
      "Epoch [70/150] Loss: 0.632845\n",
      "Epoch [80/150] Loss: 0.533144\n",
      "Epoch [90/150] Loss: 0.318114\n",
      "Epoch [100/150] Loss: 0.134115\n",
      "Epoch [110/150] Loss: 0.091833\n",
      "Epoch [120/150] Loss: 0.511309\n",
      "Epoch [130/150] Loss: 0.824286\n",
      "Epoch [140/150] Loss: 0.021323\n",
      "Epoch [150/150] Loss: 0.273364\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.998740157480315, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.92, 'Val_F1': 0.9484536082474226, 'Val_Accuracy': 0.988399071925754, 'Val_AUC': 0.998740157480315, 'Val_TP': 46, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 4}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.9943307086614173, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 1.0, 'Val_Recall': 0.9183673469387755, 'Val_F1': 0.9574468085106383, 'Val_Accuracy': 0.9907192575406032, 'Val_AUC': 0.9996794529330056, 'Val_TP': 45, 'Val_TN': 382, 'Val_FP': 0, 'Val_FN': 4}]\n",
      "\n",
      "--- right Fold 5 ---\n",
      "Epoch [10/150] Loss: 17.623673\n",
      "Epoch [20/150] Loss: 6.622664\n",
      "Epoch [30/150] Loss: 2.279777\n",
      "Epoch [40/150] Loss: 1.246076\n",
      "Epoch [50/150] Loss: 1.390388\n",
      "Epoch [60/150] Loss: 0.837632\n",
      "Epoch [70/150] Loss: 1.378422\n",
      "Epoch [80/150] Loss: 1.299994\n",
      "Epoch [90/150] Loss: 0.196992\n",
      "Epoch [100/150] Loss: 0.310717\n",
      "Epoch [110/150] Loss: 1.883295\n",
      "Epoch [120/150] Loss: 0.018262\n",
      "Epoch [130/150] Loss: 0.250801\n",
      "Epoch [140/150] Loss: 0.023211\n",
      "Epoch [150/150] Loss: 0.034542\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.998740157480315, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.92, 'Val_F1': 0.9484536082474226, 'Val_Accuracy': 0.988399071925754, 'Val_AUC': 0.998740157480315, 'Val_TP': 46, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 4}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.86, 'Val_F1': 0.9148936170212766, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.9943307086614173, 'Val_TP': 43, 'Val_TN': 380, 'Val_FP': 1, 'Val_FN': 7}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 1.0, 'Val_Recall': 0.9183673469387755, 'Val_F1': 0.9574468085106383, 'Val_Accuracy': 0.9907192575406032, 'Val_AUC': 0.9996794529330056, 'Val_TP': 45, 'Val_TN': 382, 'Val_FP': 0, 'Val_FN': 4}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.9361702127659575, 'Val_Recall': 0.8979591836734694, 'Val_F1': 0.9166666666666666, 'Val_Accuracy': 0.9814385150812065, 'Val_AUC': 0.9938027567047761, 'Val_TP': 44, 'Val_TN': 379, 'Val_FP': 3, 'Val_FN': 5}]\n",
      "âœ… Best fold = 4\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          4             1.0     0.911765 0.953846       0.990566       1.0       31      284        0        3\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 16:37:02.592786: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-17 16:37:02.592820: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-17 16:37:02.595167: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-17 16:37:02.631794: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-17 16:37:02.631823: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-17 16:37:02.655531: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-17 16:37:02.741824: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-17 16:37:02.780234: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 185072 microseconds.\n",
      "2025-11-17 16:37:03.560388: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-17 16:37:03.560440: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "2025-11-17 16:37:04.209318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-17 16:37:04.210806: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 285 nodes with 2 partitions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        4.0             1.0     0.911765 0.953846       0.990566       1.0       31      284        0        3 PyTorch\n",
      "        NaN             1.0     0.911765 0.953846       0.990566       1.0       31      284        0        3  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye_hb_90_repro_bestfold_only_shared\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-right-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Shared ResNet18 across 3 inputs â†’ ~45 MB TFLite\n",
    "âœ… TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "âœ… Re-evaluates .tflite file and compares results with PyTorch\n",
    "âœ… Ensures no silent divergence between frameworks\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224  # Only one resolution used now\n",
    "EPOCHS_CV = 150\n",
    "BATCH_CV = 28\n",
    "LR_CV = 0.00022\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "output_dir = os.path.join(base_path, \"tri_right_eye_hb_90_repro_bestfold_only_shared\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_right(dirs_map):\n",
    "    suffixes = {'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['right1', 'right2', 'right3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('right1', '_right_eye_1.png'), 'r2': ('right2', '_right_eye_2.png'), 'r3': ('right3', '_right_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_right_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_right(anemic_dirs)\n",
    "    bases_n = common_bases_right(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_right.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in ['right1','right2','right3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (right)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (right)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_right_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_right_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_right_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-right-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrirightDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetright(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_right_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_right_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing right resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- right Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrirightDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrirightDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetright().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'right',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"right_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"right_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"right_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"right_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetright().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrirightDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"right_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_right_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10], \n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9], \n",
    "                                  tflite_metrics[12], \n",
    "                                  \"Confusion Matrix - TFLite Model\", \n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics, \n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b603d08c-eb57-429c-8d5a-cb574f363aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=44\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=42\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=43\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=314\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=319\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=318\n",
      "âœ… TRAIN: anemic=243, non-anemic=1857, total=2100\n",
      "âœ… VAL: anemic=39, non-anemic=294, total=333\n",
      "âœ… TEST: anemic=35, non-anemic=270, total=305\n",
      "\n",
      "===== Processing left resolution: 224 =====\n",
      "\n",
      "--- left Fold 1 ---\n",
      "Epoch [10/120] Loss: 17.467903\n",
      "Epoch [20/120] Loss: 6.269332\n",
      "Epoch [30/120] Loss: 2.823962\n",
      "Epoch [40/120] Loss: 2.143372\n",
      "Epoch [50/120] Loss: 2.490998\n",
      "Epoch [60/120] Loss: 1.666552\n",
      "Epoch [70/120] Loss: 0.334114\n",
      "Epoch [80/120] Loss: 0.646267\n",
      "Epoch [90/120] Loss: 0.606793\n",
      "Epoch [100/120] Loss: 0.656115\n",
      "Epoch [110/120] Loss: 1.379440\n",
      "Epoch [120/120] Loss: 0.476546\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.8070175438596491, 'Val_Recall': 0.9387755102040817, 'Val_F1': 0.8679245283018868, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.9929039001045161, 'Val_TP': 46, 'Val_TN': 360, 'Val_FP': 11, 'Val_FN': 3}]\n",
      "\n",
      "--- left Fold 2 ---\n",
      "Epoch [10/120] Loss: 16.710300\n",
      "Epoch [20/120] Loss: 6.486700\n",
      "Epoch [30/120] Loss: 3.947555\n",
      "Epoch [40/120] Loss: 2.391767\n",
      "Epoch [50/120] Loss: 1.593726\n",
      "Epoch [60/120] Loss: 1.658750\n",
      "Epoch [70/120] Loss: 1.118300\n",
      "Epoch [80/120] Loss: 0.188144\n",
      "Epoch [90/120] Loss: 1.460091\n",
      "Epoch [100/120] Loss: 0.336575\n",
      "Epoch [110/120] Loss: 0.600031\n",
      "Epoch [120/120] Loss: 0.830268\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.8070175438596491, 'Val_Recall': 0.9387755102040817, 'Val_F1': 0.8679245283018868, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.9929039001045161, 'Val_TP': 46, 'Val_TN': 360, 'Val_FP': 11, 'Val_FN': 3}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.95, 'Val_Recall': 0.7755102040816326, 'Val_F1': 0.8539325842696629, 'Val_Accuracy': 0.969047619047619, 'Val_AUC': 0.9813521095769845, 'Val_TP': 38, 'Val_TN': 369, 'Val_FP': 2, 'Val_FN': 11}]\n",
      "\n",
      "--- left Fold 3 ---\n",
      "Epoch [10/120] Loss: 16.857910\n",
      "Epoch [20/120] Loss: 5.996515\n",
      "Epoch [30/120] Loss: 3.094333\n",
      "Epoch [40/120] Loss: 2.467873\n",
      "Epoch [50/120] Loss: 0.977394\n",
      "Epoch [60/120] Loss: 0.298203\n",
      "Epoch [70/120] Loss: 1.131039\n",
      "Epoch [80/120] Loss: 1.516027\n",
      "Epoch [90/120] Loss: 1.343120\n",
      "Epoch [100/120] Loss: 0.329824\n",
      "Epoch [110/120] Loss: 0.095040\n",
      "Epoch [120/120] Loss: 0.679440\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.8070175438596491, 'Val_Recall': 0.9387755102040817, 'Val_F1': 0.8679245283018868, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.9929039001045161, 'Val_TP': 46, 'Val_TN': 360, 'Val_FP': 11, 'Val_FN': 3}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.95, 'Val_Recall': 0.7755102040816326, 'Val_F1': 0.8539325842696629, 'Val_Accuracy': 0.969047619047619, 'Val_AUC': 0.9813521095769845, 'Val_TP': 38, 'Val_TN': 369, 'Val_FP': 2, 'Val_FN': 11}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.7833333333333333, 'Val_Recall': 0.9591836734693877, 'Val_F1': 0.8623853211009175, 'Val_Accuracy': 0.9642857142857143, 'Val_AUC': 0.9944441388415204, 'Val_TP': 47, 'Val_TN': 358, 'Val_FP': 13, 'Val_FN': 2}]\n",
      "\n",
      "--- left Fold 4 ---\n",
      "Epoch [10/120] Loss: 17.196725\n",
      "Epoch [20/120] Loss: 6.480257\n",
      "Epoch [30/120] Loss: 2.860545\n",
      "Epoch [40/120] Loss: 2.024976\n",
      "Epoch [50/120] Loss: 0.937570\n",
      "Epoch [60/120] Loss: 1.187726\n",
      "Epoch [70/120] Loss: 1.226430\n",
      "Epoch [80/120] Loss: 1.334759\n",
      "Epoch [90/120] Loss: 0.720479\n",
      "Epoch [100/120] Loss: 0.868969\n",
      "Epoch [110/120] Loss: 0.568904\n",
      "Epoch [120/120] Loss: 1.050458\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.8070175438596491, 'Val_Recall': 0.9387755102040817, 'Val_F1': 0.8679245283018868, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.9929039001045161, 'Val_TP': 46, 'Val_TN': 360, 'Val_FP': 11, 'Val_FN': 3}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.95, 'Val_Recall': 0.7755102040816326, 'Val_F1': 0.8539325842696629, 'Val_Accuracy': 0.969047619047619, 'Val_AUC': 0.9813521095769845, 'Val_TP': 38, 'Val_TN': 369, 'Val_FP': 2, 'Val_FN': 11}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.7833333333333333, 'Val_Recall': 0.9591836734693877, 'Val_F1': 0.8623853211009175, 'Val_Accuracy': 0.9642857142857143, 'Val_AUC': 0.9944441388415204, 'Val_TP': 47, 'Val_TN': 358, 'Val_FP': 13, 'Val_FN': 2}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8703703703703703, 'Val_Recall': 0.9791666666666666, 'Val_F1': 0.9215686274509804, 'Val_Accuracy': 0.9809523809523809, 'Val_AUC': 0.9969198028673836, 'Val_TP': 47, 'Val_TN': 365, 'Val_FP': 7, 'Val_FN': 1}]\n",
      "\n",
      "--- left Fold 5 ---\n",
      "Epoch [10/120] Loss: 15.840823\n",
      "Epoch [20/120] Loss: 6.256372\n",
      "Epoch [30/120] Loss: 4.420064\n",
      "Epoch [40/120] Loss: 1.579969\n",
      "Epoch [50/120] Loss: 2.533235\n",
      "Epoch [60/120] Loss: 0.266720\n",
      "Epoch [70/120] Loss: 0.369207\n",
      "Epoch [80/120] Loss: 0.948158\n",
      "Epoch [90/120] Loss: 1.049054\n",
      "Epoch [100/120] Loss: 0.604376\n",
      "Epoch [110/120] Loss: 0.473606\n",
      "Epoch [120/120] Loss: 1.222631\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.8070175438596491, 'Val_Recall': 0.9387755102040817, 'Val_F1': 0.8679245283018868, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.9929039001045161, 'Val_TP': 46, 'Val_TN': 360, 'Val_FP': 11, 'Val_FN': 3}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.95, 'Val_Recall': 0.7755102040816326, 'Val_F1': 0.8539325842696629, 'Val_Accuracy': 0.969047619047619, 'Val_AUC': 0.9813521095769845, 'Val_TP': 38, 'Val_TN': 369, 'Val_FP': 2, 'Val_FN': 11}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.7833333333333333, 'Val_Recall': 0.9591836734693877, 'Val_F1': 0.8623853211009175, 'Val_Accuracy': 0.9642857142857143, 'Val_AUC': 0.9944441388415204, 'Val_TP': 47, 'Val_TN': 358, 'Val_FP': 13, 'Val_FN': 2}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8703703703703703, 'Val_Recall': 0.9791666666666666, 'Val_F1': 0.9215686274509804, 'Val_Accuracy': 0.9809523809523809, 'Val_AUC': 0.9969198028673836, 'Val_TP': 47, 'Val_TN': 365, 'Val_FP': 7, 'Val_FN': 1}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.8, 'Val_Recall': 0.75, 'Val_F1': 0.7741935483870969, 'Val_Accuracy': 0.95, 'Val_AUC': 0.9779345878136201, 'Val_TP': 36, 'Val_TN': 363, 'Val_FP': 9, 'Val_FN': 12}]\n",
      "âœ… Best fold = 4\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          4             1.0     0.942857 0.970588       0.993443  0.999577       33      270        0        2\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 18:46:27.155300: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-17 18:46:27.155354: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-17 18:46:27.157116: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-17 18:46:27.195191: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-17 18:46:27.195228: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-17 18:46:27.210790: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-17 18:46:27.257143: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-17 18:46:27.296539: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 139429 microseconds.\n",
      "2025-11-17 18:46:28.037517: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-17 18:46:28.037575: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 18:46:28.500149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-17 18:46:28.501635: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        4.0             1.0     0.942857 0.970588       0.993443  0.999577       33      270        0        2 PyTorch\n",
      "        NaN             1.0     0.942857 0.970588       0.993443  0.999577       33      270        0        2  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye_hb_90_repro_bestfold_only_shared\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-left-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Shared ResNet18 across 3 inputs â†’ ~45 MB TFLite\n",
    "âœ… TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "âœ… Re-evaluates .tflite file and compares results with PyTorch\n",
    "âœ… Ensures no silent divergence between frameworks\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00017\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "output_dir = os.path.join(base_path, \"tri_left_eye_hb_90_repro_bestfold_only_shared\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_left(dirs_map):\n",
    "    suffixes = {'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['left1', 'left2', 'left3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('left1', '_left_eye_1.png'), 'r2': ('left2', '_left_eye_2.png'), 'r3': ('left3', '_left_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_left_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_left(anemic_dirs)\n",
    "    bases_n = common_bases_left(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_left.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in ['left1','left2','left3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (left)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (left)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_left_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_left_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_left_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-left-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrileftDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetleft(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_left_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_left_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing left resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- left Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrileftDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrileftDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetleft().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'left',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"left_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"left_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"left_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"left_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetleft().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrileftDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"left_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_left_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10], \n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9], \n",
    "                                  tflite_metrics[12], \n",
    "                                  \"Confusion Matrix - TFLite Model\", \n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics, \n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc65ce1-9c8e-45f5-8a48-d60530230bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=44\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=42\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=43\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=43\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=40\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/ | files=40\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=314\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=319\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=318\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=322\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=329\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/ | files=328\n",
      "âœ… TRAIN: anemic=235, non-anemic=1824, total=2059\n",
      "âœ… VAL: anemic=37, non-anemic=291, total=328\n",
      "âœ… TEST: anemic=28, non-anemic=237, total=265\n",
      "\n",
      "===== Processing HEXA resolution: 224 =====\n",
      "\n",
      "--- HEXA Fold 1 ---\n",
      "Epoch [10/300] Loss: 60.844395\n",
      "Epoch [20/300] Loss: 6.580949\n",
      "Epoch [30/300] Loss: 4.673402\n",
      "Epoch [40/300] Loss: 4.109463\n",
      "Epoch [50/300] Loss: 1.230240\n",
      "Epoch [60/300] Loss: 0.937990\n",
      "Epoch [70/300] Loss: 2.406883\n",
      "Epoch [80/300] Loss: 0.770182\n",
      "Epoch [90/300] Loss: 1.986065\n",
      "âœ… Early stop at epoch 93: P=0.956, R=0.915\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9555555555555556, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.9347826086956522, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.9971436898863306, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 2, 'Val_FN': 4, 'Stopped_Early': True}]\n",
      "\n",
      "--- HEXA Fold 2 ---\n",
      "Epoch [10/300] Loss: 62.501173\n",
      "Epoch [20/300] Loss: 55.307941\n",
      "Epoch [30/300] Loss: 38.479361\n",
      "Epoch [40/300] Loss: 22.559348\n",
      "Epoch [50/300] Loss: 13.901448\n",
      "Epoch [60/300] Loss: 5.087291\n",
      "Epoch [70/300] Loss: 6.315525\n",
      "Epoch [80/300] Loss: 6.094041\n",
      "Epoch [90/300] Loss: 4.321974\n",
      "Epoch [100/300] Loss: 1.841459\n",
      "Epoch [110/300] Loss: 3.041650\n",
      "Epoch [120/300] Loss: 3.383403\n",
      "Epoch [130/300] Loss: 3.134818\n",
      "Epoch [140/300] Loss: 5.011588\n",
      "Epoch [150/300] Loss: 0.482000\n",
      "Epoch [160/300] Loss: 0.135951\n",
      "Epoch [170/300] Loss: 0.064061\n",
      "Epoch [180/300] Loss: 0.270633\n",
      "âœ… Early stop at epoch 189: P=0.936, R=0.936\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9555555555555556, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.9347826086956522, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.9971436898863306, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 2, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9361702127659575, 'Val_Recall': 0.9361702127659575, 'Val_F1': 0.9361702127659575, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.995336636549111, 'Val_TP': 44, 'Val_TN': 362, 'Val_FP': 3, 'Val_FN': 3, 'Stopped_Early': True}]\n",
      "\n",
      "--- HEXA Fold 3 ---\n",
      "Epoch [10/300] Loss: 63.033947\n",
      "Epoch [20/300] Loss: 55.705258\n",
      "Epoch [30/300] Loss: 40.850352\n",
      "Epoch [40/300] Loss: 26.080757\n",
      "Epoch [50/300] Loss: 12.267160\n",
      "Epoch [60/300] Loss: 6.576703\n",
      "âœ… Early stop at epoch 69: P=0.977, R=0.915\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9555555555555556, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.9347826086956522, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.9971436898863306, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 2, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9361702127659575, 'Val_Recall': 0.9361702127659575, 'Val_F1': 0.9361702127659575, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.995336636549111, 'Val_TP': 44, 'Val_TN': 362, 'Val_FP': 3, 'Val_FN': 3, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.945054945054945, 'Val_Accuracy': 0.9878640776699029, 'Val_AUC': 0.9953366365491111, 'Val_TP': 43, 'Val_TN': 364, 'Val_FP': 1, 'Val_FN': 4, 'Stopped_Early': True}]\n",
      "\n",
      "--- HEXA Fold 4 ---\n",
      "Epoch [10/300] Loss: 63.182812\n",
      "Epoch [20/300] Loss: 57.770856\n",
      "Epoch [30/300] Loss: 40.624169\n",
      "Epoch [40/300] Loss: 10.119486\n",
      "Epoch [50/300] Loss: 7.167930\n",
      "Epoch [60/300] Loss: 4.775145\n",
      "âœ… Early stop at epoch 67: P=0.979, R=0.979\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9555555555555556, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.9347826086956522, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.9971436898863306, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 2, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9361702127659575, 'Val_Recall': 0.9361702127659575, 'Val_F1': 0.9361702127659575, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.995336636549111, 'Val_TP': 44, 'Val_TN': 362, 'Val_FP': 3, 'Val_FN': 3, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.945054945054945, 'Val_Accuracy': 0.9878640776699029, 'Val_AUC': 0.9953366365491111, 'Val_TP': 43, 'Val_TN': 364, 'Val_FP': 1, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.9787234042553191, 'Val_F1': 0.9787234042553191, 'Val_Accuracy': 0.9951456310679612, 'Val_AUC': 0.9998251238705916, 'Val_TP': 46, 'Val_TN': 364, 'Val_FP': 1, 'Val_FN': 1, 'Stopped_Early': True}]\n",
      "\n",
      "--- HEXA Fold 5 ---\n",
      "Epoch [10/300] Loss: 63.704879\n",
      "Epoch [20/300] Loss: 54.447120\n",
      "Epoch [30/300] Loss: 12.111396\n",
      "Epoch [40/300] Loss: 3.394614\n",
      "Epoch [50/300] Loss: 2.609509\n",
      "Epoch [60/300] Loss: 1.287642\n",
      "âœ… Early stop at epoch 66: P=0.977, R=0.915\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.9555555555555556, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.9347826086956522, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.9971436898863306, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 2, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.9361702127659575, 'Val_Recall': 0.9361702127659575, 'Val_F1': 0.9361702127659575, 'Val_Accuracy': 0.9854368932038835, 'Val_AUC': 0.995336636549111, 'Val_TP': 44, 'Val_TN': 362, 'Val_FP': 3, 'Val_FN': 3, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.945054945054945, 'Val_Accuracy': 0.9878640776699029, 'Val_AUC': 0.9953366365491111, 'Val_TP': 43, 'Val_TN': 364, 'Val_FP': 1, 'Val_FN': 4, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.9787234042553191, 'Val_Recall': 0.9787234042553191, 'Val_F1': 0.9787234042553191, 'Val_Accuracy': 0.9951456310679612, 'Val_AUC': 0.9998251238705916, 'Val_TP': 46, 'Val_TN': 364, 'Val_FP': 1, 'Val_FN': 1, 'Stopped_Early': True}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.9772727272727273, 'Val_Recall': 0.9148936170212766, 'Val_F1': 0.945054945054945, 'Val_Accuracy': 0.9878345498783455, 'Val_AUC': 0.999766191255553, 'Val_TP': 43, 'Val_TN': 363, 'Val_FP': 1, 'Val_FN': 4, 'Stopped_Early': True}]\n",
      "âœ… Best fold = 4\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          4             1.0     0.964286 0.981818       0.996226       1.0       27      237        0        1\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_373_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 23:47:04.078876: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-17 23:47:04.078912: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-17 23:47:04.081196: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-11-17 23:47:04.119885: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-17 23:47:04.119904: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-11-17 23:47:04.141262: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-17 23:47:04.218146: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-11-17 23:47:04.295029: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 213837 microseconds.\n",
      "2025-11-17 23:47:05.356276: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-17 23:47:05.356336: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 21.796 G  ops, equivalently 10.898 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_eye_resnet18_shared.tflite (48.96 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 48.96 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 23:47:05.908276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-17 23:47:05.909611: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0', 'serving_default_input4:0', 'serving_default_input5:0', 'serving_default_input6:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        4.0             1.0     0.964286 0.981818       0.996226       1.0       27      237        0        1 PyTorch\n",
      "        NaN             1.0     0.964286 0.981818       0.996226       1.0       27      237        0        1  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/hexa_eye_hb_90_repro_bestfold_only_shared2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Hexa-Eye ResNet18 with SHARED BACKBONE + EARLY STOPPING & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Single ResNet18 used across 6 inputs â†’ ~47 MB TFLite\n",
    "âœ… Stops fold early if Val Precision & Recall >= 0.90\n",
    "âœ… Re-evaluates .tflite model and compares predictions with PyTorch\n",
    "âœ… Ensures faithful deployment (no numerical drift)\n",
    "âœ… Deterministic training + FlexDelegate support for GELU\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 300\n",
    "BATCH_CV = 8  # Reduced due to 6 inputs per sample\n",
    "LR_CV = 0.000256\n",
    "\n",
    "\n",
    "# ðŸ”¥ NEW: Early stop if both P and R >= this threshold\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "output_dir = os.path.join(base_path, \"hexa_eye_hb_90_repro_bestfold_only_shared2\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "# All six input directories\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_hexa(dirs_map):\n",
    "    suffixes = {\n",
    "        'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png',\n",
    "        'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'\n",
    "    }\n",
    "    bases_sets = []\n",
    "    for k in dirs_map.keys():\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_hexa_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {f'r{i}': [] for i in range(1,7)}\n",
    "    out['filenames'] = []\n",
    "    key_map = [\n",
    "        ('left1', '_left_eye_1.png'),\n",
    "        ('left2', '_left_eye_2.png'),\n",
    "        ('left3', '_left_eye_3.png'),\n",
    "        ('right1', '_right_eye_1.png'),\n",
    "        ('right2', '_right_eye_2.png'),\n",
    "        ('right3', '_right_eye_3.png')\n",
    "    ]\n",
    "    for b in bases:\n",
    "        imgs, failed = [], False\n",
    "        for long_k, suf in key_map:\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if not failed:\n",
    "            for i, img in enumerate(imgs):\n",
    "                out[f'r{i+1}'].append(img)\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_hexa_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_hexa(anemic_dirs)\n",
    "    bases_n = common_bases_hexa(non_dirs)\n",
    "    imgs_a = load_hexa_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_hexa_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    \n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'r4': imgs_a['r4'] + imgs_n['r4'],\n",
    "        'r5': imgs_a['r5'] + imgs_n['r5'],\n",
    "        'r6': imgs_a['r6'] + imgs_n['r6'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_hexa.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in dirs_map.keys():\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (HEXA)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (HEXA)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_hexa_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_hexa_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_hexa_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No hexa-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class HexaDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[f'r{i}'][idx] for i in range(1,7)]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE (6 inputs)\n",
    "# =========================\n",
    "class HexaResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Single shared backbone\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # Fusion head: 6*512 â†’ 1\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(6 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5, x6):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        f4 = self.backbone(x4)\n",
    "        f5 = self.backbone(x5)\n",
    "        f6 = self.backbone(x6)\n",
    "        x = torch.cat([f1, f2, f3, f4, f5, f6], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "    \n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "    \n",
    "    for imgs, labels in loader:\n",
    "        x_list = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(*x_list)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(6))\n",
    "    onnx_path = os.path.join(output_dir, \"hexa_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"hexa_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[f'input{i}' for i in range(1,7)],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Sort by name to ensure correct order\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data[f'r{j}'][i] for j in range(1,7)]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'], \n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'], \n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1], \n",
    "                   tflite_metrics[2], tflite_metrics[3], \n",
    "                   tflite_metrics[4]]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    \n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP (with EARLY STOPPING)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing HEXA resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- HEXA Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(HexaDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(HexaDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = HexaResNet().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        stopped_early = False\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x_list = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(*x_list)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "            # ðŸ”¥ EARLY STOPPING CHECK\n",
    "            if EARLY_STOP_PR is not None:\n",
    "                val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "                p, r = val_metrics[0], val_metrics[1]\n",
    "                if not (np.isnan(p) or np.isnan(r)) and p >= EARLY_STOP_PR and r >= EARLY_STOP_PR and p < 1 and r < 1:\n",
    "                    print(f\"âœ… Early stop at epoch {epoch+1}: P={p:.3f}, R={r:.3f}\")\n",
    "                    stopped_early = True\n",
    "                    break  # exit inner loop\n",
    "\n",
    "        # Final evaluation after training (or early stop)\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'HEXA',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8],\n",
    "            'Stopped_Early': stopped_early\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"hexa_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"hexa_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"hexa_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90) & (df['Val_Precision'] < 1) & (df['Val_Recall'] < 1)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"hexa_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = HexaResNet().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation\n",
    "    test_loader = make_loader(HexaDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"hexa_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10], \n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\", \n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9], \n",
    "                          test_metrics[12], \n",
    "                          \"Confusion Matrix - PyTorch Model\", \n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"hexa_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10], \n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\", \n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9], \n",
    "                                  tflite_metrics[12], \n",
    "                                  \"Confusion Matrix - TFLite Model\", \n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics, \n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0453e18-b308-4704-a93b-b0f674647f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f44b9c-2bea-4a92-a393-5af76fa7a888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611319-7a43-4d79-a34f-6ccfc35b7c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
