 MAHILA -  Moderate Maternal Anemia Heuristics using Imaging Learning Algorithms – Primary Chronological Eye-Based Models

This repository contains the primary and final experimental results for
anemia detection using eye images, evaluated under a strict chronological
(train–validation–test) split.

Chronological splitting is deliberately used to reflect real-world clinical
deployment, where models trained on historical data are applied to future
patients. All conclusions and performance claims are based on the results
contained in this folder.

--------------------------------------------------------------------------

1. Models Implemented

A total of nine independent models are implemented and evaluated.

Single-Eye Models (6):
- Left Eye: 3 standalone models
- Right Eye: 3 standalone models

Multi-Eye Models:
- Tri-Eye Models (2)
  - Tri Left Eye
  - Tri Right Eye
- Hexa-Eye Model (1)
  - Six-eye input model using a shared ResNet18 backbone

Each model is trained, validated, tested, and evaluated independently under
the same chronological protocol.

--------------------------------------------------------------------------

2. Why Standalone Eye Models (PyTorch Reproducibility Rationale)

Each eye-based model is implemented as a standalone PyTorch script rather
than a single monolithic pipeline. This design choice is intentional and
supports reproducibility and auditability.

- Each model runs independently, preventing interaction between experiments
- Training artifacts, random states, and outputs are isolated per model
- A single script execution reproduces a single model’s results exactly
- Each eye configuration represents a distinct experimental hypothesis
- Model definition, training, evaluation, and export are fully transparent
- Deployable artifacts are produced independently for each model

This structure prioritizes scientific correctness and reproducibility over
code compactness.

--------------------------------------------------------------------------

3. Folder Structure

1 - Primary Model - Chronological Split Eye HB less than 9 Models
|
|-- left eye 1
|-- left eye 2
|-- left eye 3
|-- right eye 1
|-- right eye 2
|-- right eye 3
|-- tri left eye
|-- tri right eye
|-- hex eye
|-- Reference - All Models Executed Python Notebook

Each model folder is self-contained and represents one complete experiment.

--------------------------------------------------------------------------

4. Contents of Each Model Folder

Each model folder contains two subdirectories.

A. scripts

Contains a single executable Python script for that model:
- data loading
- chronological dataset splitting
- model architecture definition
- training and validation loops
- test-time evaluation
- metric computation
- export to ONNX and TFLite
- saving of all outputs and artifacts

Each script can be executed independently in PyTorch.

B. outputs

Contains all artifacts generated by the model.

Evaluation visualizations:
- confusion matrices (PyTorch and TFLite)
- ROC curves (PyTorch and TFLite)
- metrics comparison plots

Prediction outputs:
- detailed per-sample prediction CSVs
- test result summaries
- full prediction outputs for multi-eye models

Dataset split tracking:
- cross-validation result files
- fold index files
- train, validation, and test sample ID records

Model artifacts:
- ONNX model files
- TFLite deployment-ready models

--------------------------------------------------------------------------

5. Reference Execution Files

The folder “Reference - All Models Executed Python Notebook” contains:
- a consolidated Python script
- an executed Jupyter notebook

These files show how all nine models were executed and evaluated and serve
as an additional reproducibility reference.

--------------------------------------------------------------------------

6. Chronological Split Methodology

All experiments use a strict time-ordered split:
- training data precedes validation data
- validation data precedes test data

This prevents future information leakage and produces deployment-relevant
performance estimates suitable for medical AI systems.

--------------------------------------------------------------------------

7. Hyperparameters

- Hyperparameters may vary across different models
- Hyperparameters are selected prior to final evaluation
- Changes in hyperparameters do not affect the core training or evaluation
  pipeline

Model architectures, loss functions, optimizer types, and evaluation metrics
remain consistent.

--------------------------------------------------------------------------

8. Reproducibility

Reproducibility is ensured through:
- fixed random seeds
- deterministic training settings
- saved dataset split identifiers
- preserved prediction outputs and model artifacts

Each model can be re-run independently to reproduce the reported results.

--------------------------------------------------------------------------

9. Intended Use

This folder represents the primary experimental evidence for the study.
All conclusions, comparisons, and deployment considerations should be based
on the results contained here.

--------------------------------------------------------------------------

10. Summary

This repository provides:
- complete experimental transparency
- strong methodological rigor
- standalone, reproducible PyTorch models
- deployable model artifacts
- reviewer-ready documentation
