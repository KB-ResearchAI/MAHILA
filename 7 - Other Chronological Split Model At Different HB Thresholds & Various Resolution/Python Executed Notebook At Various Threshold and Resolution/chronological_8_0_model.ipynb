{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xDI5AzkXpe75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/lib/python3/dist-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "\n",
      "Checking for leakage by FULL FILENAME...\n",
      "\n",
      "‚úÖ No leakage ‚Äî all filenames are unique to each split.\n",
      "TEST: 18 anemic / 358 total\n",
      "üîÄ Randomizing test dataset order...\n",
      "TEST: 18 anemic / 358 total\n",
      "‚ö†Ô∏è Adding 5.0% label noise to test set (for analysis)...\n",
      "Final test labels used: 29 anemic / 358 total\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 18 anemic / 358 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/200 Loss: 2.2576\n",
      "Epoch 40/200 Loss: 0.2841\n",
      "Epoch 60/200 Loss: 0.2492\n",
      "Epoch 80/200 Loss: 0.0185\n",
      "Epoch 100/200 Loss: 0.0043\n",
      "Epoch 120/200 Loss: 5.9037\n",
      "Epoch 140/200 Loss: 0.0282\n",
      "Epoch 160/200 Loss: 0.0050\n",
      "Epoch 180/200 Loss: 0.0036\n",
      "Epoch 200/200 Loss: 6.4801\n",
      "Fold 1 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/200 Loss: 3.2278\n",
      "Epoch 40/200 Loss: 0.6461\n",
      "Epoch 60/200 Loss: 0.1706\n",
      "Epoch 80/200 Loss: 3.9380\n",
      "Epoch 100/200 Loss: 0.5774\n",
      "Epoch 120/200 Loss: 0.3720\n",
      "Epoch 140/200 Loss: 0.0047\n",
      "Epoch 160/200 Loss: 0.0041\n",
      "Epoch 180/200 Loss: 0.0415\n",
      "Epoch 200/200 Loss: 0.0034\n",
      "Fold 2 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/200 Loss: 1.3743\n",
      "Epoch 40/200 Loss: 0.8187\n",
      "Epoch 60/200 Loss: 1.4580\n",
      "Epoch 80/200 Loss: 0.0849\n",
      "Epoch 100/200 Loss: 0.0049\n",
      "Epoch 120/200 Loss: 0.0047\n",
      "Epoch 140/200 Loss: 0.0007\n",
      "Epoch 160/200 Loss: 0.0005\n",
      "Epoch 180/200 Loss: 0.0005\n",
      "Epoch 200/200 Loss: 0.0001\n",
      "Fold 3 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/200 Loss: 2.1885\n",
      "Epoch 40/200 Loss: 0.1269\n",
      "Epoch 60/200 Loss: 0.2115\n",
      "Epoch 80/200 Loss: 0.0071\n",
      "Epoch 100/200 Loss: 1.4840\n",
      "Epoch 120/200 Loss: 0.0356\n",
      "Epoch 140/200 Loss: 0.0151\n",
      "Epoch 160/200 Loss: 0.9095\n",
      "Epoch 180/200 Loss: 0.0108\n",
      "Epoch 200/200 Loss: 0.0023\n",
      "Fold 4 ‚Üí P=0.333, R=0.067\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/200 Loss: 2.8265\n",
      "Epoch 40/200 Loss: 0.3574\n",
      "Epoch 60/200 Loss: 0.1036\n",
      "Epoch 80/200 Loss: 0.2322\n",
      "Epoch 100/200 Loss: 0.0123\n",
      "Epoch 120/200 Loss: 0.0012\n",
      "Epoch 140/200 Loss: 0.0006\n",
      "Epoch 160/200 Loss: 0.0005\n",
      "Epoch 180/200 Loss: 0.7207\n",
      "Epoch 200/200 Loss: 0.3124\n",
      "Fold 5 ‚Üí P=0.000, R=0.000\n",
      "‚úÖ Best fold = 4 | P=0.333, R=0.067\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.2500\n",
      "Recall:    0.0556\n",
      "F1 score:  0.0909\n",
      "Accuracy:  0.9441\n",
      "AUC:       0.7044\n",
      "TP, TN, FP, FN: 1, 337, 3, 17\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:28:43.442602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-24 16:28:43.444298: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:28:48.141126: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-24 16:28:48.141164: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-24 16:28:48.143683: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-24 16:28:48.176620: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-24 16:28:48.176655: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-24 16:28:48.209244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-11-24 16:28:48.210046: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-24 16:28:48.273737: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-24 16:28:48.294895: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 151217 microseconds.\n",
      "2025-11-24 16:28:48.386138: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-24 16:28:48.656276: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [  1   3 780 780]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.2500\n",
      "Recall:    0.0556\n",
      "F1 score:  0.0909\n",
      "Accuracy:  0.9441\n",
      "AUC:       0.7044\n",
      "TP, TN, FP, FN: 1, 337, 3, 17\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.2500, R: 0.0556, AUC: 0.7044\n",
      "TFLite  ‚Üí P: 0.2500, R: 0.0556, AUC: 0.7044\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_1_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 16 anemic / 369 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 2.3179\n",
      "Epoch 40/120 Loss: 1.0506\n",
      "Epoch 60/120 Loss: 2.4613\n",
      "Epoch 80/120 Loss: 0.0429\n",
      "Epoch 100/120 Loss: 0.0748\n",
      "Epoch 120/120 Loss: 0.0136\n",
      "Fold 1 ‚Üí P=0.333, R=0.062\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 2.7804\n",
      "Epoch 40/120 Loss: 0.9026\n",
      "Epoch 60/120 Loss: 0.7624\n",
      "Epoch 80/120 Loss: 0.0336\n",
      "Epoch 100/120 Loss: 0.0385\n",
      "Epoch 120/120 Loss: 0.5944\n",
      "Fold 2 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 3.2195\n",
      "Epoch 40/120 Loss: 1.0219\n",
      "Epoch 60/120 Loss: 1.2034\n",
      "Epoch 80/120 Loss: 0.0812\n",
      "Epoch 100/120 Loss: 0.1589\n",
      "Epoch 120/120 Loss: 0.0073\n",
      "Fold 3 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 4.7302\n",
      "Epoch 40/120 Loss: 0.2632\n",
      "Epoch 60/120 Loss: 0.5581\n",
      "Epoch 80/120 Loss: 0.0282\n",
      "Epoch 100/120 Loss: 2.3100\n",
      "Epoch 120/120 Loss: 0.6746\n",
      "Fold 4 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 3.9361\n",
      "Epoch 40/120 Loss: 0.8941\n",
      "Epoch 60/120 Loss: 0.5965\n",
      "Epoch 80/120 Loss: 0.0565\n",
      "Epoch 100/120 Loss: 0.0152\n",
      "Epoch 120/120 Loss: 2.2855\n",
      "Fold 5 ‚Üí P=0.000, R=0.000\n",
      "‚úÖ Best fold = 1 | P=0.333, R=0.062\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "Accuracy:  0.9566\n",
      "AUC:       0.5570\n",
      "TP, TN, FP, FN: 0, 353, 0, 16\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 00:53:17.406232: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-25 00:53:17.406268: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-25 00:53:17.408179: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 00:53:17.441352: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-25 00:53:17.441386: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 00:53:17.473621: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-25 00:53:17.537996: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 00:53:17.558726: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 150551 microseconds.\n",
      "2025-11-25 00:53:17.873311: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [  1   3 780 780]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "Accuracy:  0.9566\n",
      "AUC:       0.5570\n",
      "TP, TN, FP, FN: 0, 353, 0, 16\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.0000, R: 0.0000, AUC: 0.5570\n",
      "TFLite  ‚Üí P: 0.0000, R: 0.0000, AUC: 0.5570\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_2_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 16 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/25 Loss: 18.0728\n",
      "Epoch 25/25 Loss: 16.2887\n",
      "Fold 1 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/25 Loss: 16.9351\n",
      "Epoch 25/25 Loss: 17.3838\n",
      "Fold 2 ‚Üí P=0.167, R=0.062\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/25 Loss: 17.6860\n",
      "Epoch 25/25 Loss: 17.8716\n",
      "Fold 3 ‚Üí P=0.125, R=0.062\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/25 Loss: 17.8286\n",
      "Epoch 25/25 Loss: 18.3741\n",
      "Fold 4 ‚Üí P=0.500, R=0.188\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/25 Loss: 17.1243\n",
      "Epoch 25/25 Loss: 18.7870\n",
      "Fold 5 ‚Üí P=0.000, R=0.000\n",
      "‚úÖ Best fold = 4 | P=0.500, R=0.188\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.5625\n",
      "Recall:    0.5625\n",
      "F1 score:  0.5625\n",
      "Accuracy:  0.9612\n",
      "AUC:       0.9752\n",
      "TP, TN, FP, FN: 9, 338, 7, 7\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 02:30:59.995005: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-25 02:30:59.995053: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-25 02:30:59.997126: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 02:31:00.029422: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-25 02:31:00.029449: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 02:31:00.040093: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-25 02:31:00.087364: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-11-25 02:31:00.108171: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 111050 microseconds.\n",
      "2025-11-25 02:31:00.461903: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [  1   3 780 780]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.5625\n",
      "Recall:    0.5625\n",
      "F1 score:  0.5625\n",
      "Accuracy:  0.9612\n",
      "AUC:       0.9752\n",
      "TP, TN, FP, FN: 9, 338, 7, 7\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.5625, R: 0.5625, AUC: 0.9752\n",
      "TFLite  ‚Üí P: 0.5625, R: 0.5625, AUC: 0.9752\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_2_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 15 anemic / 368 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 7.8888\n",
      "Epoch 40/120 Loss: 5.9280\n",
      "Epoch 60/120 Loss: 3.6877\n",
      "Epoch 80/120 Loss: 2.4564\n",
      "Epoch 100/120 Loss: 1.4054\n",
      "Epoch 120/120 Loss: 0.7118\n",
      "Fold 1 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 7.9905\n",
      "Epoch 40/120 Loss: 5.6494\n",
      "Epoch 60/120 Loss: 3.5912\n",
      "Epoch 80/120 Loss: 2.4994\n",
      "Epoch 100/120 Loss: 1.3974\n",
      "Epoch 120/120 Loss: 0.7453\n",
      "Fold 2 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 7.6796\n",
      "Epoch 40/120 Loss: 5.1703\n",
      "Epoch 60/120 Loss: 2.9161\n",
      "Epoch 80/120 Loss: 1.6436\n",
      "Epoch 100/120 Loss: 0.9700\n",
      "Epoch 120/120 Loss: 0.6273\n",
      "Fold 3 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 8.2415\n",
      "Epoch 40/120 Loss: 6.1482\n",
      "Epoch 60/120 Loss: 3.9936\n",
      "Epoch 80/120 Loss: 2.4376\n",
      "Epoch 100/120 Loss: 1.6178\n",
      "Epoch 120/120 Loss: 0.8792\n",
      "Fold 4 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 7.9735\n",
      "Epoch 40/120 Loss: 5.8180\n",
      "Epoch 60/120 Loss: 3.5612\n",
      "Epoch 80/120 Loss: 2.2146\n",
      "Epoch 100/120 Loss: 1.3196\n",
      "Epoch 120/120 Loss: 0.6945\n",
      "Fold 5 ‚Üí P=0.000, R=0.000\n",
      "‚úÖ Best fold = 2 | P=0.000, R=0.000\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "Accuracy:  0.9592\n",
      "AUC:       0.7075\n",
      "TP, TN, FP, FN: 0, 353, 0, 15\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 11:06:49.511868: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-25 11:06:49.511908: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-25 11:06:49.514047: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 11:06:49.546981: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-25 11:06:49.547015: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 11:06:49.571442: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-25 11:06:49.632390: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 11:06:49.653106: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 139063 microseconds.\n",
      "2025-11-25 11:06:50.001821: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [  1   3 780 780]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "Accuracy:  0.9592\n",
      "AUC:       0.7075\n",
      "TP, TN, FP, FN: 0, 353, 0, 15\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.0000, R: 0.0000, AUC: 0.7075\n",
      "TFLite  ‚Üí P: 0.0000, R: 0.0000, AUC: 0.7075\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_3_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 18 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/20 Loss: 8.0959\n",
      "Fold 1 ‚Üí P=1.000, R=0.067\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/20 Loss: 9.2539\n",
      "Fold 2 ‚Üí P=0.250, R=0.067\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/20 Loss: 7.8281\n",
      "Fold 3 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/20 Loss: 9.2772\n",
      "Fold 4 ‚Üí P=0.333, R=0.067\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/20 Loss: 7.0266\n",
      "Fold 5 ‚Üí P=0.179, R=0.467\n",
      "‚úÖ Best fold = 5 | P=0.179, R=0.467\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.2941\n",
      "Recall:    0.2778\n",
      "F1 score:  0.2857\n",
      "Accuracy:  0.9307\n",
      "AUC:       0.8291\n",
      "TP, TN, FP, FN: 5, 331, 12, 13\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 12:30:34.655332: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-25 12:30:34.655373: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-25 12:30:34.657117: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 12:30:34.690655: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-25 12:30:34.690683: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 12:30:34.701721: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-25 12:30:34.724674: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-11-25 12:30:34.745546: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 88433 microseconds.\n",
      "2025-11-25 12:30:35.013875: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [  1   3 780 780]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.2941\n",
      "Recall:    0.2778\n",
      "F1 score:  0.2857\n",
      "Accuracy:  0.9307\n",
      "AUC:       0.8291\n",
      "TP, TN, FP, FN: 5, 331, 12, 13\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.2941, R: 0.2778, AUC: 0.8291\n",
      "TFLite  ‚Üí P: 0.2941, R: 0.2778, AUC: 0.8291\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_LEFT_EYE_3_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 18 anemic / 365 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/220 Loss: 3.9976\n",
      "Epoch 40/220 Loss: 0.6679\n",
      "Epoch 60/220 Loss: 0.5886\n",
      "Epoch 80/220 Loss: 0.1968\n",
      "Epoch 100/220 Loss: 0.1103\n",
      "Epoch 120/220 Loss: 0.0195\n",
      "Epoch 140/220 Loss: 0.3975\n",
      "Epoch 160/220 Loss: 0.0756\n",
      "Epoch 180/220 Loss: 0.0585\n",
      "Epoch 200/220 Loss: 2.1185\n",
      "Epoch 220/220 Loss: 0.0062\n",
      "Fold 1 ‚Üí P=0.000, R=0.000\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/220 Loss: 4.0564\n",
      "Epoch 40/220 Loss: 0.8562\n",
      "Epoch 60/220 Loss: 0.1907\n",
      "Epoch 80/220 Loss: 0.2874\n",
      "Epoch 100/220 Loss: 0.5526\n",
      "Epoch 120/220 Loss: 0.0260\n",
      "Epoch 140/220 Loss: 0.6293\n",
      "Epoch 160/220 Loss: 0.0308\n",
      "Epoch 180/220 Loss: 0.0467\n",
      "Epoch 200/220 Loss: 0.1589\n",
      "Epoch 220/220 Loss: 0.4256\n",
      "Fold 2 ‚Üí P=0.333, R=0.125\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/220 Loss: 6.8976\n",
      "Epoch 40/220 Loss: 1.4471\n",
      "Epoch 60/220 Loss: 0.3478\n",
      "Epoch 80/220 Loss: 0.3577\n",
      "Epoch 100/220 Loss: 0.4891\n",
      "Epoch 120/220 Loss: 0.0216\n",
      "Epoch 140/220 Loss: 0.0849\n",
      "Epoch 160/220 Loss: 0.0897\n",
      "Epoch 180/220 Loss: 0.1153\n",
      "Epoch 200/220 Loss: 1.2614\n",
      "Epoch 220/220 Loss: 0.3858\n",
      "Fold 3 ‚Üí P=0.073, R=0.200\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/220 Loss: 5.7334\n",
      "Epoch 40/220 Loss: 1.3294\n",
      "Epoch 60/220 Loss: 0.3853\n",
      "Epoch 80/220 Loss: 0.7768\n",
      "Epoch 100/220 Loss: 0.2037\n",
      "Epoch 120/220 Loss: 0.0510\n",
      "Epoch 140/220 Loss: 0.5119\n",
      "Epoch 160/220 Loss: 0.0173\n",
      "Epoch 180/220 Loss: 0.1543\n",
      "Epoch 200/220 Loss: 3.1133\n",
      "Epoch 220/220 Loss: 0.1685\n",
      "Fold 4 ‚Üí P=0.286, R=0.133\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/220 Loss: 6.1210\n",
      "Epoch 40/220 Loss: 0.8036\n",
      "Epoch 60/220 Loss: 0.9486\n",
      "Epoch 80/220 Loss: 0.6635\n",
      "Epoch 100/220 Loss: 0.2862\n",
      "Epoch 120/220 Loss: 0.0251\n",
      "Epoch 140/220 Loss: 0.8990\n",
      "Epoch 160/220 Loss: 0.0576\n",
      "Epoch 180/220 Loss: 0.0603\n",
      "Epoch 200/220 Loss: 0.5083\n",
      "Epoch 220/220 Loss: 0.1150\n",
      "Fold 5 ‚Üí P=0.000, R=0.000\n",
      "‚úÖ Best fold = 4 | P=0.286, R=0.133\n",
      "\n",
      "üìä FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.2500\n",
      "Recall:    0.0556\n",
      "F1 score:  0.0909\n",
      "Accuracy:  0.9452\n",
      "AUC:       0.6475\n",
      "TP, TN, FP, FN: 1, 344, 3, 17\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   ‚úÖ ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 04:26:19.603892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-26 04:26:19.603930: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-26 04:26:19.605567: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-26 04:26:19.651357: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-26 04:26:19.651398: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-26 04:26:19.662753: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-26 04:26:19.687432: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-11-26 04:26:19.710666: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 105101 microseconds.\n",
      "2025-11-26 04:26:19.999174: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 75.905 G  ops, equivalently 37.952 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "‚úÖ TFLite conversion pipeline complete.\n",
      "\n",
      "üîç Loading TFLite model and re-evaluating on original test set...\n",
      "üîç TFLite model input shape: [   1    3 1024 1024]\n",
      "   ‚û§ Detected layout: NCHW\n",
      "\n",
      "üìä TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.2500\n",
      "Recall:    0.0556\n",
      "F1 score:  0.0909\n",
      "Accuracy:  0.9452\n",
      "AUC:       0.6475\n",
      "TP, TN, FP, FN: 1, 344, 3, 17\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "üîç Comparing with original PyTorch test results:\n",
      "PyTorch ‚Üí P: 0.2500, R: 0.0556, AUC: 0.6475\n",
      "TFLite  ‚Üí P: 0.2500, R: 0.0556, AUC: 0.6475\n",
      "‚úÖ TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_RIGHT_EYE_1_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4742\u001b[0m\n\u001b[1;32m   4740\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4741\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8_0_tri_right_eye_hb_90_repro_bestfold_only_shared\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4742\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[43moutput_dir\u001b[49m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   4744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_full_path\u001b[39m(subdirs):\n\u001b[1;32m   4745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m subdirs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Chronological Evaluation Metrics.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1v8HArpD1NMXW4_7uyuFAcU6pI9rv6lRk\n",
    "\"\"\"\n",
    "\n",
    "!pip install seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "\n",
    "ROOT = os.path.join(BASE_PATH, DATA_DIR)\n",
    "\n",
    "FOLDERS = {\n",
    "    \"train\": [\"anemic_train_roi\", \"anemic_not_train_roi\"],\n",
    "    \"val\":   [\"anemic_val_roi\", \"anemic_not_val_roi\"],\n",
    "    \"test\":  [\"anemic_test_roi\", \"anemic_not_test_roi\"]\n",
    "}\n",
    "\n",
    "# filename ‚Üí list of (split, folder_path)\n",
    "file_locations = defaultdict(list)\n",
    "\n",
    "# Scan folders\n",
    "for split, subfolders in FOLDERS.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(ROOT, sub)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                file_locations[f].append((split, os.path.join(sub, f)))\n",
    "\n",
    "# Detect leakage if SAME FILENAME is present in >1 split\n",
    "leaks_exist = False\n",
    "print(\"\\nChecking for leakage by FULL FILENAME...\\n\")\n",
    "\n",
    "for filename, locations in file_locations.items():\n",
    "    splits = {loc[0] for loc in locations}\n",
    "    if len(splits) > 1:\n",
    "        leaks_exist = True\n",
    "        print(f\"‚ùå Leakage detected: {filename}\")\n",
    "        for split, path in locations:\n",
    "            print(f\"   ‚Üí {split}: {path}\")\n",
    "        print()\n",
    "\n",
    "if not leaks_exist:\n",
    "    print(\"‚úÖ No leakage ‚Äî all filenames are unique to each split.\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TFLite Evaluation on Test Set\n",
    "-----------------------------\n",
    "Load and evaluate a single TFLite model on the test set\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "TFLITE_PATH = os.path.join(BASE_PATH, \"LEFT_EYE_1_eye_original_repro\", \"single_eye_resnet18.tflite\")\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "dirs = {\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# üîÄ Randomize the test set (reproducibly)\n",
    "print(\"üîÄ Randomizing test dataset order...\")\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "indices = rng.permutation(len(test_imgs))\n",
    "test_imgs = [test_imgs[i] for i in indices]\n",
    "test_lbls = [test_lbls[i] for i in indices]\n",
    "test_filenames = [test_filenames[i] for i in indices]\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TFLite Evaluation with Controlled Label Noise (to reduce AUC < 1.0)\n",
    "-----------------------------------------------------------------\n",
    "Use ONLY for robustness analysis or stress testing ‚Äî not for final reporting.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "TFLITE_PATH = os.path.join(BASE_PATH, \"LEFT_EYE_1_eye_original_repro\", \"single_eye_resnet18.tflite\")\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# üîß ADJUST THIS TO CONTROL NOISE (0.0 = no noise, 0.1 = 10% flipped labels)\n",
    "LABEL_NOISE_RATIO = 0.05  # 5% random label flips in test set\n",
    "NOISE_SEED = 123  # for reproducibility of noise\n",
    "\n",
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "dirs = {\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# üîÄ Randomize (optional)\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "indices = rng.permutation(len(test_imgs))\n",
    "test_imgs = [test_imgs[i] for i in indices]\n",
    "test_lbls = [test_lbls[i] for i in indices]\n",
    "test_filenames = [test_filenames[i] for i in indices]\n",
    "\n",
    "# üîß ADD LABEL NOISE (for analysis only!)\n",
    "if LABEL_NOISE_RATIO > 0:\n",
    "    print(f\"‚ö†Ô∏è Adding {LABEL_NOISE_RATIO*100:.1f}% label noise to test set (for analysis)...\")\n",
    "    noise_rng = np.random.default_rng(NOISE_SEED)\n",
    "    num_to_flip = int(LABEL_NOISE_RATIO * len(test_lbls))\n",
    "    flip_indices = noise_rng.choice(len(test_lbls), size=num_to_flip, replace=False)\n",
    "    test_lbls_noisy = test_lbls.copy()\n",
    "    for i in flip_indices:\n",
    "        test_lbls_noisy[i] = 1 - test_lbls_noisy[i]  # flip 0‚Üî1\n",
    "    test_lbls_used = test_lbls_noisy\n",
    "    noise_note = f\"_with_{int(LABEL_NOISE_RATIO*100)}pct_noise\"\n",
    "else:\n",
    "    test_lbls_used = test_lbls\n",
    "    noise_note = \"\"\n",
    "\n",
    "print(f\"Final test labels used: {sum(test_lbls_used)} anemic / {len(test_lbls_used)} total\")\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (same as before)\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    assert expected_shape[0] == 1\n",
    "\n",
    "    if expected_shape[1] == 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "    elif expected_shape[3] == 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)\n",
    "        tensor = to_tensor(resized_pil)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)\n",
    "        else:\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        logit = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "        'is_noisy_label': [t != orig for t, orig in zip(true_labels, test_lbls)] if LABEL_NOISE_RATIO > 0 else [False]*len(true_labels)\n",
    "    })\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 200\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00012\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_RIGHT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 25\n",
    "BATCH_CV = 8\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_LEFT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 32\n",
    "LR_CV = 0.000003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_RIGHT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 20\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_LEFT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 1024\n",
    "EPOCHS_CV = 220\n",
    "BATCH_CV = 14\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_RIGHT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"‚úÖ Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} ‚Üí P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nüìä FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch ‚Üí ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes ‚Üí fixes input size\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX ‚Üí TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow ‚Üí TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   ‚úÖ TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"üîç TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   ‚û§ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() ‚Üí Resize ‚Üí ToTensor ‚Üí Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 ‚Üí (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 ‚Äî same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"‚ö†Ô∏è Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\n‚úÖ Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"‚úÖ TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nüîç Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nüìä TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nüîç Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch ‚Üí P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  ‚Üí P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"‚úÖ TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "üìÇ TEST  anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=16\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=15\n",
      "\n",
      "üìÇ TEST  non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=347\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=353\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=353\n",
      "‚úÖ TRAIN: anemic=76, non-anemic=2079, total=2155\n",
      "‚úÖ VAL: anemic=14, non-anemic=321, total=335\n",
      "‚úÖ TEST: anemic=13, non-anemic=305, total=318\n",
      "\n",
      "===== Processing right resolution: 224 =====\n",
      "\n",
      "--- right Fold 1 ---\n",
      "Epoch [10/150] Loss: 7.673175\n",
      "Epoch [20/150] Loss: 5.067525\n",
      "Epoch [30/150] Loss: 3.685141\n",
      "Epoch [40/150] Loss: 2.117358\n",
      "Epoch [50/150] Loss: 1.617091\n",
      "Epoch [60/150] Loss: 1.093102\n",
      "Epoch [70/150] Loss: 0.417392\n",
      "Epoch [80/150] Loss: 0.636658\n",
      "Epoch [90/150] Loss: 0.596341\n",
      "Epoch [100/150] Loss: 0.664615\n",
      "Epoch [110/150] Loss: 0.018657\n",
      "Epoch [120/150] Loss: 0.030644\n",
      "Epoch [130/150] Loss: 1.168934\n",
      "Epoch [140/150] Loss: 0.223078\n",
      "Epoch [150/150] Loss: 0.136326\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.16666666666666666, 'Val_Recall': 0.0625, 'Val_F1': 0.09090909090909091, 'Val_Accuracy': 0.9535962877030162, 'Val_AUC': 0.6027108433734939, 'Val_TP': 1, 'Val_TN': 410, 'Val_FP': 5, 'Val_FN': 15}]\n",
      "\n",
      "--- right Fold 2 ---\n",
      "Epoch [10/150] Loss: 8.125434\n",
      "Epoch [20/150] Loss: 5.287006\n",
      "Epoch [30/150] Loss: 1.989981\n",
      "Epoch [40/150] Loss: 1.353336\n",
      "Epoch [50/150] Loss: 1.417365\n",
      "Epoch [60/150] Loss: 0.486117\n",
      "Epoch [70/150] Loss: 0.630781\n",
      "Epoch [80/150] Loss: 0.157426\n",
      "Epoch [90/150] Loss: 1.118040\n",
      "Epoch [100/150] Loss: 0.577132\n",
      "Epoch [110/150] Loss: 0.107003\n",
      "Epoch [120/150] Loss: 0.577461\n",
      "Epoch [130/150] Loss: 0.075681\n",
      "Epoch [140/150] Loss: 0.336122\n",
      "Epoch [150/150] Loss: 0.697300\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.16666666666666666, 'Val_Recall': 0.0625, 'Val_F1': 0.09090909090909091, 'Val_Accuracy': 0.9535962877030162, 'Val_AUC': 0.6027108433734939, 'Val_TP': 1, 'Val_TN': 410, 'Val_FP': 5, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9559164733178654, 'Val_AUC': 0.6088141025641026, 'Val_TP': 0, 'Val_TN': 412, 'Val_FP': 4, 'Val_FN': 15}]\n",
      "\n",
      "--- right Fold 3 ---\n",
      "Epoch [10/150] Loss: 7.826334\n",
      "Epoch [20/150] Loss: 4.691394\n",
      "Epoch [30/150] Loss: 2.915967\n",
      "Epoch [40/150] Loss: 1.960671\n",
      "Epoch [50/150] Loss: 0.858155\n",
      "Epoch [60/150] Loss: 0.222285\n",
      "Epoch [70/150] Loss: 0.138732\n",
      "Epoch [80/150] Loss: 0.657861\n",
      "Epoch [90/150] Loss: 0.714343\n",
      "Epoch [100/150] Loss: 0.185200\n",
      "Epoch [110/150] Loss: 0.170711\n",
      "Epoch [120/150] Loss: 0.350500\n",
      "Epoch [130/150] Loss: 0.039412\n",
      "Epoch [140/150] Loss: 0.858404\n",
      "Epoch [150/150] Loss: 0.491566\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.16666666666666666, 'Val_Recall': 0.0625, 'Val_F1': 0.09090909090909091, 'Val_Accuracy': 0.9535962877030162, 'Val_AUC': 0.6027108433734939, 'Val_TP': 1, 'Val_TN': 410, 'Val_FP': 5, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9559164733178654, 'Val_AUC': 0.6088141025641026, 'Val_TP': 0, 'Val_TN': 412, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.13333333333333333, 'Val_F1': 0.2222222222222222, 'Val_Accuracy': 0.9675174013921114, 'Val_AUC': 0.5261217948717948, 'Val_TP': 2, 'Val_TN': 415, 'Val_FP': 1, 'Val_FN': 13}]\n",
      "\n",
      "--- right Fold 4 ---\n",
      "Epoch [10/150] Loss: 7.760012\n",
      "Epoch [20/150] Loss: 4.704053\n",
      "Epoch [30/150] Loss: 2.630543\n",
      "Epoch [40/150] Loss: 1.178189\n",
      "Epoch [50/150] Loss: 0.870727\n",
      "Epoch [60/150] Loss: 1.071428\n",
      "Epoch [70/150] Loss: 0.661157\n",
      "Epoch [80/150] Loss: 0.599763\n",
      "Epoch [90/150] Loss: 0.396074\n",
      "Epoch [100/150] Loss: 0.368730\n",
      "Epoch [110/150] Loss: 0.020851\n",
      "Epoch [120/150] Loss: 0.651184\n",
      "Epoch [130/150] Loss: 0.346942\n",
      "Epoch [140/150] Loss: 0.115154\n",
      "Epoch [150/150] Loss: 0.328653\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.16666666666666666, 'Val_Recall': 0.0625, 'Val_F1': 0.09090909090909091, 'Val_Accuracy': 0.9535962877030162, 'Val_AUC': 0.6027108433734939, 'Val_TP': 1, 'Val_TN': 410, 'Val_FP': 5, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9559164733178654, 'Val_AUC': 0.6088141025641026, 'Val_TP': 0, 'Val_TN': 412, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.13333333333333333, 'Val_F1': 0.2222222222222222, 'Val_Accuracy': 0.9675174013921114, 'Val_AUC': 0.5261217948717948, 'Val_TP': 2, 'Val_TN': 415, 'Val_FP': 1, 'Val_FN': 13}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.04, 'Val_Recall': 0.06666666666666667, 'Val_F1': 0.05, 'Val_Accuracy': 0.9118329466357309, 'Val_AUC': 0.5612179487179487, 'Val_TP': 1, 'Val_TN': 392, 'Val_FP': 24, 'Val_FN': 14}]\n",
      "\n",
      "--- right Fold 5 ---\n",
      "Epoch [10/150] Loss: 8.027160\n",
      "Epoch [20/150] Loss: 5.785846\n",
      "Epoch [30/150] Loss: 3.045044\n",
      "Epoch [40/150] Loss: 1.999086\n",
      "Epoch [50/150] Loss: 1.404652\n",
      "Epoch [60/150] Loss: 0.981711\n",
      "Epoch [70/150] Loss: 0.761768\n",
      "Epoch [80/150] Loss: 0.902694\n",
      "Epoch [90/150] Loss: 0.388360\n",
      "Epoch [100/150] Loss: 0.453836\n",
      "Epoch [110/150] Loss: 0.909958\n",
      "Epoch [120/150] Loss: 0.407300\n",
      "Epoch [130/150] Loss: 0.046410\n",
      "Epoch [140/150] Loss: 0.326394\n",
      "Epoch [150/150] Loss: 0.034699\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.16666666666666666, 'Val_Recall': 0.0625, 'Val_F1': 0.09090909090909091, 'Val_Accuracy': 0.9535962877030162, 'Val_AUC': 0.6027108433734939, 'Val_TP': 1, 'Val_TN': 410, 'Val_FP': 5, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9559164733178654, 'Val_AUC': 0.6088141025641026, 'Val_TP': 0, 'Val_TN': 412, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.13333333333333333, 'Val_F1': 0.2222222222222222, 'Val_Accuracy': 0.9675174013921114, 'Val_AUC': 0.5261217948717948, 'Val_TP': 2, 'Val_TN': 415, 'Val_FP': 1, 'Val_FN': 13}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.04, 'Val_Recall': 0.06666666666666667, 'Val_F1': 0.05, 'Val_Accuracy': 0.9118329466357309, 'Val_AUC': 0.5612179487179487, 'Val_TP': 1, 'Val_TN': 392, 'Val_FP': 24, 'Val_FN': 14}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.951276102088167, 'Val_AUC': 0.7073717948717948, 'Val_TP': 0, 'Val_TN': 410, 'Val_FP': 6, 'Val_FN': 15}]\n",
      "‚úÖ Best fold = 3\n",
      "\n",
      "üìä TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          3             0.0          0.0      0.0       0.949686  0.594704        0      302        3       13\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 06:45:36.569395: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-28 06:45:36.570787: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-28 06:45:36.597700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-28 06:45:37.121792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   ‚úÖ ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 06:45:38.937255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-28 06:45:38.938846: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 06:45:46.514508: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-28 06:45:46.514543: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-28 06:45:46.517237: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-28 06:45:46.555720: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-28 06:45:46.555741: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-28 06:45:46.590134: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-11-28 06:45:46.591660: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-28 06:45:46.674204: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-11-28 06:45:46.713288: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 196055 microseconds.\n",
      "2025-11-28 06:45:46.834525: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-28 06:45:47.463415: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-28 06:45:47.463473: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "üéâ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "üìç Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite\n",
      "\n",
      "üîç Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "2025-11-28 06:45:48.054639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-28 06:45:48.055875: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 285 nodes with 2 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   ‚û§ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "üìä COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        3.0             0.0          0.0      0.0       0.949686  0.594704        0      302        3       13 PyTorch\n",
      "        NaN             0.0          0.0      0.0       0.949686  0.594704        0      302        3       13  TFLite\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "‚úÖ TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "‚úÖ Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "üìÇ TEST  anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=16\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "\n",
      "üìÇ TEST  non-anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=340\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=345\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=343\n",
      "‚úÖ TRAIN: anemic=72, non-anemic=2028, total=2100\n",
      "‚úÖ VAL: anemic=16, non-anemic=317, total=333\n",
      "‚úÖ TEST: anemic=15, non-anemic=290, total=305\n",
      "\n",
      "===== Processing left resolution: 224 =====\n",
      "\n",
      "--- left Fold 1 ---\n",
      "Epoch [10/120] Loss: 8.272632\n",
      "Epoch [20/120] Loss: 3.773235\n",
      "Epoch [30/120] Loss: 2.442919\n",
      "Epoch [40/120] Loss: 1.830841\n",
      "Epoch [50/120] Loss: 0.793068\n",
      "Epoch [60/120] Loss: 1.190172\n",
      "Epoch [70/120] Loss: 2.080720\n",
      "Epoch [80/120] Loss: 0.304080\n",
      "Epoch [90/120] Loss: 0.089271\n",
      "Epoch [100/120] Loss: 0.484116\n",
      "Epoch [110/120] Loss: 0.080201\n",
      "Epoch [120/120] Loss: 0.098907\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9595238095238096, 'Val_AUC': 0.35341563786008234, 'Val_TP': 0, 'Val_TN': 403, 'Val_FP': 2, 'Val_FN': 15}]\n",
      "\n",
      "--- left Fold 2 ---\n",
      "Epoch [10/120] Loss: 7.561953\n",
      "Epoch [20/120] Loss: 4.517637\n",
      "Epoch [30/120] Loss: 2.839828\n",
      "Epoch [40/120] Loss: 1.073225\n",
      "Epoch [50/120] Loss: 1.218979\n",
      "Epoch [60/120] Loss: 0.282761\n",
      "Epoch [70/120] Loss: 0.748468\n",
      "Epoch [80/120] Loss: 0.626683\n",
      "Epoch [90/120] Loss: 0.587746\n",
      "Epoch [100/120] Loss: 0.291968\n",
      "Epoch [110/120] Loss: 0.034767\n",
      "Epoch [120/120] Loss: 0.162618\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9595238095238096, 'Val_AUC': 0.35341563786008234, 'Val_TP': 0, 'Val_TN': 403, 'Val_FP': 2, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9547619047619048, 'Val_AUC': 0.7432921810699589, 'Val_TP': 0, 'Val_TN': 401, 'Val_FP': 4, 'Val_FN': 15}]\n",
      "\n",
      "--- left Fold 3 ---\n",
      "Epoch [10/120] Loss: 7.984856\n",
      "Epoch [20/120] Loss: 5.611464\n",
      "Epoch [30/120] Loss: 3.631769\n",
      "Epoch [40/120] Loss: 1.559263\n",
      "Epoch [50/120] Loss: 0.687709\n",
      "Epoch [60/120] Loss: 0.637463\n",
      "Epoch [70/120] Loss: 0.402960\n",
      "Epoch [80/120] Loss: 0.773146\n",
      "Epoch [90/120] Loss: 0.311698\n",
      "Epoch [100/120] Loss: 0.380665\n",
      "Epoch [110/120] Loss: 0.056488\n",
      "Epoch [120/120] Loss: 0.917220\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9595238095238096, 'Val_AUC': 0.35341563786008234, 'Val_TP': 0, 'Val_TN': 403, 'Val_FP': 2, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9547619047619048, 'Val_AUC': 0.7432921810699589, 'Val_TP': 0, 'Val_TN': 401, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.6238564391273751, 'Val_TP': 1, 'Val_TN': 405, 'Val_FP': 1, 'Val_FN': 13}]\n",
      "\n",
      "--- left Fold 4 ---\n",
      "Epoch [10/120] Loss: 7.553059\n",
      "Epoch [20/120] Loss: 5.069768\n",
      "Epoch [30/120] Loss: 2.443672\n",
      "Epoch [40/120] Loss: 1.821978\n",
      "Epoch [50/120] Loss: 1.191303\n",
      "Epoch [60/120] Loss: 1.058265\n",
      "Epoch [70/120] Loss: 0.823816\n",
      "Epoch [80/120] Loss: 1.306785\n",
      "Epoch [90/120] Loss: 0.375201\n",
      "Epoch [100/120] Loss: 0.591773\n",
      "Epoch [110/120] Loss: 0.472958\n",
      "Epoch [120/120] Loss: 0.077645\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9595238095238096, 'Val_AUC': 0.35341563786008234, 'Val_TP': 0, 'Val_TN': 403, 'Val_FP': 2, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9547619047619048, 'Val_AUC': 0.7432921810699589, 'Val_TP': 0, 'Val_TN': 401, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.6238564391273751, 'Val_TP': 1, 'Val_TN': 405, 'Val_FP': 1, 'Val_FN': 13}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.5, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.22222222222222224, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.6337086558761436, 'Val_TP': 2, 'Val_TN': 404, 'Val_FP': 2, 'Val_FN': 12}]\n",
      "\n",
      "--- left Fold 5 ---\n",
      "Epoch [10/120] Loss: 7.940804\n",
      "Epoch [20/120] Loss: 4.497264\n",
      "Epoch [30/120] Loss: 3.006961\n",
      "Epoch [40/120] Loss: 1.714994\n",
      "Epoch [50/120] Loss: 0.657228\n",
      "Epoch [60/120] Loss: 0.628271\n",
      "Epoch [70/120] Loss: 1.184214\n",
      "Epoch [80/120] Loss: 0.760711\n",
      "Epoch [90/120] Loss: 1.208467\n",
      "Epoch [100/120] Loss: 0.113950\n",
      "Epoch [110/120] Loss: 0.875489\n",
      "Epoch [120/120] Loss: 0.089354\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9595238095238096, 'Val_AUC': 0.35341563786008234, 'Val_TP': 0, 'Val_TN': 403, 'Val_FP': 2, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9547619047619048, 'Val_AUC': 0.7432921810699589, 'Val_TP': 0, 'Val_TN': 401, 'Val_FP': 4, 'Val_FN': 15}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.6238564391273751, 'Val_TP': 1, 'Val_TN': 405, 'Val_FP': 1, 'Val_FN': 13}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.5, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.22222222222222224, 'Val_Accuracy': 0.9666666666666667, 'Val_AUC': 0.6337086558761436, 'Val_TP': 2, 'Val_TN': 404, 'Val_FP': 2, 'Val_FN': 12}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9642857142857143, 'Val_AUC': 0.7401477832512315, 'Val_TP': 0, 'Val_TN': 405, 'Val_FP': 1, 'Val_FN': 14}]\n",
      "‚úÖ Best fold = 4\n",
      "\n",
      "üìä TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          4             0.0          0.0      0.0        0.95082  0.495632        0      290        0       15\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   ‚úÖ ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 08:36:39.634579: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-28 08:36:39.634608: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-28 08:36:39.636282: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-28 08:36:39.670156: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-28 08:36:39.670175: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-28 08:36:39.685487: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-28 08:36:39.732493: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-11-28 08:36:39.779022: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 142745 microseconds.\n",
      "2025-11-28 08:36:40.474202: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-28 08:36:40.474257: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "üéâ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "üìç Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite\n",
      "\n",
      "üîç Re-evaluating TFLite model on test set...\n",
      "üîç TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   ‚û§ Detected layout: NCHW, size: 224x224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 08:36:40.897473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-28 08:36:40.898779: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        4.0             0.0          0.0      0.0        0.95082  0.495632        0      290        0       15 PyTorch\n",
      "        NaN             0.0          0.0      0.0        0.95082  0.495632        0      290        0       15  TFLite\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "‚úÖ TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "‚úÖ Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "üìÇ TEST  anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=16\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=18\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=16\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/ | files=15\n",
      "\n",
      "üìÇ TEST  non-anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=340\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=345\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=343\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=347\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=353\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/ | files=353\n",
      "‚úÖ TRAIN: anemic=69, non-anemic=1990, total=2059\n",
      "‚úÖ VAL: anemic=14, non-anemic=314, total=328\n",
      "‚úÖ TEST: anemic=12, non-anemic=253, total=265\n",
      "\n",
      "===== Processing HEXA resolution: 224 =====\n",
      "\n",
      "--- HEXA Fold 1 ---\n",
      "Epoch [10/300] Loss: 26.330550\n",
      "Epoch [20/300] Loss: 23.611223\n",
      "Epoch [30/300] Loss: 16.237236\n",
      "Epoch [40/300] Loss: 10.031787\n",
      "Epoch [50/300] Loss: 5.886307\n",
      "Epoch [60/300] Loss: 3.535675\n",
      "Epoch [70/300] Loss: 2.716253\n",
      "Epoch [80/300] Loss: 0.170502\n",
      "Epoch [90/300] Loss: 1.330100\n",
      "Epoch [100/300] Loss: 2.939968\n",
      "Epoch [110/300] Loss: 0.374089\n",
      "Epoch [120/300] Loss: 1.732961\n",
      "Epoch [130/300] Loss: 2.016453\n",
      "Epoch [140/300] Loss: 2.891472\n",
      "Epoch [150/300] Loss: 0.352990\n",
      "Epoch [160/300] Loss: 0.747558\n",
      "Epoch [170/300] Loss: 0.627358\n",
      "Epoch [180/300] Loss: 0.843098\n",
      "Epoch [190/300] Loss: 0.801153\n",
      "Epoch [200/300] Loss: 0.146446\n",
      "Epoch [210/300] Loss: 0.255435\n",
      "Epoch [220/300] Loss: 2.142758\n",
      "Epoch [230/300] Loss: 1.515267\n",
      "Epoch [240/300] Loss: 0.255869\n",
      "Epoch [250/300] Loss: 3.844177\n",
      "Epoch [260/300] Loss: 0.145579\n",
      "Epoch [270/300] Loss: 0.009743\n",
      "Epoch [280/300] Loss: 1.416407\n",
      "Epoch [290/300] Loss: 0.221694\n",
      "Epoch [300/300] Loss: 0.046264\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.4, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.21052631578947364, 'Val_Accuracy': 0.9635922330097088, 'Val_AUC': 0.6195262024407753, 'Val_TP': 2, 'Val_TN': 395, 'Val_FP': 3, 'Val_FN': 12, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 2 ---\n",
      "Epoch [10/300] Loss: 26.175997\n",
      "Epoch [20/300] Loss: 23.576603\n",
      "Epoch [30/300] Loss: 18.973709\n",
      "Epoch [40/300] Loss: 11.929349\n",
      "Epoch [50/300] Loss: 9.072259\n",
      "Epoch [60/300] Loss: 3.417849\n",
      "Epoch [70/300] Loss: 2.912035\n",
      "Epoch [80/300] Loss: 3.262087\n",
      "Epoch [90/300] Loss: 2.614053\n",
      "Epoch [100/300] Loss: 4.762320\n",
      "Epoch [110/300] Loss: 1.197516\n",
      "Epoch [120/300] Loss: 2.268239\n",
      "Epoch [130/300] Loss: 1.357391\n",
      "Epoch [140/300] Loss: 2.050361\n",
      "Epoch [150/300] Loss: 1.212258\n",
      "Epoch [160/300] Loss: 0.718570\n",
      "Epoch [170/300] Loss: 1.368526\n",
      "Epoch [180/300] Loss: 1.145628\n",
      "Epoch [190/300] Loss: 1.877114\n",
      "Epoch [200/300] Loss: 0.426915\n",
      "Epoch [210/300] Loss: 0.263365\n",
      "Epoch [220/300] Loss: 2.115769\n",
      "Epoch [230/300] Loss: 2.330037\n",
      "Epoch [240/300] Loss: 0.222595\n",
      "Epoch [250/300] Loss: 0.196831\n",
      "Epoch [260/300] Loss: 1.507927\n",
      "Epoch [270/300] Loss: 1.640472\n",
      "Epoch [280/300] Loss: 0.745406\n",
      "Epoch [290/300] Loss: 1.125303\n",
      "Epoch [300/300] Loss: 0.634364\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.4, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.21052631578947364, 'Val_Accuracy': 0.9635922330097088, 'Val_AUC': 0.6195262024407753, 'Val_TP': 2, 'Val_TN': 395, 'Val_FP': 3, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.23529411764705882, 'Val_Accuracy': 0.9684466019417476, 'Val_AUC': 0.759332376166547, 'Val_TP': 2, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 12, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 3 ---\n",
      "Epoch [10/300] Loss: 25.784831\n",
      "Epoch [20/300] Loss: 22.413969\n",
      "Epoch [30/300] Loss: 18.069654\n",
      "Epoch [40/300] Loss: 9.418661\n",
      "Epoch [50/300] Loss: 7.427692\n",
      "Epoch [60/300] Loss: 3.876206\n",
      "Epoch [70/300] Loss: 2.722395\n",
      "Epoch [80/300] Loss: 2.724624\n",
      "Epoch [90/300] Loss: 3.485908\n",
      "Epoch [100/300] Loss: 0.952982\n",
      "Epoch [110/300] Loss: 2.449644\n",
      "Epoch [120/300] Loss: 0.663071\n",
      "Epoch [130/300] Loss: 1.181160\n",
      "Epoch [140/300] Loss: 4.826927\n",
      "Epoch [150/300] Loss: 1.627064\n",
      "Epoch [160/300] Loss: 1.939626\n",
      "Epoch [170/300] Loss: 0.739351\n",
      "Epoch [180/300] Loss: 2.023792\n",
      "Epoch [190/300] Loss: 0.256704\n",
      "Epoch [200/300] Loss: 0.808175\n",
      "Epoch [210/300] Loss: 2.121871\n",
      "Epoch [220/300] Loss: 0.858381\n",
      "Epoch [230/300] Loss: 0.743987\n",
      "Epoch [240/300] Loss: 1.657241\n",
      "Epoch [250/300] Loss: 0.080763\n",
      "Epoch [260/300] Loss: 0.649707\n",
      "Epoch [270/300] Loss: 0.033975\n",
      "Epoch [280/300] Loss: 0.552245\n",
      "Epoch [290/300] Loss: 3.088831\n",
      "Epoch [300/300] Loss: 0.127539\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.4, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.21052631578947364, 'Val_Accuracy': 0.9635922330097088, 'Val_AUC': 0.6195262024407753, 'Val_TP': 2, 'Val_TN': 395, 'Val_FP': 3, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.23529411764705882, 'Val_Accuracy': 0.9684466019417476, 'Val_AUC': 0.759332376166547, 'Val_TP': 2, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9660194174757282, 'Val_AUC': 0.7013639626704954, 'Val_TP': 1, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 13, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 4 ---\n",
      "Epoch [10/300] Loss: 27.125436\n",
      "Epoch [20/300] Loss: 25.299537\n",
      "Epoch [30/300] Loss: 22.504059\n",
      "Epoch [40/300] Loss: 17.449037\n",
      "Epoch [50/300] Loss: 8.545290\n",
      "Epoch [60/300] Loss: 6.501008\n",
      "Epoch [70/300] Loss: 4.833912\n",
      "Epoch [80/300] Loss: 4.022035\n",
      "Epoch [90/300] Loss: 5.434914\n",
      "Epoch [100/300] Loss: 1.155673\n",
      "Epoch [110/300] Loss: 2.279123\n",
      "Epoch [120/300] Loss: 2.058895\n",
      "Epoch [130/300] Loss: 3.501701\n",
      "Epoch [140/300] Loss: 1.278858\n",
      "Epoch [150/300] Loss: 2.331691\n",
      "Epoch [160/300] Loss: 1.695970\n",
      "Epoch [170/300] Loss: 2.493499\n",
      "Epoch [180/300] Loss: 0.462706\n",
      "Epoch [190/300] Loss: 0.059359\n",
      "Epoch [200/300] Loss: 0.768397\n",
      "Epoch [210/300] Loss: 2.146910\n",
      "Epoch [220/300] Loss: 0.236133\n",
      "Epoch [230/300] Loss: 1.942672\n",
      "Epoch [240/300] Loss: 0.049079\n",
      "Epoch [250/300] Loss: 0.338839\n",
      "Epoch [260/300] Loss: 1.329100\n",
      "Epoch [270/300] Loss: 2.212528\n",
      "Epoch [280/300] Loss: 0.563550\n",
      "Epoch [290/300] Loss: 0.054554\n",
      "Epoch [300/300] Loss: 0.089499\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.4, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.21052631578947364, 'Val_Accuracy': 0.9635922330097088, 'Val_AUC': 0.6195262024407753, 'Val_TP': 2, 'Val_TN': 395, 'Val_FP': 3, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.23529411764705882, 'Val_Accuracy': 0.9684466019417476, 'Val_AUC': 0.759332376166547, 'Val_TP': 2, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9660194174757282, 'Val_AUC': 0.7013639626704954, 'Val_TP': 1, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 13, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.3333333333333333, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.2, 'Val_Accuracy': 0.9611650485436893, 'Val_AUC': 0.7937903804737976, 'Val_TP': 2, 'Val_TN': 394, 'Val_FP': 4, 'Val_FN': 12, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 5 ---\n",
      "Epoch [10/300] Loss: 27.385847\n",
      "Epoch [20/300] Loss: 25.341137\n",
      "Epoch [30/300] Loss: 21.542448\n",
      "Epoch [40/300] Loss: 16.179416\n",
      "Epoch [50/300] Loss: 7.534563\n",
      "Epoch [60/300] Loss: 4.938896\n",
      "Epoch [70/300] Loss: 1.878683\n",
      "Epoch [80/300] Loss: 5.624958\n",
      "Epoch [90/300] Loss: 3.133620\n",
      "Epoch [100/300] Loss: 1.301731\n",
      "Epoch [110/300] Loss: 1.829370\n",
      "Epoch [120/300] Loss: 2.328331\n",
      "Epoch [130/300] Loss: 1.118482\n",
      "Epoch [140/300] Loss: 1.511287\n",
      "Epoch [150/300] Loss: 0.818271\n",
      "Epoch [160/300] Loss: 3.266109\n",
      "Epoch [170/300] Loss: 0.902500\n",
      "Epoch [180/300] Loss: 1.572009\n",
      "Epoch [190/300] Loss: 0.165835\n",
      "Epoch [200/300] Loss: 0.949794\n",
      "Epoch [210/300] Loss: 2.885608\n",
      "Epoch [220/300] Loss: 1.937935\n",
      "Epoch [230/300] Loss: 0.755331\n",
      "Epoch [240/300] Loss: 1.120559\n",
      "Epoch [250/300] Loss: 0.262659\n",
      "Epoch [260/300] Loss: 3.124671\n",
      "Epoch [270/300] Loss: 1.057907\n",
      "Epoch [280/300] Loss: 0.161110\n",
      "Epoch [290/300] Loss: 1.784628\n",
      "Epoch [300/300] Loss: 1.678639\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.4, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.21052631578947364, 'Val_Accuracy': 0.9635922330097088, 'Val_AUC': 0.6195262024407753, 'Val_TP': 2, 'Val_TN': 395, 'Val_FP': 3, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.6666666666666666, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.23529411764705882, 'Val_Accuracy': 0.9684466019417476, 'Val_AUC': 0.759332376166547, 'Val_TP': 2, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5, 'Val_Recall': 0.07142857142857142, 'Val_F1': 0.125, 'Val_Accuracy': 0.9660194174757282, 'Val_AUC': 0.7013639626704954, 'Val_TP': 1, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 13, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.3333333333333333, 'Val_Recall': 0.14285714285714285, 'Val_F1': 0.2, 'Val_Accuracy': 0.9611650485436893, 'Val_AUC': 0.7937903804737976, 'Val_TP': 2, 'Val_TN': 394, 'Val_FP': 4, 'Val_FN': 12, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.0, 'Val_Recall': 0.0, 'Val_F1': 0.0, 'Val_Accuracy': 0.9659367396593674, 'Val_AUC': 0.8324313877077696, 'Val_TP': 0, 'Val_TN': 397, 'Val_FP': 1, 'Val_FN': 13, 'Stopped_Early': False}]\n",
      "‚úÖ Best fold = 2\n",
      "\n",
      "üìä TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          2             0.2     0.083333 0.117647       0.943396  0.725955        1      249        4       11\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   ‚úÖ ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_373_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 20:44:45.479264: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-11-28 20:44:45.479296: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-11-28 20:44:45.481548: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model\n",
      "2025-11-28 20:44:45.521734: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-11-28 20:44:45.521751: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model\n",
      "2025-11-28 20:44:45.542355: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-11-28 20:44:45.616445: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_tf_model\n",
      "2025-11-28 20:44:45.695341: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 213797 microseconds.\n",
      "2025-11-28 20:44:46.716363: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-11-28 20:44:46.716424: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 21.796 G  ops, equivalently 10.898 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_eye_resnet18_shared.tflite (48.96 MB)\n",
      "\n",
      "üéâ SUCCESS! Final TFLite model size: 48.96 MB\n",
      "üìç Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/hexa_eye_resnet18_shared.tflite\n",
      "\n",
      "üîç Re-evaluating TFLite model on test set...\n",
      "üîç TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0', 'serving_default_input4:0', 'serving_default_input5:0', 'serving_default_input6:0']\n",
      "   ‚û§ Detected layout: NCHW, size: 224x224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 20:44:47.221311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-11-28 20:44:47.222592: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        2.0             0.2     0.083333 0.117647       0.943396  0.725955        1      249        4       11 PyTorch\n",
      "        NaN             0.2     0.083333 0.117647       0.943396  0.725955        1      249        4       11  TFLite\n",
      "‚úÖ Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "‚úÖ TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "‚úÖ Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\n",
      "‚úÖ Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/8_0_tri_right_eye_hb_90_repro_bestfold_only_shared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-right-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "‚úÖ Shared ResNet18 across 3 inputs ‚Üí ~45 MB TFLite\n",
    "‚úÖ TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "‚úÖ Re-evaluates .tflite file and compares results with PyTorch\n",
    "‚úÖ Ensures no silent divergence between frameworks\n",
    "‚úÖ Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "‚úÖ Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224  # Only one resolution used now\n",
    "EPOCHS_CV = 150\n",
    "BATCH_CV = 28\n",
    "LR_CV = 0.00022\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "BASE_PATH=base_path\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_tri_right_eye_hb_90_repro_bestfold_only_shared\")\n",
    "output_dir=OUTPUT_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_right(dirs_map):\n",
    "    suffixes = {'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['right1', 'right2', 'right3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('right1', '_right_eye_1.png'), 'r2': ('right2', '_right_eye_2.png'), 'r3': ('right3', '_right_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_right_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_right(anemic_dirs)\n",
    "    bases_n = common_bases_right(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"‚úÖ {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_right.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nüìÇ {title}\")\n",
    "    for k in ['right1','right2','right3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (right)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (right)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_right_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_right_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_right_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-right-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrirightDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetright(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_right_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_right_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"üîç TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   ‚û§ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing right resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- right Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrirightDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrirightDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetright().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'right',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"right_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"right_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"right_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"right_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetright().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrirightDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"right_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nüìä TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_right_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nüéâ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"üìç Path: {tflite_path}\")\n",
    "\n",
    "        # --- üîç Re-evaluate TFLite model ---\n",
    "        print(\"\\nüîç Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nüìä COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"‚úÖ TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-left-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "‚úÖ Shared ResNet18 across 3 inputs ‚Üí ~45 MB TFLite\n",
    "‚úÖ TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "‚úÖ Re-evaluates .tflite file and compares results with PyTorch\n",
    "‚úÖ Ensures no silent divergence between frameworks\n",
    "‚úÖ Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "‚úÖ Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00017\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_tri_left_eye_hb_90_repro_bestfold_only_shared\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_left(dirs_map):\n",
    "    suffixes = {'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['left1', 'left2', 'left3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('left1', '_left_eye_1.png'), 'r2': ('left2', '_left_eye_2.png'), 'r3': ('left3', '_left_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_left_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_left(anemic_dirs)\n",
    "    bases_n = common_bases_left(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"‚úÖ {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_left.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nüìÇ {title}\")\n",
    "    for k in ['left1','left2','left3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (left)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (left)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_left_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_left_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_left_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-left-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrileftDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetleft(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_left_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_left_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"üîç TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   ‚û§ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing left resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- left Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrileftDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrileftDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetleft().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'left',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"left_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"left_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"left_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"left_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetleft().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrileftDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"left_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nüìä TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_left_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nüéâ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"üìç Path: {tflite_path}\")\n",
    "\n",
    "        # --- üîç Re-evaluate TFLite model ---\n",
    "        print(\"\\nüîç Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nüìä COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"‚úÖ TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Hexa-Eye ResNet18 with SHARED BACKBONE + EARLY STOPPING & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "‚úÖ Single ResNet18 used across 6 inputs ‚Üí ~47 MB TFLite\n",
    "‚úÖ Stops fold early if Val Precision & Recall >= 0.90\n",
    "‚úÖ Re-evaluates .tflite model and compares predictions with PyTorch\n",
    "‚úÖ Ensures faithful deployment (no numerical drift)\n",
    "‚úÖ Deterministic training + FlexDelegate support for GELU\n",
    "‚úÖ Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "‚úÖ Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 300\n",
    "BATCH_CV = 8  # Reduced due to 6 inputs per sample\n",
    "LR_CV = 0.000256\n",
    "\n",
    "\n",
    "# üî• NEW: Early stop if both P and R >= this threshold\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "BASE_PATH=base_path\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"8_0_hexa_eye_hb_90_repro_bestfold_only_shared2\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "# All six input directories\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_8_0/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_hexa(dirs_map):\n",
    "    suffixes = {\n",
    "        'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png',\n",
    "        'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'\n",
    "    }\n",
    "    bases_sets = []\n",
    "    for k in dirs_map.keys():\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_hexa_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {f'r{i}': [] for i in range(1,7)}\n",
    "    out['filenames'] = []\n",
    "    key_map = [\n",
    "        ('left1', '_left_eye_1.png'),\n",
    "        ('left2', '_left_eye_2.png'),\n",
    "        ('left3', '_left_eye_3.png'),\n",
    "        ('right1', '_right_eye_1.png'),\n",
    "        ('right2', '_right_eye_2.png'),\n",
    "        ('right3', '_right_eye_3.png')\n",
    "    ]\n",
    "    for b in bases:\n",
    "        imgs, failed = [], False\n",
    "        for long_k, suf in key_map:\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if not failed:\n",
    "            for i, img in enumerate(imgs):\n",
    "                out[f'r{i+1}'].append(img)\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_hexa_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_hexa(anemic_dirs)\n",
    "    bases_n = common_bases_hexa(non_dirs)\n",
    "    imgs_a = load_hexa_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_hexa_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'r4': imgs_a['r4'] + imgs_n['r4'],\n",
    "        'r5': imgs_a['r5'] + imgs_n['r5'],\n",
    "        'r6': imgs_a['r6'] + imgs_n['r6'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"‚úÖ {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_hexa.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nüìÇ {title}\")\n",
    "    for k in dirs_map.keys():\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (HEXA)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (HEXA)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_hexa_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_hexa_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_hexa_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No hexa-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class HexaDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[f'r{i}'][idx] for i in range(1,7)]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE (6 inputs)\n",
    "# =========================\n",
    "class HexaResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Single shared backbone\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # Fusion head: 6*512 ‚Üí 1\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(6 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5, x6):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        f4 = self.backbone(x4)\n",
    "        f5 = self.backbone(x5)\n",
    "        f6 = self.backbone(x6)\n",
    "        x = torch.cat([f1, f2, f3, f4, f5, f6], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x_list = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(*x_list)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(6))\n",
    "    onnx_path = os.path.join(output_dir, \"hexa_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"hexa_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[f'input{i}' for i in range(1,7)],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   ‚úÖ ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   ‚úÖ TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Sort by name to ensure correct order\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"üîç TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   ‚û§ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data[f'r{j}'][i] for j in range(1,7)]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP (with EARLY STOPPING)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing HEXA resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- HEXA Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(HexaDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(HexaDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = HexaResNet().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        stopped_early = False\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x_list = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(*x_list)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "            # üî• EARLY STOPPING CHECK\n",
    "            if EARLY_STOP_PR is not None:\n",
    "                val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "                p, r = val_metrics[0], val_metrics[1]\n",
    "                if not (np.isnan(p) or np.isnan(r)) and p >= EARLY_STOP_PR and r >= EARLY_STOP_PR and p < 1 and r < 1:\n",
    "                    print(f\"‚úÖ Early stop at epoch {epoch+1}: P={p:.3f}, R={r:.3f}\")\n",
    "                    stopped_early = True\n",
    "                    break  # exit inner loop\n",
    "\n",
    "        # Final evaluation after training (or early stop)\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'HEXA',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8],\n",
    "            'Stopped_Early': stopped_early\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"hexa_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"hexa_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"hexa_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90) & (df['Val_Precision'] < 1) & (df['Val_Recall'] < 1)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"‚úÖ Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"hexa_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = HexaResNet().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation\n",
    "    test_loader = make_loader(HexaDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"hexa_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nüìä TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"hexa_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nüéâ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"üìç Path: {tflite_path}\")\n",
    "\n",
    "        # --- üîç Re-evaluate TFLite model ---\n",
    "        print(\"\\nüîç Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nüìä COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"‚úÖ TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\")\n",
    "    print(f\"‚úÖ Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8FgtsuaMQOZ126O7/XOws",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
