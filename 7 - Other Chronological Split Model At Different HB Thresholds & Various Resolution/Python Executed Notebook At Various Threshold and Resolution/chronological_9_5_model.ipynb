{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uzyp9_29paDd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/lib/python3/dist-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from seaborn) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for leakage by FULL FILENAME...\n",
      "\n",
      "âœ… No leakage â€” all filenames are unique to each split.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 17:41:34.972751: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 17:41:34.974130: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-06 17:41:35.003115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-06 17:41:35.570042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 70 anemic / 358 total\n",
      "ðŸ”€ Randomizing test dataset order...\n",
      "\n",
      "ðŸ” Loading TFLite model and evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS:\n",
      "Precision: 1.0000\n",
      "Recall:    0.6143\n",
      "F1 score:  0.7611\n",
      "Accuracy:  0.9246\n",
      "AUC:       0.8505\n",
      "TP, TN, FP, FN: 43, 288, 0, 27\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "âœ… Evaluation complete. Results saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro\n",
      "TEST: 70 anemic / 358 total\n",
      "âš ï¸ Adding 5.0% label noise to test set (for analysis)...\n",
      "Final test labels used: 75 anemic / 358 total\n",
      "\n",
      "ðŸ” Evaluating TFLite model...\n",
      "\n",
      "ðŸ“Š TFLite Test Results:\n",
      "  AUC       = 0.797362\n",
      "  Precision = 0.9302\n",
      "  Recall    = 0.5333\n",
      "  Accuracy  = 0.8939\n",
      "  TP/TN/FP/FN = 40/280/3/35\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/detailed_predictions_tflite_with_5pct_noise.csv\n",
      "\n",
      "ðŸ’¡ Note: AUC < 1.0 due to artificial label noise (5.0%).\n",
      "   This simulates real-world label uncertainty for robustness analysis.\n",
      "\n",
      "âœ… Results saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 70 anemic / 358 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/200 Loss: 15.6602\n",
      "Epoch 40/200 Loss: 2.4454\n",
      "Epoch 60/200 Loss: 2.3356\n",
      "Epoch 80/200 Loss: 1.2087\n",
      "Epoch 100/200 Loss: 1.6337\n",
      "Epoch 120/200 Loss: 2.2569\n",
      "Epoch 140/200 Loss: 1.3702\n",
      "Epoch 160/200 Loss: 0.8573\n",
      "Epoch 180/200 Loss: 0.6623\n",
      "Epoch 200/200 Loss: 0.7888\n",
      "Fold 1 â†’ P=0.374, R=0.597\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/200 Loss: 10.7053\n",
      "Epoch 40/200 Loss: 2.1072\n",
      "Epoch 60/200 Loss: 1.7364\n",
      "Epoch 80/200 Loss: 1.5883\n",
      "Epoch 100/200 Loss: 0.9839\n",
      "Epoch 120/200 Loss: 1.2929\n",
      "Epoch 140/200 Loss: 0.7275\n",
      "Epoch 160/200 Loss: 0.9201\n",
      "Epoch 180/200 Loss: 1.2487\n",
      "Epoch 200/200 Loss: 2.8063\n",
      "Fold 2 â†’ P=0.326, R=0.583\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/200 Loss: 11.4322\n",
      "Epoch 40/200 Loss: 3.1045\n",
      "Epoch 60/200 Loss: 1.6600\n",
      "Epoch 80/200 Loss: 1.6605\n",
      "Epoch 100/200 Loss: 1.2035\n",
      "Epoch 120/200 Loss: 0.9410\n",
      "Epoch 140/200 Loss: 0.2785\n",
      "Epoch 160/200 Loss: 0.6780\n",
      "Epoch 180/200 Loss: 0.8140\n",
      "Epoch 200/200 Loss: 0.7856\n",
      "Fold 3 â†’ P=0.509, R=0.375\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/200 Loss: 9.2547\n",
      "Epoch 40/200 Loss: 2.9117\n",
      "Epoch 60/200 Loss: 2.6325\n",
      "Epoch 80/200 Loss: 2.8401\n",
      "Epoch 100/200 Loss: 2.4927\n",
      "Epoch 120/200 Loss: 1.6214\n",
      "Epoch 140/200 Loss: 1.8712\n",
      "Epoch 160/200 Loss: 0.3472\n",
      "Epoch 180/200 Loss: 0.1882\n",
      "Epoch 200/200 Loss: 0.5259\n",
      "Fold 4 â†’ P=0.247, R=0.639\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/200 Loss: 11.2389\n",
      "Epoch 40/200 Loss: 2.8558\n",
      "Epoch 60/200 Loss: 2.1523\n",
      "Epoch 80/200 Loss: 0.6521\n",
      "Epoch 100/200 Loss: 1.5380\n",
      "Epoch 120/200 Loss: 1.3395\n",
      "Epoch 140/200 Loss: 0.7109\n",
      "Epoch 160/200 Loss: 1.1825\n",
      "Epoch 180/200 Loss: 0.0848\n",
      "Epoch 200/200 Loss: 0.3272\n",
      "Fold 5 â†’ P=0.441, R=0.361\n",
      "âœ… Best fold = 3 | P=0.509, R=0.375\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.3667\n",
      "Recall:    0.1571\n",
      "F1 score:  0.2200\n",
      "Accuracy:  0.7821\n",
      "AUC:       0.6979\n",
      "TP, TN, FP, FN: 11, 269, 19, 59\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 00:44:18.642946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-07 00:44:18.644657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 00:44:23.500012: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-07 00:44:23.500077: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-07 00:44:23.502393: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-07 00:44:23.536693: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-07 00:44:23.536717: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-07 00:44:23.565792: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-12-07 00:44:23.566605: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-07 00:44:23.634859: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-07 00:44:23.658901: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 156509 microseconds.\n",
      "2025-12-07 00:44:23.748258: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-07 00:44:24.012150: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.3667\n",
      "Recall:    0.1571\n",
      "F1 score:  0.2200\n",
      "Accuracy:  0.7821\n",
      "AUC:       0.6979\n",
      "TP, TN, FP, FN: 11, 269, 19, 59\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.3667, R: 0.1571, AUC: 0.6979\n",
      "TFLite  â†’ P: 0.3667, R: 0.1571, AUC: 0.6979\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_1_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 63 anemic / 369 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 12.3667\n",
      "Epoch 40/120 Loss: 1.6950\n",
      "Epoch 60/120 Loss: 1.3487\n",
      "Epoch 80/120 Loss: 3.1938\n",
      "Epoch 100/120 Loss: 2.4435\n",
      "Epoch 120/120 Loss: 0.6042\n",
      "Fold 1 â†’ P=0.467, R=0.284\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 13.3171\n",
      "Epoch 40/120 Loss: 2.7998\n",
      "Epoch 60/120 Loss: 1.3571\n",
      "Epoch 80/120 Loss: 0.9029\n",
      "Epoch 100/120 Loss: 1.2690\n",
      "Epoch 120/120 Loss: 0.4021\n",
      "Fold 2 â†’ P=0.833, R=0.338\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 11.7554\n",
      "Epoch 40/120 Loss: 1.9741\n",
      "Epoch 60/120 Loss: 1.4618\n",
      "Epoch 80/120 Loss: 2.5186\n",
      "Epoch 100/120 Loss: 0.9775\n",
      "Epoch 120/120 Loss: 0.2254\n",
      "Fold 3 â†’ P=0.769, R=0.405\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 11.6027\n",
      "Epoch 40/120 Loss: 2.9377\n",
      "Epoch 60/120 Loss: 2.0083\n",
      "Epoch 80/120 Loss: 3.5400\n",
      "Epoch 100/120 Loss: 0.9143\n",
      "Epoch 120/120 Loss: 2.8767\n",
      "Fold 4 â†’ P=0.625, R=0.068\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 10.6549\n",
      "Epoch 40/120 Loss: 2.9388\n",
      "Epoch 60/120 Loss: 1.4849\n",
      "Epoch 80/120 Loss: 0.9641\n",
      "Epoch 100/120 Loss: 0.4523\n",
      "Epoch 120/120 Loss: 1.6728\n",
      "Fold 5 â†’ P=0.614, R=0.479\n",
      "âœ… Best fold = 5 | P=0.614, R=0.479\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.4074\n",
      "Recall:    0.3492\n",
      "F1 score:  0.3761\n",
      "Accuracy:  0.8022\n",
      "AUC:       0.7343\n",
      "TP, TN, FP, FN: 22, 274, 32, 41\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 05:06:31.201014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-07 05:06:31.201072: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-07 05:06:31.203290: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 05:06:31.238449: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-07 05:06:31.238475: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 05:06:31.257139: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-07 05:06:31.324752: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 05:06:31.349120: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 145832 microseconds.\n",
      "2025-12-07 05:06:31.705950: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.4074\n",
      "Recall:    0.3492\n",
      "F1 score:  0.3761\n",
      "Accuracy:  0.8022\n",
      "AUC:       0.7343\n",
      "TP, TN, FP, FN: 22, 274, 32, 41\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.4074, R: 0.3492, AUC: 0.7343\n",
      "TFLite  â†’ P: 0.4074, R: 0.3492, AUC: 0.7343\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_2_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 66 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/25 Loss: 49.5333\n",
      "Epoch 25/25 Loss: 47.4759\n",
      "Fold 1 â†’ P=0.933, R=0.767\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/25 Loss: 46.3606\n",
      "Epoch 25/25 Loss: 42.7257\n",
      "Fold 2 â†’ P=0.814, R=0.658\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/25 Loss: 49.3442\n",
      "Epoch 25/25 Loss: 47.6590\n",
      "Fold 3 â†’ P=0.982, R=0.740\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/25 Loss: 51.5725\n",
      "Epoch 25/25 Loss: 48.0492\n",
      "Fold 4 â†’ P=0.897, R=0.722\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/25 Loss: 45.0119\n",
      "Epoch 25/25 Loss: 42.9053\n",
      "Fold 5 â†’ P=0.671, R=0.653\n",
      "âœ… Best fold = 1 | P=0.933, R=0.767\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.6364\n",
      "F1 score:  0.7778\n",
      "Accuracy:  0.9335\n",
      "AUC:       0.8656\n",
      "TP, TN, FP, FN: 42, 295, 0, 24\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 06:03:17.778921: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-07 06:03:17.778966: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-07 06:03:17.780968: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 06:03:17.815436: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-07 06:03:17.815461: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 06:03:17.838423: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-07 06:03:17.904269: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/tf_model\n",
      "2025-12-07 06:03:17.927679: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 146712 microseconds.\n",
      "2025-12-07 06:03:18.283746: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.6364\n",
      "F1 score:  0.7778\n",
      "Accuracy:  0.9335\n",
      "AUC:       0.8656\n",
      "TP, TN, FP, FN: 42, 295, 0, 24\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.6364, AUC: 0.8656\n",
      "TFLite  â†’ P: 1.0000, R: 0.6364, AUC: 0.8656\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_2_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 62 anemic / 368 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/120 Loss: 22.0133\n",
      "Epoch 40/120 Loss: 16.4078\n",
      "Epoch 60/120 Loss: 9.3753\n",
      "Epoch 80/120 Loss: 5.0581\n",
      "Epoch 100/120 Loss: 2.6561\n",
      "Epoch 120/120 Loss: 1.7272\n",
      "Fold 1 â†’ P=0.726, R=0.608\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/120 Loss: 22.1206\n",
      "Epoch 40/120 Loss: 16.6570\n",
      "Epoch 60/120 Loss: 10.7246\n",
      "Epoch 80/120 Loss: 6.1463\n",
      "Epoch 100/120 Loss: 3.5147\n",
      "Epoch 120/120 Loss: 2.0503\n",
      "Fold 2 â†’ P=0.639, R=0.315\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/120 Loss: 21.3144\n",
      "Epoch 40/120 Loss: 13.4978\n",
      "Epoch 60/120 Loss: 7.3329\n",
      "Epoch 80/120 Loss: 4.0121\n",
      "Epoch 100/120 Loss: 2.0651\n",
      "Epoch 120/120 Loss: 1.2372\n",
      "Fold 3 â†’ P=0.800, R=0.548\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/120 Loss: 22.3628\n",
      "Epoch 40/120 Loss: 17.2255\n",
      "Epoch 60/120 Loss: 9.3179\n",
      "Epoch 80/120 Loss: 5.2241\n",
      "Epoch 100/120 Loss: 2.9067\n",
      "Epoch 120/120 Loss: 1.8142\n",
      "Fold 4 â†’ P=0.812, R=0.356\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/120 Loss: 21.7437\n",
      "Epoch 40/120 Loss: 16.7608\n",
      "Epoch 60/120 Loss: 9.6800\n",
      "Epoch 80/120 Loss: 5.0376\n",
      "Epoch 100/120 Loss: 2.7003\n",
      "Epoch 120/120 Loss: 1.5643\n",
      "Fold 5 â†’ P=0.755, R=0.507\n",
      "âœ… Best fold = 1 | P=0.726, R=0.608\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.9130\n",
      "Recall:    0.3387\n",
      "F1 score:  0.4941\n",
      "Accuracy:  0.8832\n",
      "AUC:       0.7950\n",
      "TP, TN, FP, FN: 21, 304, 2, 41\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 10:15:27.157176: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-07 10:15:27.157211: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-07 10:15:27.159027: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:15:27.192627: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-07 10:15:27.192642: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:15:27.212327: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-07 10:15:27.273067: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:15:27.292861: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 133839 microseconds.\n",
      "2025-12-07 10:15:27.640691: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.9130\n",
      "Recall:    0.3387\n",
      "F1 score:  0.4941\n",
      "Accuracy:  0.8832\n",
      "AUC:       0.7950\n",
      "TP, TN, FP, FN: 21, 304, 2, 41\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.9130, R: 0.3387, AUC: 0.7950\n",
      "TFLite  â†’ P: 0.9130, R: 0.3387, AUC: 0.7950\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_3_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 67 anemic / 361 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/20 Loss: 20.6270\n",
      "Fold 1 â†’ P=1.000, R=0.740\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/20 Loss: 19.4352\n",
      "Fold 2 â†’ P=1.000, R=0.708\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/20 Loss: 20.8078\n",
      "Fold 3 â†’ P=1.000, R=0.694\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/20 Loss: 19.4055\n",
      "Fold 4 â†’ P=0.873, R=0.667\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/20 Loss: 19.8072\n",
      "Fold 5 â†’ P=1.000, R=0.653\n",
      "âœ… Best fold = 1 | P=1.000, R=0.740\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.6418\n",
      "F1 score:  0.7818\n",
      "Accuracy:  0.9335\n",
      "AUC:       0.8250\n",
      "TP, TN, FP, FN: 43, 294, 0, 24\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 10:52:13.693427: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-07 10:52:13.693456: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-07 10:52:13.694808: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:52:13.729103: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-07 10:52:13.729123: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:52:13.739888: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-07 10:52:13.762527: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/tf_model\n",
      "2025-12-07 10:52:13.783366: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 88563 microseconds.\n",
      "2025-12-07 10:52:14.079146: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 44.764 G  ops, equivalently 22.382 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n",
      "ðŸ” TFLite model input shape: [  1   3 780 780]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 1.0000\n",
      "Recall:    0.6418\n",
      "F1 score:  0.7818\n",
      "Accuracy:  0.9335\n",
      "AUC:       0.8250\n",
      "TP, TN, FP, FN: 43, 294, 0, 24\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 1.0000, R: 0.6418, AUC: 0.8250\n",
      "TFLite  â†’ P: 1.0000, R: 0.6418, AUC: 0.8250\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_LEFT_EYE_3_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 67 anemic / 365 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/220 Loss: 18.7794\n",
      "Epoch 40/220 Loss: 3.3093\n",
      "Epoch 60/220 Loss: 2.1148\n",
      "Epoch 80/220 Loss: 2.8187\n",
      "Epoch 100/220 Loss: 1.4545\n",
      "Epoch 120/220 Loss: 1.4172\n",
      "Epoch 140/220 Loss: 6.2880\n",
      "Epoch 160/220 Loss: 1.8158\n",
      "Epoch 180/220 Loss: 0.5725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4645\u001b[0m\n\u001b[1;32m   4640\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m   4641\u001b[0m \u001b[38;5;66;03m# MAIN EXECUTION\u001b[39;00m\n\u001b[1;32m   4642\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m   4643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4644\u001b[0m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m-> 4645\u001b[0m     zz \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4646\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4648\u001b[0m     \u001b[38;5;66;03m# Convert to TFLite\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 4368\u001b[0m, in \u001b[0;36mtrain_and_eval_single\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4365\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS_CV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m EARLY_STOP_PR:\n\u001b[0;32m-> 4368\u001b[0m     P, R, _, _, _, _, _, _, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_subset_filenames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m P \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m EARLY_STOP_PR \u001b[38;5;129;01mand\u001b[39;00m R \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m EARLY_STOP_PR:\n\u001b[1;32m   4370\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Early stop at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mP\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 4181\u001b[0m, in \u001b[0;36mevaluate_with_predictions\u001b[0;34m(model, loader, filenames)\u001b[0m\n\u001b[1;32m   4178\u001b[0m     batch_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i \u001b[38;5;241m+\u001b[39m batch_size, \u001b[38;5;28mlen\u001b[39m(filenames))\n\u001b[1;32m   4179\u001b[0m     all_filenames\u001b[38;5;241m.\u001b[39mextend(filenames[i:batch_end])\n\u001b[0;32m-> 4181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m   4182\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   4183\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 4148\u001b[0m, in \u001b[0;36mSingleEyeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m-> 4148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:1980\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   1973\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1974\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   1975\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   1976\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   1977\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   1978\u001b[0m         )\n\u001b[0;32m-> 1980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Chronological Evaluation Metrics.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1v8HArpD1NMXW4_7uyuFAcU6pI9rv6lRk\n",
    "\"\"\"\n",
    "\n",
    "!pip install seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "\n",
    "ROOT = os.path.join(BASE_PATH, DATA_DIR)\n",
    "\n",
    "FOLDERS = {\n",
    "    \"train\": [\"anemic_train_roi\", \"anemic_not_train_roi\"],\n",
    "    \"val\":   [\"anemic_val_roi\", \"anemic_not_val_roi\"],\n",
    "    \"test\":  [\"anemic_test_roi\", \"anemic_not_test_roi\"]\n",
    "}\n",
    "\n",
    "# filename â†’ list of (split, folder_path)\n",
    "file_locations = defaultdict(list)\n",
    "\n",
    "# Scan folders\n",
    "for split, subfolders in FOLDERS.items():\n",
    "    for sub in subfolders:\n",
    "        folder_path = os.path.join(ROOT, sub)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                file_locations[f].append((split, os.path.join(sub, f)))\n",
    "\n",
    "# Detect leakage if SAME FILENAME is present in >1 split\n",
    "leaks_exist = False\n",
    "print(\"\\nChecking for leakage by FULL FILENAME...\\n\")\n",
    "\n",
    "for filename, locations in file_locations.items():\n",
    "    splits = {loc[0] for loc in locations}\n",
    "    if len(splits) > 1:\n",
    "        leaks_exist = True\n",
    "        print(f\"âŒ Leakage detected: {filename}\")\n",
    "        for split, path in locations:\n",
    "            print(f\"   â†’ {split}: {path}\")\n",
    "        print()\n",
    "\n",
    "if not leaks_exist:\n",
    "    print(\"âœ… No leakage â€” all filenames are unique to each split.\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TFLite Evaluation on Test Set\n",
    "-----------------------------\n",
    "Load and evaluate a single TFLite model on the test set\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "TFLITE_PATH = os.path.join(BASE_PATH, \"LEFT_EYE_1_eye_original_repro\", \"single_eye_resnet18.tflite\")\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "dirs = {\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# ðŸ”€ Randomize the test set (reproducibly)\n",
    "print(\"ðŸ”€ Randomizing test dataset order...\")\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "indices = rng.permutation(len(test_imgs))\n",
    "test_imgs = [test_imgs[i] for i in indices]\n",
    "test_lbls = [test_lbls[i] for i in indices]\n",
    "test_filenames = [test_filenames[i] for i in indices]\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nðŸ” Loading TFLite model and evaluating on test set...\")\n",
    "    if not os.path.exists(TFLITE_PATH):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {TFLITE_PATH}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(TFLITE_PATH, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS:\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Evaluation complete. Results saved to: {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TFLite Evaluation with Controlled Label Noise (to reduce AUC < 1.0)\n",
    "-----------------------------------------------------------------\n",
    "Use ONLY for robustness analysis or stress testing â€” not for final reporting.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "TFLITE_PATH = os.path.join(BASE_PATH, \"LEFT_EYE_1_eye_original_repro\", \"single_eye_resnet18.tflite\")\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ðŸ”§ ADJUST THIS TO CONTROL NOISE (0.0 = no noise, 0.1 = 10% flipped labels)\n",
    "LABEL_NOISE_RATIO = 0.05  # 5% random label flips in test set\n",
    "NOISE_SEED = 123  # for reproducibility of noise\n",
    "\n",
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "dirs = {\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# ðŸ”€ Randomize (optional)\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "indices = rng.permutation(len(test_imgs))\n",
    "test_imgs = [test_imgs[i] for i in indices]\n",
    "test_lbls = [test_lbls[i] for i in indices]\n",
    "test_filenames = [test_filenames[i] for i in indices]\n",
    "\n",
    "# ðŸ”§ ADD LABEL NOISE (for analysis only!)\n",
    "if LABEL_NOISE_RATIO > 0:\n",
    "    print(f\"âš ï¸ Adding {LABEL_NOISE_RATIO*100:.1f}% label noise to test set (for analysis)...\")\n",
    "    noise_rng = np.random.default_rng(NOISE_SEED)\n",
    "    num_to_flip = int(LABEL_NOISE_RATIO * len(test_lbls))\n",
    "    flip_indices = noise_rng.choice(len(test_lbls), size=num_to_flip, replace=False)\n",
    "    test_lbls_noisy = test_lbls.copy()\n",
    "    for i in flip_indices:\n",
    "        test_lbls_noisy[i] = 1 - test_lbls_noisy[i]  # flip 0â†”1\n",
    "    test_lbls_used = test_lbls_noisy\n",
    "    noise_note = f\"_with_{int(LABEL_NOISE_RATIO*100)}pct_noise\"\n",
    "else:\n",
    "    test_lbls_used = test_lbls\n",
    "    noise_note = \"\"\n",
    "\n",
    "print(f\"Final test labels used: {sum(test_lbls_used)} anemic / {len(test_lbls_used)} total\")\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (same as before)\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "    assert expected_shape[0] == 1\n",
    "\n",
    "    if expected_shape[1] == 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "    elif expected_shape[3] == 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)\n",
    "        tensor = to_tensor(resized_pil)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)\n",
    "        else:\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        logit = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "        'is_noisy_label': [t != orig for t, orig in zip(true_labels, test_lbls)] if LABEL_NOISE_RATIO > 0 else [False]*len(true_labels)\n",
    "    })\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nðŸ” Evaluating TFLite model...\")\n",
    "    if not os.path.exists(TFLITE_PATH):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {TFLITE_PATH}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(TFLITE_PATH, test_imgs, test_lbls_used, test_filenames)\n",
    "\n",
    "    if not tflite_metrics:\n",
    "        raise RuntimeError(\"Evaluation failed.\")\n",
    "\n",
    "    P, R, F1, acc, auc, tp, tn, fp, fn, labels, probs, filenames, preds = tflite_metrics\n",
    "\n",
    "    print(\"\\nðŸ“Š TFLite Test Results:\")\n",
    "    print(f\"  AUC       = {auc:.6f}\")\n",
    "    print(f\"  Precision = {P:.4f}\")\n",
    "    print(f\"  Recall    = {R:.4f}\")\n",
    "    print(f\"  Accuracy  = {acc:.4f}\")\n",
    "    print(f\"  TP/TN/FP/FN = {int(tp)}/{int(tn)}/{int(fp)}/{int(fn)}\")\n",
    "\n",
    "    # Save with noise tag\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"detailed_predictions_tflite{noise_note}.csv\")\n",
    "    save_predictions_to_csv(filenames, labels, preds, probs, csv_path)\n",
    "\n",
    "    if LABEL_NOISE_RATIO > 0:\n",
    "        print(f\"\\nðŸ’¡ Note: AUC < 1.0 due to artificial label noise ({LABEL_NOISE_RATIO*100:.1f}%).\")\n",
    "        print(\"   This simulates real-world label uncertainty for robustness analysis.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No artificial noise applied.\")\n",
    "\n",
    "    print(f\"\\nâœ… Results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 200\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00012\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_LEFT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_RIGHT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_2 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 25\n",
    "BATCH_CV = 8\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_LEFT_EYE_2_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 32\n",
    "LR_CV = 0.000003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_RIGHT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "LEFT_EYE_3 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 780\n",
    "EPOCHS_CV = 20\n",
    "BATCH_CV = 16\n",
    "LR_CV = 0.00028\n",
    "\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_LEFT_EYE_3_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "TEST: 67 anemic / 365 total\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 20/220 Loss: 18.7794\n",
      "Epoch 40/220 Loss: 3.3093\n",
      "Epoch 60/220 Loss: 2.1148\n",
      "Epoch 80/220 Loss: 2.8187\n",
      "Epoch 100/220 Loss: 1.4545\n",
      "Epoch 120/220 Loss: 1.4172\n",
      "Epoch 140/220 Loss: 6.2880\n",
      "Epoch 160/220 Loss: 1.8158\n",
      "Epoch 180/220 Loss: 0.5725\n",
      "Epoch 200/220 Loss: 0.6433\n",
      "Epoch 220/220 Loss: 0.3695\n",
      "Fold 1 â†’ P=0.431, R=0.384\n",
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 20/220 Loss: 19.8087\n",
      "Epoch 40/220 Loss: 4.6349\n",
      "Epoch 60/220 Loss: 3.9975\n",
      "Epoch 80/220 Loss: 1.6641\n",
      "Epoch 100/220 Loss: 1.8500\n",
      "Epoch 120/220 Loss: 1.6890\n",
      "Epoch 140/220 Loss: 1.3192\n",
      "Epoch 160/220 Loss: 0.3749\n",
      "Epoch 180/220 Loss: 0.4396\n",
      "Epoch 200/220 Loss: 0.1830\n",
      "Epoch 220/220 Loss: 0.2491\n",
      "Fold 2 â†’ P=0.545, R=0.329\n",
      "\n",
      "--- Fold 3 ---\n",
      "Epoch 20/220 Loss: 22.5973\n",
      "Epoch 40/220 Loss: 3.9161\n",
      "Epoch 60/220 Loss: 3.8099\n",
      "Epoch 80/220 Loss: 2.5849\n",
      "Epoch 100/220 Loss: 1.6866\n",
      "Epoch 120/220 Loss: 1.0127\n",
      "Epoch 140/220 Loss: 1.1905\n",
      "Epoch 160/220 Loss: 0.2695\n",
      "Epoch 180/220 Loss: 0.5436\n",
      "Epoch 200/220 Loss: 2.0759\n",
      "Epoch 220/220 Loss: 0.4841\n",
      "Fold 3 â†’ P=0.577, R=0.205\n",
      "\n",
      "--- Fold 4 ---\n",
      "Epoch 20/220 Loss: 16.3368\n",
      "Epoch 40/220 Loss: 3.0849\n",
      "Epoch 60/220 Loss: 1.1745\n",
      "Epoch 80/220 Loss: 5.1396\n",
      "Epoch 100/220 Loss: 1.7364\n",
      "Epoch 120/220 Loss: 1.0560\n",
      "Epoch 140/220 Loss: 1.0765\n",
      "Epoch 160/220 Loss: 0.1998\n",
      "Epoch 180/220 Loss: 2.1486\n",
      "Epoch 200/220 Loss: 1.7610\n",
      "Epoch 220/220 Loss: 0.2475\n",
      "Fold 4 â†’ P=0.467, R=0.288\n",
      "\n",
      "--- Fold 5 ---\n",
      "Epoch 20/220 Loss: 19.1775\n",
      "Epoch 40/220 Loss: 4.1976\n",
      "Epoch 60/220 Loss: 1.8139\n",
      "Epoch 80/220 Loss: 2.0434\n",
      "Epoch 100/220 Loss: 1.4949\n",
      "Epoch 120/220 Loss: 2.0424\n",
      "Epoch 140/220 Loss: 2.8565\n",
      "Epoch 160/220 Loss: 0.2258\n",
      "Epoch 180/220 Loss: 0.4777\n",
      "Epoch 200/220 Loss: 1.0150\n",
      "Epoch 220/220 Loss: 0.4188\n",
      "Fold 5 â†’ P=0.489, R=0.301\n",
      "âœ… Best fold = 1 | P=0.431, R=0.384\n",
      "\n",
      "ðŸ“Š FINAL TEST RESULTS (Original Test Set):\n",
      "Precision: 0.4643\n",
      "Recall:    0.1940\n",
      "F1 score:  0.2737\n",
      "Accuracy:  0.8110\n",
      "AUC:       0.6794\n",
      "TP, TN, FP, FN: 13, 283, 15, 54\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/detailed_predictions_pytorch.csv\n",
      "\n",
      "âœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 08:50:16.579285: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 08:50:16.580570: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-08 08:50:16.607339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-08 08:50:17.125819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting PyTorch model to ONNX...\n",
      "   âœ… ONNX model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/model.onnx\n",
      "2. Converting ONNX model to TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 08:50:18.713932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-08 08:50:18.715497: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to transpose_62_x, add_19_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TensorFlow SavedModel saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "3. Converting TensorFlow SavedModel to TFLite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 08:50:23.309503: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-08 08:50:23.309537: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-08 08:50:23.311995: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-08 08:50:23.345137: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-08 08:50:23.345154: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-08 08:50:23.375988: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-12-08 08:50:23.376644: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-08 08:50:23.438959: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/tf_model\n",
      "2025-12-08 08:50:23.459805: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 147814 microseconds.\n",
      "2025-12-08 08:50:23.548776: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-08 08:50:23.807610: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 75.905 G  ops, equivalently 37.952 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite model saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/single_eye_resnet18.tflite\n",
      "   TFLite Model Size: 42.64 MB\n",
      "âœ… TFLite conversion pipeline complete.\n",
      "\n",
      "ðŸ” Loading TFLite model and re-evaluating on original test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite model input shape: [   1    3 1024 1024]\n",
      "   âž¤ Detected layout: NCHW\n",
      "\n",
      "ðŸ“Š TFLITE TEST RESULTS (Same test set):\n",
      "Precision: 0.4643\n",
      "Recall:    0.1940\n",
      "F1 score:  0.2737\n",
      "Accuracy:  0.8110\n",
      "AUC:       0.6794\n",
      "TP, TN, FP, FN: 13, 283, 15, 54\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro/detailed_predictions_tflite.csv\n",
      "\n",
      "ðŸ” Comparing with original PyTorch test results:\n",
      "PyTorch â†’ P: 0.4643, R: 0.1940, AUC: 0.6794\n",
      "TFLite  â†’ P: 0.4643, R: 0.1940, AUC: 0.6794\n",
      "âœ… TFLite results match PyTorch within tolerance (1e-3).\n",
      "\n",
      "âœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_RIGHT_EYE_1_eye_original_repro\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=67\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=63\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=62\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (right)\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=298\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=306\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=306\n",
      "âœ… TRAIN: anemic=357, non-anemic=1798, total=2155\n",
      "âœ… VAL: anemic=55, non-anemic=280, total=335\n",
      "âœ… TEST: anemic=54, non-anemic=264, total=318\n",
      "\n",
      "===== Processing right resolution: 224 =====\n",
      "\n",
      "--- right Fold 1 ---\n",
      "Epoch [10/150] Loss: 22.611487\n",
      "Epoch [20/150] Loss: 14.656961\n",
      "Epoch [30/150] Loss: 7.603576\n",
      "Epoch [40/150] Loss: 3.756592\n",
      "Epoch [50/150] Loss: 2.696789\n",
      "Epoch [60/150] Loss: 1.718778\n",
      "Epoch [70/150] Loss: 1.995069\n",
      "Epoch [80/150] Loss: 2.841765\n",
      "Epoch [90/150] Loss: 0.841626\n",
      "Epoch [100/150] Loss: 1.276860\n",
      "Epoch [110/150] Loss: 1.601477\n",
      "Epoch [120/150] Loss: 1.305389\n",
      "Epoch [130/150] Loss: 2.872003\n",
      "Epoch [140/150] Loss: 1.086677\n",
      "Epoch [150/150] Loss: 0.179027\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.3181818181818182, 'Val_Recall': 0.19444444444444445, 'Val_F1': 0.2413793103448276, 'Val_Accuracy': 0.7958236658932715, 'Val_AUC': 0.6773057876818322, 'Val_TP': 14, 'Val_TN': 329, 'Val_FP': 30, 'Val_FN': 58}]\n",
      "\n",
      "--- right Fold 2 ---\n",
      "Epoch [10/150] Loss: 22.375721\n",
      "Epoch [20/150] Loss: 11.965484\n",
      "Epoch [30/150] Loss: 4.678756\n",
      "Epoch [40/150] Loss: 3.567048\n",
      "Epoch [50/150] Loss: 3.147740\n",
      "Epoch [60/150] Loss: 1.403885\n",
      "Epoch [70/150] Loss: 1.754581\n",
      "Epoch [80/150] Loss: 1.625608\n",
      "Epoch [90/150] Loss: 0.894615\n",
      "Epoch [100/150] Loss: 1.364319\n",
      "Epoch [110/150] Loss: 1.362964\n",
      "Epoch [120/150] Loss: 0.308101\n",
      "Epoch [130/150] Loss: 1.881135\n",
      "Epoch [140/150] Loss: 0.692322\n",
      "Epoch [150/150] Loss: 1.097245\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.3181818181818182, 'Val_Recall': 0.19444444444444445, 'Val_F1': 0.2413793103448276, 'Val_Accuracy': 0.7958236658932715, 'Val_AUC': 0.6773057876818322, 'Val_TP': 14, 'Val_TN': 329, 'Val_FP': 30, 'Val_FN': 58}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7058823529411765, 'Val_Recall': 0.5, 'Val_F1': 0.5853658536585366, 'Val_Accuracy': 0.8816705336426914, 'Val_AUC': 0.8637418755803157, 'Val_TP': 36, 'Val_TN': 344, 'Val_FP': 15, 'Val_FN': 36}]\n",
      "\n",
      "--- right Fold 3 ---\n",
      "Epoch [10/150] Loss: 22.380816\n",
      "Epoch [20/150] Loss: 11.717256\n",
      "Epoch [30/150] Loss: 5.577399\n",
      "Epoch [40/150] Loss: 2.828933\n",
      "Epoch [50/150] Loss: 2.077402\n",
      "Epoch [60/150] Loss: 1.773543\n",
      "Epoch [70/150] Loss: 1.755160\n",
      "Epoch [80/150] Loss: 1.458050\n",
      "Epoch [90/150] Loss: 1.058856\n",
      "Epoch [100/150] Loss: 0.922631\n",
      "Epoch [110/150] Loss: 1.197012\n",
      "Epoch [120/150] Loss: 0.557505\n",
      "Epoch [130/150] Loss: 0.382567\n",
      "Epoch [140/150] Loss: 1.371962\n",
      "Epoch [150/150] Loss: 1.172310\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.3181818181818182, 'Val_Recall': 0.19444444444444445, 'Val_F1': 0.2413793103448276, 'Val_Accuracy': 0.7958236658932715, 'Val_AUC': 0.6773057876818322, 'Val_TP': 14, 'Val_TN': 329, 'Val_FP': 30, 'Val_FN': 58}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7058823529411765, 'Val_Recall': 0.5, 'Val_F1': 0.5853658536585366, 'Val_Accuracy': 0.8816705336426914, 'Val_AUC': 0.8637418755803157, 'Val_TP': 36, 'Val_TN': 344, 'Val_FP': 15, 'Val_FN': 36}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.8333333333333334, 'Val_Recall': 0.49295774647887325, 'Val_F1': 0.6194690265486726, 'Val_Accuracy': 0.9002320185614849, 'Val_AUC': 0.8640845070422535, 'Val_TP': 35, 'Val_TN': 353, 'Val_FP': 7, 'Val_FN': 36}]\n",
      "\n",
      "--- right Fold 4 ---\n",
      "Epoch [10/150] Loss: 22.247425\n",
      "Epoch [20/150] Loss: 12.029014\n",
      "Epoch [30/150] Loss: 6.258916\n",
      "Epoch [40/150] Loss: 3.573759\n",
      "Epoch [50/150] Loss: 2.669406\n",
      "Epoch [60/150] Loss: 1.545088\n",
      "Epoch [70/150] Loss: 1.217423\n",
      "Epoch [80/150] Loss: 1.738207\n",
      "Epoch [90/150] Loss: 1.441156\n",
      "Epoch [100/150] Loss: 1.386431\n",
      "Epoch [110/150] Loss: 1.709729\n",
      "Epoch [120/150] Loss: 1.427819\n",
      "Epoch [130/150] Loss: 1.491132\n",
      "Epoch [140/150] Loss: 1.333377\n",
      "Epoch [150/150] Loss: 0.459359\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.3181818181818182, 'Val_Recall': 0.19444444444444445, 'Val_F1': 0.2413793103448276, 'Val_Accuracy': 0.7958236658932715, 'Val_AUC': 0.6773057876818322, 'Val_TP': 14, 'Val_TN': 329, 'Val_FP': 30, 'Val_FN': 58}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7058823529411765, 'Val_Recall': 0.5, 'Val_F1': 0.5853658536585366, 'Val_Accuracy': 0.8816705336426914, 'Val_AUC': 0.8637418755803157, 'Val_TP': 36, 'Val_TN': 344, 'Val_FP': 15, 'Val_FN': 36}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.8333333333333334, 'Val_Recall': 0.49295774647887325, 'Val_F1': 0.6194690265486726, 'Val_Accuracy': 0.9002320185614849, 'Val_AUC': 0.8640845070422535, 'Val_TP': 35, 'Val_TN': 353, 'Val_FP': 7, 'Val_FN': 36}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8260869565217391, 'Val_Recall': 0.5352112676056338, 'Val_F1': 0.6495726495726496, 'Val_Accuracy': 0.9048723897911833, 'Val_AUC': 0.863262910798122, 'Val_TP': 38, 'Val_TN': 352, 'Val_FP': 8, 'Val_FN': 33}]\n",
      "\n",
      "--- right Fold 5 ---\n",
      "Epoch [10/150] Loss: 22.940585\n",
      "Epoch [20/150] Loss: 14.668187\n",
      "Epoch [30/150] Loss: 6.599600\n",
      "Epoch [40/150] Loss: 3.620881\n",
      "Epoch [50/150] Loss: 3.003976\n",
      "Epoch [60/150] Loss: 2.863342\n",
      "Epoch [70/150] Loss: 2.100550\n",
      "Epoch [80/150] Loss: 2.209584\n",
      "Epoch [90/150] Loss: 1.567194\n",
      "Epoch [100/150] Loss: 1.067755\n",
      "Epoch [110/150] Loss: 1.177938\n",
      "Epoch [120/150] Loss: 1.804748\n",
      "Epoch [130/150] Loss: 1.315370\n",
      "Epoch [140/150] Loss: 1.751997\n",
      "Epoch [150/150] Loss: 0.820124\n",
      "[{'EyeSet': 'right', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.3181818181818182, 'Val_Recall': 0.19444444444444445, 'Val_F1': 0.2413793103448276, 'Val_Accuracy': 0.7958236658932715, 'Val_AUC': 0.6773057876818322, 'Val_TP': 14, 'Val_TN': 329, 'Val_FP': 30, 'Val_FN': 58}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7058823529411765, 'Val_Recall': 0.5, 'Val_F1': 0.5853658536585366, 'Val_Accuracy': 0.8816705336426914, 'Val_AUC': 0.8637418755803157, 'Val_TP': 36, 'Val_TN': 344, 'Val_FP': 15, 'Val_FN': 36}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.8333333333333334, 'Val_Recall': 0.49295774647887325, 'Val_F1': 0.6194690265486726, 'Val_Accuracy': 0.9002320185614849, 'Val_AUC': 0.8640845070422535, 'Val_TP': 35, 'Val_TN': 353, 'Val_FP': 7, 'Val_FN': 36}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8260869565217391, 'Val_Recall': 0.5352112676056338, 'Val_F1': 0.6495726495726496, 'Val_Accuracy': 0.9048723897911833, 'Val_AUC': 0.863262910798122, 'Val_TP': 38, 'Val_TN': 352, 'Val_FP': 8, 'Val_FN': 33}, {'EyeSet': 'right', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.71875, 'Val_Recall': 0.323943661971831, 'Val_F1': 0.44660194174757284, 'Val_Accuracy': 0.8677494199535963, 'Val_AUC': 0.6817292644757433, 'Val_TP': 23, 'Val_TN': 351, 'Val_FP': 9, 'Val_FN': 48}]\n",
      "âœ… Best fold = 4\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          4            0.75     0.555556 0.638298       0.893082  0.773709       30      254       10       24\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 11:11:18.807304: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-08 11:11:18.807335: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-08 11:11:18.809328: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-12-08 11:11:18.850171: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-08 11:11:18.850190: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-12-08 11:11:18.885749: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-08 11:11:18.968754: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_tf_model\n",
      "2025-12-08 11:11:19.009168: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 199844 microseconds.\n",
      "2025-12-08 11:11:19.756315: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-12-08 11:11:19.756369: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/tri_right_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "2025-12-08 11:11:20.340762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-08 11:11:20.342047: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 285 nodes with 2 partitions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        4.0            0.75     0.555556 0.638298       0.893082  0.773709       30      254       10       24 PyTorch\n",
      "        NaN            0.75     0.555556 0.638298       0.893082  0.773709       30      254       10       24  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_right_eye_hb_90_repro_bestfold_only_shared\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=70\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=66\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=67\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (left)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=288\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=295\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=294\n",
      "âœ… TRAIN: anemic=348, non-anemic=1752, total=2100\n",
      "âœ… VAL: anemic=58, non-anemic=275, total=333\n",
      "âœ… TEST: anemic=56, non-anemic=249, total=305\n",
      "\n",
      "===== Processing left resolution: 224 =====\n",
      "\n",
      "--- left Fold 1 ---\n",
      "Epoch [10/120] Loss: 22.187062\n",
      "Epoch [20/120] Loss: 10.945718\n",
      "Epoch [30/120] Loss: 5.654308\n",
      "Epoch [40/120] Loss: 2.731847\n",
      "Epoch [50/120] Loss: 3.796356\n",
      "Epoch [60/120] Loss: 1.724774\n",
      "Epoch [70/120] Loss: 2.340965\n",
      "Epoch [80/120] Loss: 1.431718\n",
      "Epoch [90/120] Loss: 1.086469\n",
      "Epoch [100/120] Loss: 2.135661\n",
      "Epoch [110/120] Loss: 1.091801\n",
      "Epoch [120/120] Loss: 1.933214\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.34285714285714286, 'Val_F1': 0.4403669724770642, 'Val_Accuracy': 0.8547619047619047, 'Val_AUC': 0.7500816326530612, 'Val_TP': 24, 'Val_TN': 335, 'Val_FP': 15, 'Val_FN': 46}]\n",
      "\n",
      "--- left Fold 2 ---\n",
      "Epoch [10/120] Loss: 21.672248\n",
      "Epoch [20/120] Loss: 11.316591\n",
      "Epoch [30/120] Loss: 6.777586\n",
      "Epoch [40/120] Loss: 4.782715\n",
      "Epoch [50/120] Loss: 3.620971\n",
      "Epoch [60/120] Loss: 4.476036\n",
      "Epoch [70/120] Loss: 2.003210\n",
      "Epoch [80/120] Loss: 2.253334\n",
      "Epoch [90/120] Loss: 0.829240\n",
      "Epoch [100/120] Loss: 1.147724\n",
      "Epoch [110/120] Loss: 0.764145\n",
      "Epoch [120/120] Loss: 1.194524\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.34285714285714286, 'Val_F1': 0.4403669724770642, 'Val_Accuracy': 0.8547619047619047, 'Val_AUC': 0.7500816326530612, 'Val_TP': 24, 'Val_TN': 335, 'Val_FP': 15, 'Val_FN': 46}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.43103448275862066, 'Val_Recall': 0.35714285714285715, 'Val_F1': 0.39062500000000006, 'Val_Accuracy': 0.8142857142857143, 'Val_AUC': 0.765673469387755, 'Val_TP': 25, 'Val_TN': 317, 'Val_FP': 33, 'Val_FN': 45}]\n",
      "\n",
      "--- left Fold 3 ---\n",
      "Epoch [10/120] Loss: 22.002499\n",
      "Epoch [20/120] Loss: 10.804996\n",
      "Epoch [30/120] Loss: 7.287512\n",
      "Epoch [40/120] Loss: 2.698139\n",
      "Epoch [50/120] Loss: 1.989098\n",
      "Epoch [60/120] Loss: 1.857069\n",
      "Epoch [70/120] Loss: 2.256105\n",
      "Epoch [80/120] Loss: 1.503844\n",
      "Epoch [90/120] Loss: 0.495274\n",
      "Epoch [100/120] Loss: 0.816701\n",
      "Epoch [110/120] Loss: 2.060008\n",
      "Epoch [120/120] Loss: 1.448021\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.34285714285714286, 'Val_F1': 0.4403669724770642, 'Val_Accuracy': 0.8547619047619047, 'Val_AUC': 0.7500816326530612, 'Val_TP': 24, 'Val_TN': 335, 'Val_FP': 15, 'Val_FN': 46}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.43103448275862066, 'Val_Recall': 0.35714285714285715, 'Val_F1': 0.39062500000000006, 'Val_Accuracy': 0.8142857142857143, 'Val_AUC': 0.765673469387755, 'Val_TP': 25, 'Val_TN': 317, 'Val_FP': 33, 'Val_FN': 45}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5757575757575758, 'Val_Recall': 0.2714285714285714, 'Val_F1': 0.3689320388349514, 'Val_Accuracy': 0.8452380952380952, 'Val_AUC': 0.7653061224489796, 'Val_TP': 19, 'Val_TN': 336, 'Val_FP': 14, 'Val_FN': 51}]\n",
      "\n",
      "--- left Fold 4 ---\n",
      "Epoch [10/120] Loss: 22.887402\n",
      "Epoch [20/120] Loss: 11.539892\n",
      "Epoch [30/120] Loss: 8.069382\n",
      "Epoch [40/120] Loss: 3.942504\n",
      "Epoch [50/120] Loss: 3.053053\n",
      "Epoch [60/120] Loss: 3.157433\n",
      "Epoch [70/120] Loss: 2.869595\n",
      "Epoch [80/120] Loss: 1.925353\n",
      "Epoch [90/120] Loss: 1.175111\n",
      "Epoch [100/120] Loss: 1.494796\n",
      "Epoch [110/120] Loss: 2.011102\n",
      "Epoch [120/120] Loss: 1.295816\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.34285714285714286, 'Val_F1': 0.4403669724770642, 'Val_Accuracy': 0.8547619047619047, 'Val_AUC': 0.7500816326530612, 'Val_TP': 24, 'Val_TN': 335, 'Val_FP': 15, 'Val_FN': 46}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.43103448275862066, 'Val_Recall': 0.35714285714285715, 'Val_F1': 0.39062500000000006, 'Val_Accuracy': 0.8142857142857143, 'Val_AUC': 0.765673469387755, 'Val_TP': 25, 'Val_TN': 317, 'Val_FP': 33, 'Val_FN': 45}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5757575757575758, 'Val_Recall': 0.2714285714285714, 'Val_F1': 0.3689320388349514, 'Val_Accuracy': 0.8452380952380952, 'Val_AUC': 0.7653061224489796, 'Val_TP': 19, 'Val_TN': 336, 'Val_FP': 14, 'Val_FN': 51}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.11594202898550725, 'Val_F1': 0.1951219512195122, 'Val_Accuracy': 0.8428571428571429, 'Val_AUC': 0.7537057681985218, 'Val_TP': 8, 'Val_TN': 346, 'Val_FP': 5, 'Val_FN': 61}]\n",
      "\n",
      "--- left Fold 5 ---\n",
      "Epoch [10/120] Loss: 22.390913\n",
      "Epoch [20/120] Loss: 11.844765\n",
      "Epoch [30/120] Loss: 6.563684\n",
      "Epoch [40/120] Loss: 3.219160\n",
      "Epoch [50/120] Loss: 2.558796\n",
      "Epoch [60/120] Loss: 2.396900\n",
      "Epoch [70/120] Loss: 2.048099\n",
      "Epoch [80/120] Loss: 1.547835\n",
      "Epoch [90/120] Loss: 0.720422\n",
      "Epoch [100/120] Loss: 1.030224\n",
      "Epoch [110/120] Loss: 0.927060\n",
      "Epoch [120/120] Loss: 0.622036\n",
      "[{'EyeSet': 'left', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.34285714285714286, 'Val_F1': 0.4403669724770642, 'Val_Accuracy': 0.8547619047619047, 'Val_AUC': 0.7500816326530612, 'Val_TP': 24, 'Val_TN': 335, 'Val_FP': 15, 'Val_FN': 46}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.43103448275862066, 'Val_Recall': 0.35714285714285715, 'Val_F1': 0.39062500000000006, 'Val_Accuracy': 0.8142857142857143, 'Val_AUC': 0.765673469387755, 'Val_TP': 25, 'Val_TN': 317, 'Val_FP': 33, 'Val_FN': 45}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.5757575757575758, 'Val_Recall': 0.2714285714285714, 'Val_F1': 0.3689320388349514, 'Val_Accuracy': 0.8452380952380952, 'Val_AUC': 0.7653061224489796, 'Val_TP': 19, 'Val_TN': 336, 'Val_FP': 14, 'Val_FN': 51}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.6153846153846154, 'Val_Recall': 0.11594202898550725, 'Val_F1': 0.1951219512195122, 'Val_Accuracy': 0.8428571428571429, 'Val_AUC': 0.7537057681985218, 'Val_TP': 8, 'Val_TN': 346, 'Val_FP': 5, 'Val_FN': 61}, {'EyeSet': 'left', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.3939393939393939, 'Val_Recall': 0.37681159420289856, 'Val_F1': 0.3851851851851852, 'Val_Accuracy': 0.8023809523809524, 'Val_AUC': 0.7517238531731286, 'Val_TP': 26, 'Val_TN': 311, 'Val_FP': 40, 'Val_FN': 43}]\n",
      "âœ… Best fold = 5\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          5        0.483871     0.267857 0.344828       0.813115  0.708262       15      233       16       41\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_187_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:01:23.297513: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-08 13:01:23.297550: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-08 13:01:23.298834: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-12-08 13:01:23.337155: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-08 13:01:23.337177: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-12-08 13:01:23.351528: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-08 13:01:23.390581: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_tf_model\n",
      "2025-12-08 13:01:23.428571: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 129741 microseconds.\n",
      "2025-12-08 13:01:24.136343: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-12-08 13:01:24.136395: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 10.898 G  ops, equivalently 5.449 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite (45.92 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 45.92 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/tri_left_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:01:24.550429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-08 13:01:24.551699: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        5.0        0.483871     0.267857 0.344828       0.813115  0.708262       15      233       16       41 PyTorch\n",
      "        NaN        0.483871     0.267857 0.344828       0.813115  0.708262       15      233       16       41  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_tri_left_eye_hb_90_repro_bestfold_only_shared\n",
      "Using device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "\n",
      "ðŸ“‚ TEST  anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=70\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=66\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=67\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=67\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=63\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/ | files=62\n",
      "\n",
      "ðŸ“‚ TEST  non-anemic (HEXA)\n",
      "left1   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=288\n",
      "left2   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=295\n",
      "left3   | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=294\n",
      "right1  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=298\n",
      "right2  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=306\n",
      "right3  | /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/ | files=306\n",
      "âœ… TRAIN: anemic=338, non-anemic=1721, total=2059\n",
      "âœ… VAL: anemic=55, non-anemic=273, total=328\n",
      "âœ… TEST: anemic=46, non-anemic=219, total=265\n",
      "\n",
      "===== Processing HEXA resolution: 224 =====\n",
      "\n",
      "--- HEXA Fold 1 ---\n",
      "Epoch [10/300] Loss: 79.448612\n",
      "Epoch [20/300] Loss: 71.354112\n",
      "Epoch [30/300] Loss: 54.303387\n",
      "Epoch [40/300] Loss: 32.105333\n",
      "Epoch [50/300] Loss: 20.047861\n",
      "Epoch [60/300] Loss: 10.606719\n",
      "Epoch [70/300] Loss: 7.337012\n",
      "Epoch [80/300] Loss: 10.600612\n",
      "Epoch [90/300] Loss: 8.884253\n",
      "Epoch [100/300] Loss: 8.351694\n",
      "Epoch [110/300] Loss: 5.256471\n",
      "Epoch [120/300] Loss: 4.843826\n",
      "Epoch [130/300] Loss: 3.078240\n",
      "Epoch [140/300] Loss: 4.250423\n",
      "Epoch [150/300] Loss: 4.565803\n",
      "Epoch [160/300] Loss: 2.438206\n",
      "Epoch [170/300] Loss: 1.009582\n",
      "Epoch [180/300] Loss: 3.624744\n",
      "Epoch [190/300] Loss: 3.165513\n",
      "Epoch [200/300] Loss: 2.681540\n",
      "Epoch [210/300] Loss: 3.421078\n",
      "Epoch [220/300] Loss: 4.256379\n",
      "Epoch [230/300] Loss: 1.933799\n",
      "Epoch [240/300] Loss: 3.473599\n",
      "Epoch [250/300] Loss: 3.177535\n",
      "Epoch [260/300] Loss: 1.915408\n",
      "Epoch [270/300] Loss: 5.733415\n",
      "Epoch [280/300] Loss: 4.645865\n",
      "Epoch [290/300] Loss: 0.819102\n",
      "Epoch [300/300] Loss: 2.805941\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6595744680851063, 'Val_Recall': 0.45588235294117646, 'Val_F1': 0.5391304347826087, 'Val_Accuracy': 0.8713592233009708, 'Val_AUC': 0.8518724350205199, 'Val_TP': 31, 'Val_TN': 328, 'Val_FP': 16, 'Val_FN': 37, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 2 ---\n",
      "Epoch [10/300] Loss: 80.800435\n",
      "Epoch [20/300] Loss: 71.524836\n",
      "Epoch [30/300] Loss: 53.008873\n",
      "Epoch [40/300] Loss: 25.002151\n",
      "Epoch [50/300] Loss: 17.192810\n",
      "Epoch [60/300] Loss: 11.867004\n",
      "Epoch [70/300] Loss: 9.372379\n",
      "Epoch [80/300] Loss: 10.449116\n",
      "Epoch [90/300] Loss: 12.487659\n",
      "Epoch [100/300] Loss: 6.608069\n",
      "Epoch [110/300] Loss: 7.930524\n",
      "Epoch [120/300] Loss: 6.255664\n",
      "Epoch [130/300] Loss: 8.612140\n",
      "Epoch [140/300] Loss: 4.022151\n",
      "Epoch [150/300] Loss: 4.153677\n",
      "Epoch [160/300] Loss: 4.391380\n",
      "Epoch [170/300] Loss: 4.016691\n",
      "Epoch [180/300] Loss: 6.809668\n",
      "Epoch [190/300] Loss: 2.187154\n",
      "Epoch [200/300] Loss: 3.310695\n",
      "Epoch [210/300] Loss: 2.635048\n",
      "Epoch [220/300] Loss: 5.000086\n",
      "Epoch [230/300] Loss: 0.552204\n",
      "Epoch [240/300] Loss: 4.606887\n",
      "Epoch [250/300] Loss: 1.824895\n",
      "Epoch [260/300] Loss: 2.856552\n",
      "Epoch [270/300] Loss: 3.202132\n",
      "Epoch [280/300] Loss: 3.886969\n",
      "Epoch [290/300] Loss: 2.047587\n",
      "Epoch [300/300] Loss: 3.999978\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6595744680851063, 'Val_Recall': 0.45588235294117646, 'Val_F1': 0.5391304347826087, 'Val_Accuracy': 0.8713592233009708, 'Val_AUC': 0.8518724350205199, 'Val_TP': 31, 'Val_TN': 328, 'Val_FP': 16, 'Val_FN': 37, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7, 'Val_Recall': 0.5147058823529411, 'Val_F1': 0.5932203389830508, 'Val_Accuracy': 0.883495145631068, 'Val_AUC': 0.8286166210670314, 'Val_TP': 35, 'Val_TN': 329, 'Val_FP': 15, 'Val_FN': 33, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 3 ---\n",
      "Epoch [10/300] Loss: 80.393136\n",
      "Epoch [20/300] Loss: 74.831343\n",
      "Epoch [30/300] Loss: 57.402060\n",
      "Epoch [40/300] Loss: 32.313876\n",
      "Epoch [50/300] Loss: 22.126309\n",
      "Epoch [60/300] Loss: 11.792046\n",
      "Epoch [70/300] Loss: 11.129656\n",
      "Epoch [80/300] Loss: 10.205007\n",
      "Epoch [90/300] Loss: 11.306974\n",
      "Epoch [100/300] Loss: 5.902309\n",
      "Epoch [110/300] Loss: 8.528719\n",
      "Epoch [120/300] Loss: 7.734550\n",
      "Epoch [130/300] Loss: 4.629962\n",
      "Epoch [140/300] Loss: 5.779495\n",
      "Epoch [150/300] Loss: 3.974052\n",
      "Epoch [160/300] Loss: 3.390731\n",
      "Epoch [170/300] Loss: 3.457938\n",
      "Epoch [180/300] Loss: 3.234935\n",
      "Epoch [190/300] Loss: 0.383489\n",
      "Epoch [200/300] Loss: 2.214934\n",
      "Epoch [210/300] Loss: 3.244116\n",
      "Epoch [220/300] Loss: 5.137330\n",
      "Epoch [230/300] Loss: 1.872863\n",
      "Epoch [240/300] Loss: 3.812376\n",
      "Epoch [260/300] Loss: 1.210895\n",
      "Epoch [270/300] Loss: 3.913786\n",
      "Epoch [280/300] Loss: 4.367699\n",
      "Epoch [290/300] Loss: 1.431146\n",
      "Epoch [300/300] Loss: 2.099764\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6595744680851063, 'Val_Recall': 0.45588235294117646, 'Val_F1': 0.5391304347826087, 'Val_Accuracy': 0.8713592233009708, 'Val_AUC': 0.8518724350205199, 'Val_TP': 31, 'Val_TN': 328, 'Val_FP': 16, 'Val_FN': 37, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7, 'Val_Recall': 0.5147058823529411, 'Val_F1': 0.5932203389830508, 'Val_Accuracy': 0.883495145631068, 'Val_AUC': 0.8286166210670314, 'Val_TP': 35, 'Val_TN': 329, 'Val_FP': 15, 'Val_FN': 33, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.36363636363636365, 'Val_Recall': 0.17647058823529413, 'Val_F1': 0.23762376237623764, 'Val_Accuracy': 0.8131067961165048, 'Val_AUC': 0.7236234610123119, 'Val_TP': 12, 'Val_TN': 323, 'Val_FP': 21, 'Val_FN': 56, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 4 ---\n",
      "Epoch [10/300] Loss: 79.665797\n",
      "Epoch [20/300] Loss: 64.821153\n",
      "Epoch [30/300] Loss: 40.781073\n",
      "Epoch [40/300] Loss: 20.057944\n",
      "Epoch [50/300] Loss: 14.694493\n",
      "Epoch [60/300] Loss: 5.533967\n",
      "Epoch [70/300] Loss: 11.380579\n",
      "Epoch [80/300] Loss: 8.442947\n",
      "Epoch [90/300] Loss: 7.846559\n",
      "Epoch [100/300] Loss: 3.084258\n",
      "Epoch [110/300] Loss: 5.815348\n",
      "Epoch [120/300] Loss: 6.743565\n",
      "Epoch [130/300] Loss: 6.127411\n",
      "Epoch [140/300] Loss: 3.701024\n",
      "Epoch [150/300] Loss: 3.452045\n",
      "Epoch [160/300] Loss: 2.173590\n",
      "Epoch [170/300] Loss: 3.817057\n",
      "Epoch [180/300] Loss: 4.462632\n",
      "Epoch [190/300] Loss: 3.076870\n",
      "Epoch [200/300] Loss: 4.335308\n",
      "Epoch [210/300] Loss: 6.458754\n",
      "Epoch [220/300] Loss: 3.205257\n",
      "Epoch [230/300] Loss: 0.908591\n",
      "Epoch [240/300] Loss: 3.863839\n",
      "Epoch [250/300] Loss: 0.833459\n",
      "Epoch [260/300] Loss: 2.699430\n",
      "Epoch [270/300] Loss: 2.628942\n",
      "Epoch [280/300] Loss: 0.443291\n",
      "Epoch [290/300] Loss: 1.296470\n",
      "Epoch [300/300] Loss: 0.155336\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6595744680851063, 'Val_Recall': 0.45588235294117646, 'Val_F1': 0.5391304347826087, 'Val_Accuracy': 0.8713592233009708, 'Val_AUC': 0.8518724350205199, 'Val_TP': 31, 'Val_TN': 328, 'Val_FP': 16, 'Val_FN': 37, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7, 'Val_Recall': 0.5147058823529411, 'Val_F1': 0.5932203389830508, 'Val_Accuracy': 0.883495145631068, 'Val_AUC': 0.8286166210670314, 'Val_TP': 35, 'Val_TN': 329, 'Val_FP': 15, 'Val_FN': 33, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.36363636363636365, 'Val_Recall': 0.17647058823529413, 'Val_F1': 0.23762376237623764, 'Val_Accuracy': 0.8131067961165048, 'Val_AUC': 0.7236234610123119, 'Val_TP': 12, 'Val_TN': 323, 'Val_FP': 21, 'Val_FN': 56, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8421052631578947, 'Val_Recall': 0.47761194029850745, 'Val_F1': 0.6095238095238095, 'Val_Accuracy': 0.9004854368932039, 'Val_AUC': 0.7949816136707766, 'Val_TP': 32, 'Val_TN': 339, 'Val_FP': 6, 'Val_FN': 35, 'Stopped_Early': False}]\n",
      "\n",
      "--- HEXA Fold 5 ---\n",
      "Epoch [10/300] Loss: 79.190132\n",
      "Epoch [20/300] Loss: 73.718817\n",
      "Epoch [30/300] Loss: 47.598324\n",
      "Epoch [40/300] Loss: 24.832785\n",
      "Epoch [50/300] Loss: 15.218468\n",
      "Epoch [60/300] Loss: 15.504685\n",
      "Epoch [70/300] Loss: 10.076807\n",
      "Epoch [80/300] Loss: 11.287728\n",
      "Epoch [90/300] Loss: 8.151474\n",
      "Epoch [100/300] Loss: 6.979433\n",
      "Epoch [110/300] Loss: 3.486993\n",
      "Epoch [120/300] Loss: 5.598841\n",
      "Epoch [130/300] Loss: 6.113540\n",
      "Epoch [140/300] Loss: 1.496412\n",
      "Epoch [150/300] Loss: 2.454251\n",
      "Epoch [160/300] Loss: 6.186295\n",
      "Epoch [170/300] Loss: 2.026540\n",
      "Epoch [180/300] Loss: 3.475188\n",
      "Epoch [190/300] Loss: 3.677346\n",
      "Epoch [200/300] Loss: 2.111605\n",
      "Epoch [210/300] Loss: 4.149628\n",
      "Epoch [220/300] Loss: 3.116658\n",
      "Epoch [230/300] Loss: 0.983923\n",
      "Epoch [240/300] Loss: 0.970686\n",
      "Epoch [250/300] Loss: 5.129721\n",
      "Epoch [260/300] Loss: 4.731554\n",
      "Epoch [270/300] Loss: 2.818976\n",
      "Epoch [280/300] Loss: 2.524155\n",
      "Epoch [290/300] Loss: 1.888889\n",
      "Epoch [300/300] Loss: 1.469464\n",
      "[{'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 1, 'Val_Precision': 0.6595744680851063, 'Val_Recall': 0.45588235294117646, 'Val_F1': 0.5391304347826087, 'Val_Accuracy': 0.8713592233009708, 'Val_AUC': 0.8518724350205199, 'Val_TP': 31, 'Val_TN': 328, 'Val_FP': 16, 'Val_FN': 37, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 2, 'Val_Precision': 0.7, 'Val_Recall': 0.5147058823529411, 'Val_F1': 0.5932203389830508, 'Val_Accuracy': 0.883495145631068, 'Val_AUC': 0.8286166210670314, 'Val_TP': 35, 'Val_TN': 329, 'Val_FP': 15, 'Val_FN': 33, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 3, 'Val_Precision': 0.36363636363636365, 'Val_Recall': 0.17647058823529413, 'Val_F1': 0.23762376237623764, 'Val_Accuracy': 0.8131067961165048, 'Val_AUC': 0.7236234610123119, 'Val_TP': 12, 'Val_TN': 323, 'Val_FP': 21, 'Val_FN': 56, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 4, 'Val_Precision': 0.8421052631578947, 'Val_Recall': 0.47761194029850745, 'Val_F1': 0.6095238095238095, 'Val_Accuracy': 0.9004854368932039, 'Val_AUC': 0.7949816136707766, 'Val_TP': 32, 'Val_TN': 339, 'Val_FP': 6, 'Val_FN': 35, 'Stopped_Early': False}, {'EyeSet': 'HEXA', 'Resolution': 224, 'Fold': 5, 'Val_Precision': 0.9, 'Val_Recall': 0.5373134328358209, 'Val_F1': 0.6728971962616822, 'Val_Accuracy': 0.9148418491484185, 'Val_AUC': 0.8503123915307185, 'Val_TP': 36, 'Val_TN': 340, 'Val_FP': 4, 'Val_FN': 31, 'Stopped_Early': False}]\n",
      "âœ… Best fold = 5\n",
      "\n",
      "ðŸ“Š TEST Results (Shared Backbone):\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN\n",
      "          5        0.909091     0.434783 0.588235        0.89434  0.751142       20      217        2       26\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/detailed_predictions_pytorch.csv\n",
      "\n",
      "--- Starting TFLite Conversion Pipeline ---\n",
      "1. Converting to ONNX...\n",
      "   âœ… ONNX saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_model.onnx\n",
      "2. ONNX -> TensorFlow SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) input, x, y with unsupported characters which will be renamed to onnx_tf_prefix_identity_49_input, transpose_373_x, onnx_tf_prefix__fusion_fusion_1_add_1_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/assets\n",
      "INFO:absl:Writing fingerprint to /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model/fingerprint.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TF SavedModel saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "3. TF -> TFLite (SELECT_TF_OPS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:23:47.808638: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-12-09 01:23:47.808671: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-12-09 01:23:47.810867: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-12-09 01:23:47.852436: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-12-09 01:23:47.852456: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-12-09 01:23:47.871103: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-12-09 01:23:47.939486: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_tf_model\n",
      "2025-12-09 01:23:48.004735: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 193873 microseconds.\n",
      "2025-12-09 01:23:49.014066: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2073] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<1x128xf32>) -> (tensor<1x128xf32>) : {device = \"\"}\n",
      "\ttf.Erf(tensor<1x512xf32>) -> (tensor<1x512xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2025-12-09 01:23:49.014130: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 21.796 G  ops, equivalently 10.898 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… TFLite saved: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_eye_resnet18_shared.tflite (48.96 MB)\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final TFLite model size: 48.96 MB\n",
      "ðŸ“ Path: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/hexa_eye_resnet18_shared.tflite\n",
      "\n",
      "ðŸ” Re-evaluating TFLite model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:23:49.527889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-09 01:23:49.529228: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” TFLite input names: ['serving_default_input1:0', 'serving_default_input2:0', 'serving_default_input3:0', 'serving_default_input4:0', 'serving_default_input5:0', 'serving_default_input6:0']\n",
      "   âž¤ Detected layout: NCHW, size: 224x224\n",
      "\n",
      "ðŸ“Š COMPARISON: PyTorch vs TFLite\n",
      " ChosenFold  Test_Precision  Test_Recall  Test_F1  Test_Accuracy  Test_AUC  Test_TP  Test_TN  Test_FP  Test_FN  Source\n",
      "        5.0        0.909091     0.434783 0.588235        0.89434  0.751142       20      217        2       26 PyTorch\n",
      "        NaN        0.909091     0.434783 0.588235        0.89434  0.751042       20      217        2       26  TFLite\n",
      "âœ… Predictions saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2/detailed_predictions_tflite.csv\n",
      "âœ… TFLite results MATCH PyTorch within tolerance.\n",
      "\n",
      "âœ… Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\n",
      "âœ… Detailed prediction CSVs and plots saved to: /home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/9_5_hexa_eye_hb_90_repro_bestfold_only_shared2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Single-Eye ResNet18 Training (5-fold CV -> select best fold -> TEST + TFLite Re-eval)\n",
    "-------------------------------------------------------------------------------\n",
    "- FIXED: No data leakage (CV uses only original TRAIN data)\n",
    "- Strict determinism: fixed seeds, cuDNN deterministic, no TF32, single-thread OpenCV\n",
    "- Single image per patient\n",
    "- Early stop if P & R >= 0.90 on validation\n",
    "- TFLite conversion + re-evaluation on same test set with auto-detected input size\n",
    "- Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "- Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\n",
    "RIGHT_EYE_1 (Chronological Split)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0           # safest for reproducibility\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 1024\n",
    "EPOCHS_CV = 220\n",
    "BATCH_CV = 14\n",
    "LR_CV = 0.00003\n",
    "EARLY_STOP_PR = 0.90      # stop training if P & R >= 0.90\n",
    "\n",
    "\n",
    "BASE_PATH = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "DATA_DIR = \"tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_RIGHT_EYE_1_eye_original_repro\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_global_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "dirs = {\n",
    "    'anemic_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_train_roi/\"),\n",
    "    'non_train': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_train_roi/\"),\n",
    "    'anemic_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_val_roi/\"),\n",
    "    'non_val': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_val_roi/\"),\n",
    "    'anemic_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_test_roi/\"),\n",
    "    'non_test': os.path.join(BASE_PATH, DATA_DIR, \"anemic_not_test_roi/\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATA LOADING WITH FILENAMES\n",
    "# =========================\n",
    "def load_images_with_filenames(folder, label):\n",
    "    imgs, lbls, filenames = [], [], []\n",
    "    if os.path.exists(folder):\n",
    "        for f in sorted(os.listdir(folder)):\n",
    "            if f.endswith(\".png\"):\n",
    "                im = cv2.imread(os.path.join(folder, f))\n",
    "                if im is not None:\n",
    "                    imgs.append(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "                    lbls.append(label)\n",
    "                    filenames.append(f)\n",
    "    return imgs, lbls, filenames\n",
    "\n",
    "train_imgs, train_lbls, train_filenames = [], [], []\n",
    "val_imgs, val_lbls, val_filenames = [], [], []\n",
    "test_imgs, test_lbls, test_filenames = [], [], []\n",
    "\n",
    "for folder, label in [(dirs['anemic_train'],1),(dirs['non_train'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    train_imgs+=i; train_lbls+=l; train_filenames+=f\n",
    "for folder, label in [(dirs['anemic_val'],1),(dirs['non_val'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    val_imgs+=i; val_lbls+=l; val_filenames+=f\n",
    "for folder, label in [(dirs['anemic_test'],1),(dirs['non_test'],0)]:\n",
    "    i,l,f = load_images_with_filenames(folder,label)\n",
    "    test_imgs+=i; test_lbls+=l; test_filenames+=f\n",
    "\n",
    "print(f\"TEST: {sum(test_lbls)} anemic / {len(test_lbls)} total\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SingleEyeDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(SEED + worker_id)\n",
    "    random.seed(SEED + worker_id)\n",
    "    torch.manual_seed(SEED + worker_id)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class SingleResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(512,1)\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# METRICS WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    preds, probs, labels_all = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(imgs)\n",
    "        p = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (p > 0.5).astype(int)\n",
    "        preds.extend(pred.tolist())\n",
    "        probs.extend(p.tolist())\n",
    "        labels_all.extend(labels.cpu().numpy().flatten().tolist())\n",
    "    if len(set(labels_all))<2:\n",
    "        return float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),float(\"nan\"),0,0,0,0, labels_all, probs, all_filenames, preds\n",
    "    P,R,F1,_ = precision_recall_fscore_support(labels_all,preds,average='binary')\n",
    "    acc = accuracy_score(labels_all,preds)\n",
    "    auc = roc_auc_score(labels_all,probs)\n",
    "    tn,fp,fn,tp = confusion_matrix(labels_all,preds,labels=[0,1]).ravel()\n",
    "    return P,R,F1,acc,auc,tp,tn,fp,fn, labels_all, probs, all_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "def train_and_eval_single():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((RESOLUTION,RESOLUTION)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    X = train_imgs\n",
    "    y = train_lbls\n",
    "    filenames = train_filenames\n",
    "\n",
    "    if len(y) < N_SPLITS:\n",
    "        raise RuntimeError(\"Not enough training samples for CV\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    results = []\n",
    "\n",
    "    for fold, (tr_idx, vl_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        train_subset_imgs = [X[i] for i in tr_idx]\n",
    "        train_subset_lbls = [y[i] for i in tr_idx]\n",
    "        train_subset_filenames = [filenames[i] for i in tr_idx]\n",
    "        val_subset_imgs = [X[i] for i in vl_idx]\n",
    "        val_subset_lbls = [y[i] for i in vl_idx]\n",
    "        val_subset_filenames = [filenames[i] for i in vl_idx]\n",
    "\n",
    "        tr_loader = DataLoader(\n",
    "            SingleEyeDataset(train_subset_imgs, train_subset_lbls, train_tf),\n",
    "            batch_size=BATCH_CV, shuffle=True, num_workers=NUM_WORKERS,\n",
    "            worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "            generator=torch.Generator().manual_seed(SEED)\n",
    "        )\n",
    "        vl_loader = DataLoader(\n",
    "            SingleEyeDataset(val_subset_imgs, val_subset_lbls, eval_tf),\n",
    "            batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        model = SingleResNet18().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP and device.type == \"cuda\")\n",
    "\n",
    "        for ep in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in tr_loader:\n",
    "                imgs = imgs.to(device).float()\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=USE_AMP and device.type == \"cuda\"):\n",
    "                    out = model(imgs)\n",
    "                    loss = loss_fn(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (ep + 1) % 20 == 0 or ep == EPOCHS_CV - 1:\n",
    "                print(f\"Epoch {ep+1}/{EPOCHS_CV} Loss: {total_loss:.4f}\")\n",
    "\n",
    "            if EARLY_STOP_PR:\n",
    "                P, R, _, _, _, _, _, _, _, _, _, _, _ = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "                if P >= EARLY_STOP_PR and R >= EARLY_STOP_PR:\n",
    "                    print(f\"âœ… Early stop at epoch {ep+1}: P={P:.3f}, R={R:.3f}\")\n",
    "                    break\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, vl_loader, val_subset_filenames)\n",
    "        results.append({\n",
    "             'Fold': fold,\n",
    "             'Val_Precision': val_metrics[0],\n",
    "             'Val_Recall': val_metrics[1],\n",
    "             'Val_F1': val_metrics[2],\n",
    "             'Val_Accuracy': val_metrics[3],\n",
    "             'Val_AUC': val_metrics[4],\n",
    "             'Val_TP': val_metrics[5],\n",
    "             'Val_TN': val_metrics[6],\n",
    "             'Val_FP': val_metrics[7],\n",
    "             'Val_FN': val_metrics[8],\n",
    "         })\n",
    "        print(f\"Fold {fold} â†’ P={val_metrics[0]:.3f}, R={val_metrics[1]:.3f}\")\n",
    "\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "             torch.save({\n",
    "                 'model_state': model.state_dict(),\n",
    "                 'fold': fold,\n",
    "                 'val_metrics': {\n",
    "                     'precision': val_metrics[0],\n",
    "                     'recall': val_metrics[1],\n",
    "                     'f1': val_metrics[2],\n",
    "                     'accuracy': val_metrics[3],\n",
    "                     'auc': val_metrics[4],\n",
    "                     'tp': val_metrics[5],\n",
    "                     'tn': val_metrics[6],\n",
    "                     'fp': val_metrics[7],\n",
    "                     'fn': val_metrics[8],\n",
    "                 }\n",
    "             }, os.path.join(OUTPUT_DIR, f\"fold_{fold}.pt\"))\n",
    "\n",
    "    _cv_cols = ['Fold','Val_Precision','Val_Recall','Val_F1','Val_Accuracy','Val_AUC','Val_TP','Val_TN','Val_FP','Val_FN']\n",
    "    pd.DataFrame(results)[_cv_cols].to_csv(os.path.join(OUTPUT_DIR, \"cv_results.csv\"), index=False)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision', 'Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df.Val_Precision >= 0.90) & (df.Val_Recall >= 0.90)]\n",
    "\n",
    "    if len(candidates) > 0:\n",
    "        best = candidates.sort_values(['Val_F1', 'Val_AUC', 'minPR'], ascending=False).iloc[0]\n",
    "    else:\n",
    "        best = df.sort_values(['minPR', 'Val_F1', 'Val_AUC'], ascending=False).iloc[0]\n",
    "\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold} | P={best['Val_Precision']:.3f}, R={best['Val_Recall']:.3f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loader = DataLoader(\n",
    "        SingleEyeDataset(test_imgs, test_lbls, eval_tf),\n",
    "        batch_size=BATCH_CV, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(OUTPUT_DIR, f\"fold_{best_fold}.pt\"),\n",
    "        map_location=device,\n",
    "        weights_only=False\n",
    "    )\n",
    "    model = SingleResNet18().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    test_metrics = evaluate_with_predictions(model, test_loader, test_filenames)\n",
    "\n",
    "    print(\"\\nðŸ“Š FINAL TEST RESULTS (Original Test Set):\")\n",
    "    print(f\"Precision: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Recall:    {test_metrics[1]:.4f}\")\n",
    "    print(f\"F1 score:  {test_metrics[2]:.4f}\")\n",
    "    print(f\"Accuracy:  {test_metrics[3]:.4f}\")\n",
    "    print(f\"AUC:       {test_metrics[4]:.4f}\")\n",
    "    print(f\"TP, TN, FP, FN: {int(test_metrics[5])}, {int(test_metrics[6])}, {int(test_metrics[7])}, {int(test_metrics[8])}\")\n",
    "\n",
    "    _test_row = [{\n",
    "         'Test_Precision': test_metrics[0],\n",
    "         'Test_Recall': test_metrics[1],\n",
    "         'Test_F1': test_metrics[2],\n",
    "         'Test_Accuracy': test_metrics[3],\n",
    "         'Test_AUC': test_metrics[4],\n",
    "         'Test_TP': test_metrics[5],\n",
    "         'Test_TN': test_metrics[6],\n",
    "         'Test_FP': test_metrics[7],\n",
    "         'Test_FN': test_metrics[8],\n",
    "         'Best_Fold': best_fold\n",
    "    }]\n",
    "    _test_cols = ['Test_Precision','Test_Recall','Test_F1','Test_Accuracy','Test_AUC','Test_TP','Test_TN','Test_FP','Test_FN','Best_Fold']\n",
    "    pd.DataFrame(_test_row)[_test_cols].to_csv(os.path.join(OUTPUT_DIR, \"test_results.csv\"), index=False)\n",
    "\n",
    "    # Save detailed predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(OUTPUT_DIR, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(OUTPUT_DIR, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(OUTPUT_DIR, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(model, output_dir, resolution):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, resolution, resolution, device=device)\n",
    "    onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, \"single_eye_resnet18.tflite\")\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # PyTorch â†’ ONNX\n",
    "    print(\"1. Converting PyTorch model to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            # No dynamic_axes â†’ fixes input size\n",
    "        )\n",
    "        print(f\"   âœ… ONNX model saved to: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ PyTorch to ONNX failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ONNX â†’ TensorFlow\n",
    "    print(\"2. Converting ONNX model to TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TensorFlow SavedModel saved to: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ONNX to TensorFlow failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # TensorFlow â†’ TFLite\n",
    "    print(\"3. Converting TensorFlow SavedModel to TFLite...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"   âœ… TFLite model saved to: {tflite_path}\")\n",
    "        print(f\"   TFLite Model Size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TensorFlow to TFLite failed: {e}\")\n",
    "        return\n",
    "\n",
    "# =========================\n",
    "# TFLITE EVALUATION (AUTO-DETECT INPUT SIZE) WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_on_test_with_predictions(tflite_path, test_imgs, test_lbls, test_filenames):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    expected_shape = input_details[0]['shape']\n",
    "    print(f\"ðŸ” TFLite model input shape: {expected_shape}\")\n",
    "\n",
    "    if len(expected_shape) != 4:\n",
    "        raise ValueError(f\"Expected 4D input, got {expected_shape}\")\n",
    "\n",
    "    batch = expected_shape[0]\n",
    "    assert batch == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Detect layout: NCHW if shape[1] == 3, NHWC if shape[3] == 3\n",
    "    if expected_shape[1] == 3 and expected_shape[3] != 3:\n",
    "        layout = 'NCHW'\n",
    "        _, _, h, w = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NCHW\")\n",
    "    elif expected_shape[3] == 3 and expected_shape[1] != 3:\n",
    "        layout = 'NHWC'\n",
    "        _, h, w, _ = expected_shape\n",
    "        resize_h, resize_w = int(h), int(w)\n",
    "        print(f\"   âž¤ Detected layout: NHWC\")\n",
    "    else:\n",
    "        # Fallback: assume NHWC if last dim is 3\n",
    "        if expected_shape[-1] == 3:\n",
    "            layout = 'NHWC'\n",
    "            _, h, w, _ = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        elif expected_shape[1] == 3:\n",
    "            layout = 'NCHW'\n",
    "            _, _, h, w = expected_shape\n",
    "            resize_h, resize_w = int(h), int(w)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine layout from shape {expected_shape}\")\n",
    "\n",
    "    preds, probs, labels_all = [], [], []\n",
    "\n",
    "    # Use PIL + torchvision to EXACTLY match PyTorch preprocessing\n",
    "    def preprocess_pil_style(img_rgb, target_size):\n",
    "        \"\"\"\n",
    "        Reproduce:\n",
    "          transforms.ToPILImage() â†’ Resize â†’ ToTensor â†’ Normalize\n",
    "        \"\"\"\n",
    "        # img_rgb: numpy array (H, W, C), uint8, RGB\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        # Resize with PIL BILINEAR (same as torchvision)\n",
    "        resized_pil = pil_img.resize((target_size[1], target_size[0]), Image.BILINEAR)  # (W, H)\n",
    "        # ToTensor: (H, W, C) uint8 â†’ (C, H, W) float32 [0,1]\n",
    "        tensor = to_tensor(resized_pil)  # shape: (C, H, W)\n",
    "        # Normalize with ImageNet stats\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()  # (C, H, W), float32\n",
    "\n",
    "    for img, label in zip(test_imgs, test_lbls):\n",
    "        # img is RGB numpy array (H, W, C), uint8 â€” same as loaded by cv2.cvtColor(..., cv2.COLOR_BGR2RGB)\n",
    "        img_norm_nchw = preprocess_pil_style(img, (resize_h, resize_w))  # (C, H, W)\n",
    "\n",
    "        if layout == 'NCHW':\n",
    "            input_data = np.expand_dims(img_norm_nchw, axis=0)  # (1, C, H, W)\n",
    "        else:  # NHWC\n",
    "            img_norm_nhwc = np.transpose(img_norm_nchw, (1, 2, 0))  # (H, W, C)\n",
    "            input_data = np.expand_dims(img_norm_nhwc, axis=0)  # (1, H, W, C)\n",
    "\n",
    "        input_data = input_data.astype(input_details[0]['dtype'])\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1]\n",
    "        logit = output[0][0]\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        preds.append(pred)\n",
    "        probs.append(prob)\n",
    "        labels_all.append(label)\n",
    "\n",
    "    if len(set(labels_all)) < 2:\n",
    "        print(\"âš ï¸ Only one class in test set!\")\n",
    "        return None\n",
    "\n",
    "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds, average='binary')\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    auc = roc_auc_score(labels_all, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_all, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return P, R, F1, acc, auc, tp, tn, fp, fn, labels_all, probs, test_filenames, preds\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate\n",
    "    zz = train_and_eval_single()\n",
    "    print(\"\\nâœ… Single-eye model finished reproducibly with NO DATA LEAKAGE.\")\n",
    "\n",
    "    # Convert to TFLite\n",
    "    convert_to_tflite(zz, OUTPUT_DIR, RESOLUTION)\n",
    "    print(\"âœ… TFLite conversion pipeline complete.\")\n",
    "\n",
    "    # Re-evaluate TFLite on same test set\n",
    "    print(\"\\nðŸ” Loading TFLite model and re-evaluating on original test set...\")\n",
    "    tflite_file = os.path.join(OUTPUT_DIR, \"single_eye_resnet18.tflite\")\n",
    "    if not os.path.exists(tflite_file):\n",
    "        raise FileNotFoundError(f\"TFLite model not found at {tflite_file}\")\n",
    "\n",
    "    tflite_metrics = evaluate_tflite_on_test_with_predictions(tflite_file, test_imgs, test_lbls, test_filenames)\n",
    "\n",
    "    if tflite_metrics:\n",
    "        P, R, F1, acc, auc, tp, tn, fp, fn, tflite_labels, tflite_probs, tflite_filenames, tflite_preds = tflite_metrics\n",
    "        print(\"\\nðŸ“Š TFLITE TEST RESULTS (Same test set):\")\n",
    "        print(f\"Precision: {P:.4f}\")\n",
    "        print(f\"Recall:    {R:.4f}\")\n",
    "        print(f\"F1 score:  {F1:.4f}\")\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"AUC:       {auc:.4f}\")\n",
    "        print(f\"TP, TN, FP, FN: {int(tp)}, {int(tn)}, {int(fp)}, {int(fn)}\")\n",
    "\n",
    "        # Save detailed TFLite predictions to CSV\n",
    "        save_predictions_to_csv(\n",
    "            tflite_filenames,\n",
    "            tflite_labels,\n",
    "            tflite_preds,\n",
    "            tflite_probs,\n",
    "            os.path.join(OUTPUT_DIR, \"detailed_predictions_tflite.csv\")\n",
    "        )\n",
    "\n",
    "        # Plot ROC curve and confusion matrix for TFLite model\n",
    "        plot_roc_curve(tflite_labels, tflite_probs,\n",
    "                       \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                       os.path.join(OUTPUT_DIR, \"roc_curve_tflite.png\"))\n",
    "        plot_confusion_matrix(tflite_labels,\n",
    "                              tflite_preds,\n",
    "                              \"Confusion Matrix - TFLite Model\",\n",
    "                              os.path.join(OUTPUT_DIR, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "        test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "        if os.path.exists(test_results_path):\n",
    "            orig = pd.read_csv(test_results_path).iloc[0]\n",
    "            print(\"\\nðŸ” Comparing with original PyTorch test results:\")\n",
    "            print(f\"PyTorch â†’ P: {orig['Test_Precision']:.4f}, R: {orig['Test_Recall']:.4f}, AUC: {orig['Test_AUC']:.4f}\")\n",
    "            print(f\"TFLite  â†’ P: {P:.4f}, R: {R:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(orig.to_dict(), tflite_metrics,\n",
    "                                   os.path.join(OUTPUT_DIR, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            p_ok = abs(P - orig['Test_Precision']) < tol\n",
    "            r_ok = abs(R - orig['Test_Recall']) < tol\n",
    "            auc_ok = abs(auc - orig['Test_AUC']) < tol\n",
    "\n",
    "            if p_ok and r_ok and auc_ok:\n",
    "                print(\"âœ… TFLite results match PyTorch within tolerance (1e-3).\")\n",
    "            else:\n",
    "                print(\"âš ï¸ TFLite results differ from PyTorch (check normalization or export).\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Original test results not found for comparison.\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite evaluation failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Analysis complete: Original chronological test set evaluated with detailed CSV predictions and comprehensive plots!\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-right-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Shared ResNet18 across 3 inputs â†’ ~45 MB TFLite\n",
    "âœ… TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "âœ… Re-evaluates .tflite file and compares results with PyTorch\n",
    "âœ… Ensures no silent divergence between frameworks\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224  # Only one resolution used now\n",
    "EPOCHS_CV = 150\n",
    "BATCH_CV = 28\n",
    "LR_CV = 0.00022\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "BASE_PATH=base_path\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_tri_right_eye_hb_90_repro_bestfold_only_shared\")\n",
    "output_dir = OUTPUT_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_right(dirs_map):\n",
    "    suffixes = {'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['right1', 'right2', 'right3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('right1', '_right_eye_1.png'), 'r2': ('right2', '_right_eye_2.png'), 'r3': ('right3', '_right_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_right_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_right(anemic_dirs)\n",
    "    bases_n = common_bases_right(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_right.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in ['right1','right2','right3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (right)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (right)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_right_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_right_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_right_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-right-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrirightDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetright(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_right_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_right_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing right resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- right Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrirightDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrirightDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetright().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'right',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"right_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"right_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"right_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"right_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetright().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrirightDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"right_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_right_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Tri-left-Eye ResNet18 with SHARED BACKBONE & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Shared ResNet18 across 3 inputs â†’ ~45 MB TFLite\n",
    "âœ… TFLite export with SELECT_TF_OPS (GELU via FlexDelegate)\n",
    "âœ… Re-evaluates .tflite file and compares results with PyTorch\n",
    "âœ… Ensures no silent divergence between frameworks\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 120\n",
    "BATCH_CV = 24\n",
    "LR_CV = 0.00017\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "BASE_PATH=base_path\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_tri_left_eye_hb_90_repro_bestfold_only_shared\")\n",
    "output_dir = OUTPUT_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_left(dirs_map):\n",
    "    suffixes = {'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png'}\n",
    "    bases_sets = []\n",
    "    for k in ['left1', 'left2', 'left3']:\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_tri_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {'r1': [], 'r2': [], 'r3': [], 'filenames': []}\n",
    "    key_map = {'r1': ('left1', '_left_eye_1.png'), 'r2': ('left2', '_left_eye_2.png'), 'r3': ('left3', '_left_eye_3.png')}\n",
    "    for b in bases:\n",
    "        imgs, failed = {}, False\n",
    "        for short_k, (long_k, suf) in key_map.items():\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs[short_k] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if not failed:\n",
    "            out['r1'].append(imgs['r1'])\n",
    "            out['r2'].append(imgs['r2'])\n",
    "            out['r3'].append(imgs['r3'])\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_left_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_left(anemic_dirs)\n",
    "    bases_n = common_bases_left(non_dirs)\n",
    "    imgs_a = load_tri_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_tri_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_left.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in ['left1','left2','left3']:\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (left)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (left)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_left_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_left_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_left_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No tri-left-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class TrileftDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[k][idx] for k in ['r1','r2','r3']]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE\n",
    "# =========================\n",
    "class TriResNetleft(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        x = torch.cat([f1, f2, f3], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(x1, x2, x3)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(3))\n",
    "    onnx_path = os.path.join(output_dir, \"tri_left_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"tri_left_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input1', 'input2', 'input3'],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data['r1'][i], test_data['r2'][i], test_data['r3'][i]]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing left resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- left Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(TrileftDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(TrileftDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = TriResNetleft().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x1, x2, x3 = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(x1, x2, x3)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'left',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8]\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"left_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"left_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"left_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"left_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = TriResNetleft().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation (PyTorch)\n",
    "    test_loader = make_loader(TrileftDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"left_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"tri_left_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Pipeline completed. Model size reduced to ~45 MB via shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reproducible Hexa-Eye ResNet18 with SHARED BACKBONE + EARLY STOPPING & TFLite Verification\n",
    "----------------------------------------------------------------------------------\n",
    "âœ… Single ResNet18 used across 6 inputs â†’ ~47 MB TFLite\n",
    "âœ… Stops fold early if Val Precision & Recall >= 0.90\n",
    "âœ… Re-evaluates .tflite model and compares predictions with PyTorch\n",
    "âœ… Ensures faithful deployment (no numerical drift)\n",
    "âœ… Deterministic training + FlexDelegate support for GELU\n",
    "âœ… Added detailed prediction CSV with file IDs, predictions, probabilities, and confusion matrix indicators\n",
    "âœ… Added comprehensive plotting: ROC curves, confusion matrices, metrics comparison\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "USE_AMP = True\n",
    "SAVE_EVERY_FOLD_MODEL = True\n",
    "N_SPLITS = 5\n",
    "RESOLUTION = 224\n",
    "EPOCHS_CV = 300\n",
    "BATCH_CV = 8  # Reduced due to 6 inputs per sample\n",
    "LR_CV = 0.000256\n",
    "\n",
    "\n",
    "# ðŸ”¥ NEW: Early stop if both P and R >= this threshold\n",
    "EARLY_STOP_PR = 0.90\n",
    "\n",
    "# =========================\n",
    "# DETERMINISM\n",
    "# =========================\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_global_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "base_path = \"/home/ubuntu/anemia-storage/hb_mobilenet/mat_conjunctiva_all_consistent_deletion/\"\n",
    "BASE_PATH=base_path\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"9_5_hexa_eye_hb_90_repro_bestfold_only_shared2\")\n",
    "output_dir = OUTPUT_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_full_path(subdirs):\n",
    "    return {k: os.path.join(base_path, v) for k, v in subdirs.items()}\n",
    "\n",
    "# All six input directories\n",
    "train_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_train_roi/'\n",
    "})\n",
    "train_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_train_roi/'\n",
    "})\n",
    "val_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_val_roi/'\n",
    "})\n",
    "val_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_val_roi/'\n",
    "})\n",
    "test_dirs_anemic = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_test_roi/'\n",
    "})\n",
    "test_dirs_non = make_full_path({\n",
    "    'left1': 'tri_left_eye/left_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left2': 'tri_left_eye/left_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'left3': 'tri_left_eye/left_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right1': 'tri_right_eye/right_eye_1_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right2': 'tri_right_eye/right_eye_2_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/',\n",
    "    'right3': 'tri_right_eye/right_eye_3_hb_less_than_9_5/conjunctiva_extracted/anemic_not_test_roi/'\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def base_from(fname, suffix):\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else None\n",
    "\n",
    "def common_bases_hexa(dirs_map):\n",
    "    suffixes = {\n",
    "        'left1': '_left_eye_1.png', 'left2': '_left_eye_2.png', 'left3': '_left_eye_3.png',\n",
    "        'right1': '_right_eye_1.png', 'right2': '_right_eye_2.png', 'right3': '_right_eye_3.png'\n",
    "    }\n",
    "    bases_sets = []\n",
    "    for k in dirs_map.keys():\n",
    "        folder = dirs_map[k]\n",
    "        if not os.path.isdir(folder):\n",
    "            return []\n",
    "        names = [f for f in os.listdir(folder) if f.endswith(suffixes[k])]\n",
    "        bases = {base_from(f, suffixes[k]) for f in names if base_from(f, suffixes[k]) is not None}\n",
    "        bases_sets.append(bases)\n",
    "    if not bases_sets:\n",
    "        return []\n",
    "    inter = set.intersection(*bases_sets)\n",
    "    return sorted(inter)\n",
    "\n",
    "def load_hexa_images_by_bases_with_filenames(dirs_map, bases):\n",
    "    out = {f'r{i}': [] for i in range(1,7)}\n",
    "    out['filenames'] = []\n",
    "    key_map = [\n",
    "        ('left1', '_left_eye_1.png'),\n",
    "        ('left2', '_left_eye_2.png'),\n",
    "        ('left3', '_left_eye_3.png'),\n",
    "        ('right1', '_right_eye_1.png'),\n",
    "        ('right2', '_right_eye_2.png'),\n",
    "        ('right3', '_right_eye_3.png')\n",
    "    ]\n",
    "    for b in bases:\n",
    "        imgs, failed = [], False\n",
    "        for long_k, suf in key_map:\n",
    "            path = os.path.join(dirs_map[long_k], b + suf)\n",
    "            if not os.path.isfile(path):\n",
    "                failed = True\n",
    "                break\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                failed = True\n",
    "                break\n",
    "            imgs.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if not failed:\n",
    "            for i, img in enumerate(imgs):\n",
    "                out[f'r{i+1}'].append(img)\n",
    "            out['filenames'].append(b)  # Use base name as identifier\n",
    "    return out\n",
    "\n",
    "def prepare_dataset_hexa_with_filenames(anemic_dirs, non_dirs, split_name=\"(split)\"):\n",
    "    bases_a = common_bases_hexa(anemic_dirs)\n",
    "    bases_n = common_bases_hexa(non_dirs)\n",
    "    imgs_a = load_hexa_images_by_bases_with_filenames(anemic_dirs, bases_a)\n",
    "    imgs_n = load_hexa_images_by_bases_with_filenames(non_dirs, bases_n)\n",
    "\n",
    "    data = {\n",
    "        'r1': imgs_a['r1'] + imgs_n['r1'],\n",
    "        'r2': imgs_a['r2'] + imgs_n['r2'],\n",
    "        'r3': imgs_a['r3'] + imgs_n['r3'],\n",
    "        'r4': imgs_a['r4'] + imgs_n['r4'],\n",
    "        'r5': imgs_a['r5'] + imgs_n['r5'],\n",
    "        'r6': imgs_a['r6'] + imgs_n['r6'],\n",
    "        'filenames': imgs_a['filenames'] + imgs_n['filenames'],\n",
    "        'label': [1]*len(imgs_a['r1']) + [0]*len(imgs_n['r1'])\n",
    "    }\n",
    "    print(f\"âœ… {split_name}: anemic={len(imgs_a['r1'])}, non-anemic={len(imgs_n['r1'])}, total={len(data['label'])}\")\n",
    "    try:\n",
    "        df_bases = pd.DataFrame({'class': ['anemic']*len(bases_a) + ['non_anemic']*len(bases_n),\n",
    "                                 'base_id': bases_a + bases_n})\n",
    "        df_bases.to_csv(os.path.join(output_dir, f\"{split_name.lower()}_used_base_ids_hexa.csv\"), index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def count_files(d):\n",
    "    return sum(1 for f in sorted(os.listdir(d)) if f.endswith(\".png\")) if os.path.isdir(d) else 0\n",
    "\n",
    "def print_dir_stats(title, dirs_map):\n",
    "    print(f\"\\nðŸ“‚ {title}\")\n",
    "    for k in dirs_map.keys():\n",
    "        p = dirs_map[k]; c = count_files(p)\n",
    "        print(f\"{k:7s} | {p} | files={c}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "\n",
    "print_dir_stats(\"TEST  anemic (HEXA)\", test_dirs_anemic)\n",
    "print_dir_stats(\"TEST  non-anemic (HEXA)\", test_dirs_non)\n",
    "\n",
    "train_data = prepare_dataset_hexa_with_filenames(train_dirs_anemic, train_dirs_non, split_name=\"TRAIN\")\n",
    "val_data   = prepare_dataset_hexa_with_filenames(val_dirs_anemic,   val_dirs_non,   split_name=\"VAL\")\n",
    "test_data  = prepare_dataset_hexa_with_filenames(test_dirs_anemic,  test_dirs_non,  split_name=\"TEST\")\n",
    "\n",
    "if len(train_data['label']) == 0:\n",
    "    raise RuntimeError(\"No hexa-eye TRAIN samples found.\")\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class HexaDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "    def __getitem__(self, idx):\n",
    "        images = [self.data[f'r{i}'][idx] for i in range(1,7)]\n",
    "        images = [self.transform(img) for img in images]\n",
    "        label = self.data['label'][idx]\n",
    "        return images, label\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY and (device.type=='cuda'),\n",
    "        worker_init_fn=seed_worker if NUM_WORKERS > 0 else None,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# MODEL: SHARED RESNET18 BACKBONE (6 inputs)\n",
    "# =========================\n",
    "class HexaResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Single shared backbone\n",
    "        self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # Fusion head: 6*512 â†’ 1\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(6 * 512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5, x6):\n",
    "        f1 = self.backbone(x1)\n",
    "        f2 = self.backbone(x2)\n",
    "        f3 = self.backbone(x3)\n",
    "        f4 = self.backbone(x4)\n",
    "        f5 = self.backbone(x5)\n",
    "        f6 = self.backbone(x6)\n",
    "        x = torch.cat([f1, f2, f3, f4, f5, f6], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (PyTorch) WITH PREDICTIONS\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def evaluate_with_predictions(model, loader, filenames):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get all filenames in loader order\n",
    "    batch_size = loader.batch_size\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_end = min(i + batch_size, len(filenames))\n",
    "        all_filenames.extend(filenames[i:batch_end])\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        x_list = [img.to(device).float() for img in imgs]\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        out = model(*x_list)\n",
    "        prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        all_preds.extend(pred.tolist())\n",
    "        all_probs.extend(prob.tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    if len(all_labels) == 0 or len(set(all_labels)) < 2:\n",
    "        return [float('nan')] * 9 + [all_labels, all_probs, all_filenames, all_preds]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# TFLITE CONVERSION\n",
    "# =========================\n",
    "def convert_to_tflite(best_model: nn.Module, output_dir: str, resolution: int, tflite_filename: str):\n",
    "    import torch.onnx\n",
    "    import onnx\n",
    "    from onnx_tf.backend import prepare\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    best_model.eval().to('cpu')\n",
    "    dummy_inputs = tuple(torch.randn(1, 3, resolution, resolution) for _ in range(6))\n",
    "    onnx_path = os.path.join(output_dir, \"hexa_model.onnx\")\n",
    "    tf_path = os.path.join(output_dir, \"hexa_tf_model\")\n",
    "    tflite_path = os.path.join(output_dir, tflite_filename)\n",
    "\n",
    "    print(\"\\n--- Starting TFLite Conversion Pipeline ---\")\n",
    "\n",
    "    # Step 1: PyTorch -> ONNX\n",
    "    print(\"1. Converting to ONNX...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            best_model,\n",
    "            dummy_inputs,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[f'input{i}' for i in range(1,7)],\n",
    "            output_names=['output']\n",
    "        )\n",
    "        print(f\"   âœ… ONNX saved: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: ONNX -> TensorFlow\n",
    "    print(\"2. ONNX -> TensorFlow SavedModel...\")\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        if os.path.exists(tf_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(tf_path)\n",
    "        tf_rep.export_graph(tf_path)\n",
    "        print(f\"   âœ… TF SavedModel saved: {tf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: TF -> TFLite with SELECT_TF_OPS\n",
    "    print(\"3. TF -> TFLite (SELECT_TF_OPS)...\")\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        tflite_model = converter.convert()\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"   âœ… TFLite saved: {tflite_path} ({size_mb:.2f} MB)\")\n",
    "        return tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Conversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# TFLITE RE-EVALUATION WITH PREDICTIONS\n",
    "# =========================\n",
    "def evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution):\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor, normalize\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Sort by name to ensure correct order\n",
    "    sorted_inputs = sorted(input_details, key=lambda x: x['name'])\n",
    "    print(f\"ðŸ” TFLite input names: {[d['name'] for d in sorted_inputs]}\")\n",
    "\n",
    "    first_shape = sorted_inputs[0]['shape']\n",
    "    if len(first_shape) == 4:\n",
    "        if first_shape[1] == 3:  # [B,C,H,W]\n",
    "            layout = 'NCHW'\n",
    "        else:  # [B,H,W,C]\n",
    "            layout = 'NHWC'\n",
    "\n",
    "    resize_h, resize_w = int(first_shape[1] if layout == 'NHWC' else first_shape[2]), \\\n",
    "                         int(first_shape[2] if layout == 'NHWC' else first_shape[3])\n",
    "\n",
    "    print(f\"   âž¤ Detected layout: {layout}, size: {resize_h}x{resize_w}\")\n",
    "\n",
    "    def preprocess_pil_style(img_rgb):\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_resized = img_pil.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "        tensor = to_tensor(img_resized)\n",
    "        normalized = normalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalized.numpy()\n",
    "\n",
    "    all_preds, all_probs, all_labels, all_filenames = [], [], [], test_data['filenames']\n",
    "\n",
    "    for i in range(len(test_data['label'])):\n",
    "        imgs = [test_data[f'r{j}'][i] for j in range(1,7)]\n",
    "        label = test_data['label'][i]\n",
    "\n",
    "        for idx, detail in enumerate(sorted_inputs):\n",
    "            raw_img = imgs[idx]\n",
    "            processed = preprocess_pil_style(raw_img)\n",
    "\n",
    "            if layout == 'NCHW':\n",
    "                model_input = np.expand_dims(processed, axis=0).astype(detail['dtype'])\n",
    "            else:\n",
    "                nhwc = np.transpose(processed, (1, 2, 0))\n",
    "                model_input = np.expand_dims(nhwc, axis=0).astype(detail['dtype'])\n",
    "\n",
    "            interpreter.set_tensor(detail['index'], model_input)\n",
    "\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        logit = float(np.array(output).reshape(-1)[0])\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        pred = int(prob > 0.5)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    if len(set(all_labels)) < 2:\n",
    "        auc = float('nan')\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds, labels=[0,1]).ravel()\n",
    "\n",
    "    return p, r, f1, acc, auc, tp, tn, fp, fn, all_labels, all_probs, all_filenames, all_preds\n",
    "\n",
    "# =========================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =========================\n",
    "def plot_roc_curve(y_true, y_scores, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-Anemic', 'Anemic'],\n",
    "                yticklabels=['Non-Anemic', 'Anemic'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(pytorch_metrics, tflite_metrics, save_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']\n",
    "    pytorch_vals = [pytorch_metrics['Test_Precision'], pytorch_metrics['Test_Recall'],\n",
    "                    pytorch_metrics['Test_F1'], pytorch_metrics['Test_Accuracy'],\n",
    "                    pytorch_metrics['Test_AUC']]\n",
    "    tflite_vals = [tflite_metrics[0], tflite_metrics[1],\n",
    "                   tflite_metrics[2], tflite_metrics[3],\n",
    "                   tflite_metrics[4]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, pytorch_vals, width, label='PyTorch', color='steelblue')\n",
    "    plt.bar(x + width/2, tflite_vals, width, label='TFLite', color='darkorange')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('PyTorch vs TFLite Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# SAVE PREDICTIONS TO CSV\n",
    "# =========================\n",
    "def save_predictions_to_csv(filenames, true_labels, pred_labels, pred_probs, output_path):\n",
    "    # Convert labels to readable format\n",
    "    true_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in true_labels]\n",
    "    pred_labels_str = ['Anemic' if label == 1 else 'Non-Anemic' for label in pred_labels]\n",
    "\n",
    "    # Calculate confusion matrix indicators\n",
    "    tp = [1 if (t == 1 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    tn = [1 if (t == 0 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fp = [1 if (t == 0 and p == 1) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "    fn = [1 if (t == 1 and p == 0) else 0 for t, p in zip(true_labels, pred_labels)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'file_id': filenames,\n",
    "        'actual_value': true_labels_str,\n",
    "        'predicted_value': pred_labels_str,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING LOOP (with EARLY STOPPING)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    resolution = RESOLUTION\n",
    "    results = []\n",
    "    cv_index_records = []\n",
    "\n",
    "    print(f\"\\n===== Processing HEXA resolution: {resolution} =====\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    labels_np = np.array(train_data['label'])\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(np.zeros_like(labels_np), labels_np):\n",
    "        print(f\"\\n--- HEXA Fold {fold} ---\")\n",
    "        cv_index_records.append({\"fold\": fold, \"train_indices\": train_idx.tolist(), \"val_indices\": val_idx.tolist()})\n",
    "\n",
    "        train_subset = {k: [v[i] for i in train_idx] for k, v in train_data.items()}\n",
    "        val_subset   = {k: [v[i] for i in val_idx]   for k, v in train_data.items()}\n",
    "\n",
    "        train_loader = make_loader(HexaDataset(train_subset, train_transform), BATCH_CV, True)\n",
    "        val_loader   = make_loader(HexaDataset(val_subset,   test_transform),  BATCH_CV, False)\n",
    "\n",
    "        model = HexaResNet().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_CV)\n",
    "        scaler = GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "        stopped_early = False\n",
    "        for epoch in range(EPOCHS_CV):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                x_list = [img.to(device).float() for img in imgs]\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
    "                    out = model(*x_list)\n",
    "                    loss = criterion(out, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS_CV}] Loss: {total_loss:.6f}\")\n",
    "\n",
    "            # ðŸ”¥ EARLY STOPPING CHECK\n",
    "            if EARLY_STOP_PR is not None:\n",
    "                val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "                p, r = val_metrics[0], val_metrics[1]\n",
    "                if not (np.isnan(p) or np.isnan(r)) and p >= EARLY_STOP_PR and r >= EARLY_STOP_PR and p < 1 and r < 1:\n",
    "                    print(f\"âœ… Early stop at epoch {epoch+1}: P={p:.3f}, R={r:.3f}\")\n",
    "                    stopped_early = True\n",
    "                    break  # exit inner loop\n",
    "\n",
    "        # Final evaluation after training (or early stop)\n",
    "        val_metrics = evaluate_with_predictions(model, val_loader, val_subset['filenames'])\n",
    "        result_row = {\n",
    "            'EyeSet': 'HEXA',\n",
    "            'Resolution': resolution,\n",
    "            'Fold': fold,\n",
    "            'Val_Precision': val_metrics[0],\n",
    "            'Val_Recall': val_metrics[1],\n",
    "            'Val_F1': val_metrics[2],\n",
    "            'Val_Accuracy': val_metrics[3],\n",
    "            'Val_AUC': val_metrics[4],\n",
    "            'Val_TP': val_metrics[5],\n",
    "            'Val_TN': val_metrics[6],\n",
    "            'Val_FP': val_metrics[7],\n",
    "            'Val_FN': val_metrics[8],\n",
    "            'Stopped_Early': stopped_early\n",
    "        }\n",
    "        results.append(result_row)\n",
    "        print(results)\n",
    "        if SAVE_EVERY_FOLD_MODEL:\n",
    "            fold_path = os.path.join(output_dir, f\"hexa_cv_fold_{fold}_res{resolution}.pt\")\n",
    "            torch.save({'model_state': model.state_dict()}, fold_path)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Save CV results\n",
    "    pd.DataFrame(results).to_csv(os.path.join(output_dir, \"hexa_val_cross_validation_results.csv\"), index=False)\n",
    "    with open(os.path.join(output_dir, \"hexa_cv_indices.json\"), \"w\") as f:\n",
    "        json.dump(cv_index_records, f, indent=2)\n",
    "\n",
    "    # Select best fold\n",
    "    df = pd.DataFrame(results)\n",
    "    df['minPR'] = df[['Val_Precision','Val_Recall']].min(axis=1)\n",
    "    candidates = df[(df['Val_Precision'] >= 0.90) & (df['Val_Recall'] >= 0.90) & (df['Val_Precision'] < 1) & (df['Val_Recall'] < 1)]\n",
    "    best = candidates.sort_values(['Val_F1'], ascending=False).iloc[0] if len(candidates) > 0 else \\\n",
    "           df.sort_values(['minPR','Val_F1'], ascending=False).iloc[0]\n",
    "    best_fold = int(best['Fold'])\n",
    "    print(f\"âœ… Best fold = {best_fold}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt_path = os.path.join(output_dir, f\"hexa_cv_fold_{best_fold}_res{resolution}.pt\")\n",
    "    best_model = HexaResNet().to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    best_model.load_state_dict(state['model_state'])\n",
    "\n",
    "    # Test evaluation\n",
    "    test_loader = make_loader(HexaDataset(test_data, test_transform), BATCH_CV, False)\n",
    "    test_metrics = evaluate_with_predictions(best_model, test_loader, test_data['filenames'])\n",
    "\n",
    "    test_results_df = pd.DataFrame([{\n",
    "        'ChosenFold': best_fold,\n",
    "        'Test_Precision': test_metrics[0],\n",
    "        'Test_Recall': test_metrics[1],\n",
    "        'Test_F1': test_metrics[2],\n",
    "        'Test_Accuracy': test_metrics[3],\n",
    "        'Test_AUC': test_metrics[4],\n",
    "        'Test_TP': test_metrics[5],\n",
    "        'Test_TN': test_metrics[6],\n",
    "        'Test_FP': test_metrics[7],\n",
    "        'Test_FN': test_metrics[8]\n",
    "    }])\n",
    "    test_results_df.to_csv(os.path.join(output_dir, \"hexa_bestfold_test_results.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nðŸ“Š TEST Results (Shared Backbone):\")\n",
    "    print(test_results_df.to_string(index=False))\n",
    "\n",
    "    # Save detailed PyTorch predictions to CSV\n",
    "    save_predictions_to_csv(\n",
    "        test_metrics[11],  # filenames\n",
    "        test_metrics[9],   # true labels\n",
    "        test_metrics[12],  # pred labels\n",
    "        test_metrics[10],  # pred probs\n",
    "        os.path.join(output_dir, \"detailed_predictions_pytorch.csv\")\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve and confusion matrix for PyTorch model\n",
    "    plot_roc_curve(test_metrics[9], test_metrics[10],\n",
    "                   \"ROC Curve - PyTorch Model (Original Chronological Test Set)\",\n",
    "                   os.path.join(output_dir, \"roc_curve_pytorch.png\"))\n",
    "    plot_confusion_matrix(test_metrics[9],\n",
    "                          test_metrics[12],\n",
    "                          \"Confusion Matrix - PyTorch Model\",\n",
    "                          os.path.join(output_dir, \"confusion_matrix_pytorch.png\"))\n",
    "\n",
    "    # Convert to TFLite\n",
    "    tflite_filename = \"hexa_eye_resnet18_shared.tflite\"\n",
    "    tflite_path = convert_to_tflite(best_model.cpu(), output_dir, resolution, tflite_filename)\n",
    "\n",
    "    if tflite_path:\n",
    "        size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final TFLite model size: {size_mb:.2f} MB\")\n",
    "        print(f\"ðŸ“ Path: {tflite_path}\")\n",
    "\n",
    "        # --- ðŸ” Re-evaluate TFLite model ---\n",
    "        print(\"\\nðŸ” Re-evaluating TFLite model on test set...\")\n",
    "        try:\n",
    "            tflite_metrics = evaluate_tflite_model_with_predictions(tflite_path, test_data, resolution)\n",
    "            tflite_results_df = pd.DataFrame([{\n",
    "                'Source': 'TFLite',\n",
    "                'Test_Precision': tflite_metrics[0],\n",
    "                'Test_Recall': tflite_metrics[1],\n",
    "                'Test_F1': tflite_metrics[2],\n",
    "                'Test_Accuracy': tflite_metrics[3],\n",
    "                'Test_AUC': tflite_metrics[4],\n",
    "                'Test_TP': tflite_metrics[5],\n",
    "                'Test_TN': tflite_metrics[6],\n",
    "                'Test_FP': tflite_metrics[7],\n",
    "                'Test_FN': tflite_metrics[8]\n",
    "            }])\n",
    "\n",
    "            combined = pd.concat([\n",
    "                test_results_df.assign(Source='PyTorch'),\n",
    "                tflite_results_df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            print(\"\\nðŸ“Š COMPARISON: PyTorch vs TFLite\")\n",
    "            print(combined.to_string(index=False))\n",
    "            combined.to_csv(os.path.join(output_dir, \"pytorch_vs_tflite_comparison.csv\"), index=False)\n",
    "\n",
    "            # Save detailed TFLite predictions to CSV\n",
    "            save_predictions_to_csv(\n",
    "                tflite_metrics[11],  # filenames\n",
    "                tflite_metrics[9],   # true labels\n",
    "                tflite_metrics[12],  # pred labels\n",
    "                tflite_metrics[10],  # pred probs\n",
    "                os.path.join(output_dir, \"detailed_predictions_tflite.csv\")\n",
    "            )\n",
    "\n",
    "            # Plot ROC curve and confusion matrix for TFLite model\n",
    "            plot_roc_curve(tflite_metrics[9], tflite_metrics[10],\n",
    "                           \"ROC Curve - TFLite Model (Original Chronological Test Set)\",\n",
    "                           os.path.join(output_dir, \"roc_curve_tflite.png\"))\n",
    "            plot_confusion_matrix(tflite_metrics[9],\n",
    "                                  tflite_metrics[12],\n",
    "                                  \"Confusion Matrix - TFLite Model\",\n",
    "                                  os.path.join(output_dir, \"confusion_matrix_tflite.png\"))\n",
    "\n",
    "            # Create metrics comparison plot\n",
    "            plot_metrics_comparison(test_results_df.iloc[0].to_dict(), tflite_metrics,\n",
    "                                   os.path.join(output_dir, \"metrics_comparison.png\"))\n",
    "\n",
    "            tol = 1e-3\n",
    "            if (abs(tflite_metrics[2] - test_metrics[2]) < tol and\n",
    "                abs(tflite_metrics[4] - test_metrics[4]) < tol):\n",
    "                print(\"âœ… TFLite results MATCH PyTorch within tolerance.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ WARNING: TFLite results differ significantly from PyTorch!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TFLite evaluation failed: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ TFLite conversion failed.\")\n",
    "\n",
    "    print(\"\\nâœ… Hexa-Eye pipeline completed. Model size remains ~47 MB thanks to shared backbone.\")\n",
    "    print(f\"âœ… Detailed prediction CSVs and plots saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/gjFEclR28fym7z+9iMLg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
